import time
import numpy as np
import pandas as pd
from genetic import SymbolicTransformer
from functions import _function_map
import dataload
from datetime import datetime
from pathlib import Path
import warnings
from loguru import logger
import gzip
import yaml
import shutil
import pickle
import joblib
import schedule
import os
import talib as ta
import fitness
from datetime import datetime
from sklearn.linear_model import LinearRegression
import cloudpickle
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from scipy.stats import zscore, kurtosis, skew, yeojohnson, boxcox
from scipy.stats import tukeylambda, mstats
from expressionProgram import FeatureEvaluator
from concurrent.futures import ProcessPoolExecutor
from NormDataCheck import norm, inverse_norm

warnings.filterwarnings("ignore", category=RuntimeWarning)
pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)

# norm_y_list = ['avg_pic','avg_sic','max_ic','max_ic_train','given_ic_test']
# raw_y_list = ['calmar','sharp','sharpe_fixed_threshold','sharpe_std_threshold','max_dd','avg_mdd']

norm_y_list = ['avg_pic','avg_sic','max_ic','max_ic_train','given_ic_test', 'rolling_pic', 'rolling_rank_sic']
raw_y_list = ['calmar','sharp','sharpe_fixed_threshold','sharpe_std_threshold','max_dd','avg_mdd', 'rolling_sharp', 'avg_sharpe_ratio']

def calculate_annual_bars(freq: str) -> int:
    # åˆ›å»ºä¸€ä¸ª pandas å®šæ—¶å™¨ï¼ˆtimestampï¼‰ï¼Œå¹¶è·å– freq å‚æ•°æ‰€è¡¨ç¤ºçš„æ—¶é—´é—´éš”
    freq_timedelta = pd.to_timedelta(freq)
    
    # 24å°æ—¶ç­‰äº86400ç§’
    hours_in_a_day = pd.Timedelta(hours=24)
    
    # è®¡ç®— 24 å°æ—¶å†…åŒ…å«å¤šå°‘bar
    multiples_of_freq = hours_in_a_day // freq_timedelta
    
    #è®¡ç®—å¹´åŒ–å¤šå°‘bar
    annual_bars = 365 * multiples_of_freq
    
    return annual_bars



class GPAnalyzer:
    """
    GPAnalyzer ç±»æœ‰ä»¥ä¸‹ä¸»è¦å†…å®¹ï¼š

    initialize_his_data æ–¹æ³•ï¼Œç”¨äºåˆå§‹åŒ–å†å²æ•°æ®ã€‚è¿™ä¸ªæ–¹æ³•åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åŠ è½½æ•°æ®ï¼Œé¿å…é‡å¤åŠ è½½ã€‚
    run æ–¹æ³•ï¼Œæ›¿ä»£ä¹‹å‰çš„ main å‡½æ•°ã€‚è¿™ä¸ªæ–¹æ³•æ ¹æ®é…ç½®å†³å®šæ˜¯å•æ¬¡æ‰§è¡Œè¿˜æ˜¯å¾ªç¯æ‰§è¡Œä»»åŠ¡ã€‚

    """
    def __init__(self, yaml_file_path):
        self.yaml_file_path = yaml_file_path
        self.config = self.load_yaml_config(yaml_file_path)
        self.load_config_attributes()
        self.data_initialized = False
        self.base_model_directory = Path.cwd() / 'gp_models'
        self.initialize_his_data()
        self.total_factor_file_name = f"{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}.csv.gz"
        # self.total_factor_file_name = f"BTCUSDT_1h_1_2025-01-01_2025-01-10_2025-01-10_2025-01-20.csv.gz"
        # self.total_factor_file_path = "/Users/aming/project/python/crypto-workstation/gp_models/BTCUSDT_1h_1_2025-01-01_2025-01-10_2025-01-10_2025-01-20.csv.gz"
        self.total_factor_file_path = self.base_model_directory / self.total_factor_file_name

    def load_yaml_config(self, file_path):
        """
        ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®ã€‚
        """
        with open(file_path, 'r', encoding='utf-8') as file:
            return yaml.safe_load(file)

    def load_config_attributes(self):
        """
        ä»é…ç½®å­—å…¸ä¸­åŠ è½½å±æ€§åˆ°ç±»å®ä¾‹ã€‚
        """
        self.freq = self.config.get('freq', '')
        self.y_train_ret_period = self.config.get('y_train_ret_period', 1)
        self.sym = self.config.get('sym', '')
        self.coarse_grain_period = self.config.get('coarse_grain_period', '2h')
        self.feature_lookback_bars = self.config.get('feature_lookback_bars', 8)
        self.rolling_step = self.config.get('rolling_step', '15min')
        self.include_categories = self.config.get('include_categories', None)
        self.start_date_train = self.config.get('start_date_train', '')
        self.end_date_train = self.config.get('end_date_train', '')
        self.start_date_test = self.config.get('start_date_test', '')
        self.end_date_test = self.config.get('end_date_test', '')
        self.gp_settings = self.config.get('gp_settings', {})
        self.metric = self.gp_settings.get('metric', 'pearson')
        self.verbose_logging = self.config.get('verbose_logging', False)
        self.rolling_window = self.config.get('rolling_window', 2000)
        self.inverse_rolling_window = self.config.get('inverse_rolling_window', 200)
        self.annual_bars = calculate_annual_bars(self.freq)
        self.data_dir = self.config.get('data_dir', '')
        
        # æ–‡ä»¶è·¯å¾„é…ç½®ï¼ˆç”¨äºç›´æ¥æŒ‡å®šæ•°æ®æ–‡ä»¶ï¼‰
        self.file_path = self.config.get('file_path', None)  # tickäº¤æ˜“æ•°æ®æ–‡ä»¶è·¯å¾„
        self.kline_file_path = self.config.get('kline_file_path', None)  # Kçº¿æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¤‡ç”¨ï¼‰
        
        # æ•°æ®æºå¼€å…³ï¼šklineï¼ˆé»˜è®¤ï¼‰æˆ– microï¼ˆå¾®ç»“æ„ç‰ˆï¼‰
        self.data_source = self.config.get('data_source', 'kline')
        # å¾®ç»“æ„ç‰¹å¾æ§åˆ¶
        self.use_feature_extractors = self.config.get('use_feature_extractors', False)
        self.trades_dir = self.config.get('trades_dir', '')
        self.bar_builder = self.config.get('bar_builder', 'time')
        self.dollar_threshold = self.config.get('dollar_threshold', 1e6)
        self.active_family = self.config.get('active_family', '')
        self.feature_family_include = self.config.get('feature_family_include', [])
        self.feature_family_exclude = self.config.get('feature_family_exclude', [])
        

        # è‡ªåŠ¨åŠ è½½å…¶ä»–é…ç½®é¡¹
        for key, value in self.config.items():
            if not hasattr(self, key):
                setattr(self, key, value)

    def initialize_his_data(self):
        """
        åˆå§‹åŒ–å…±äº«æ•°æ®ï¼ŒåŒ…æ‹¬è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚
        åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ‰§è¡Œæ•°æ®åŠ è½½ã€‚
        """
        if not self.data_initialized:
            if str(self.data_source).lower() == 'micro':
                self.X_all, self.X_train, self.y_train, self.ret_train, self.X_test, self.y_test, self.ret_test, self.feature_names,self.open_train,self.open_test,self.close_train,self.close_test, self.z_index ,self.ohlc= dataload.data_prepare_micro(
                    self.sym, self.freq, self.start_date_train, self.end_date_train,
                    self.start_date_test, self.end_date_test, rolling_w=self.rolling_window, output_format='ndarry',
                    data_dir=self.data_dir, read_frequency=self.read_frequency, timeframe=self.timeframe,
                    use_feature_extractors=self.use_feature_extractors, trades_dir=self.trades_dir,
                    daily_data_template=self.daily_data_template,
                    monthly_data_template=self.monthly_data_template,
                    trades_load_mode=self.trades_load_mode,
                    bar_builder=self.bar_builder, dollar_threshold=self.dollar_threshold,
                    active_family=self.active_family, feature_family_include=self.feature_family_include,
                    feature_family_exclude=self.feature_family_exclude,
                    file_path=self.file_path, kline_file_path=self.kline_file_path)
            elif str(self.data_source).lower() == 'rolling':
                # æ–°å¢ï¼šæ»šåŠ¨ç»Ÿè®¡ç‰ˆæ•°æ®å‡†å¤‡
                self.X_all, self.X_train, self.y_train, self.ret_train, self.X_test, self.y_test, self.ret_test, self.feature_names,self.open_train,self.open_test,self.close_train,self.close_test, self.z_index ,self.ohlc= dataload.data_prepare_rolling(
                    self.sym, self.freq, self.start_date_train, self.end_date_train,
                    self.start_date_test, self.end_date_test, rolling_w=self.rolling_window, output_format='ndarry',
                    data_dir=self.data_dir, read_frequency=self.read_frequency, timeframe=self.timeframe,
                    file_path=self.file_path, 
                    rolling_windows=getattr(self, 'rolling_windows', None),
                    use_rolling_aggregator=getattr(self, 'use_rolling_aggregator', True),
                    feature_types=getattr(self, 'feature_types', None),
                    feature_keywords=getattr(self, 'feature_keywords', None))
            elif str(self.data_source).lower() == 'coarse_grain':
                # æ–°å¢ï¼šç²—ç²’åº¦ç‰¹å¾ + ç»†ç²’åº¦æ»šåŠ¨ç‰ˆæ•°æ®å‡†å¤‡
                self.X_all, self.X_train, self.y_train, self.ret_train, self.X_test, self.y_test, self.ret_test, self.feature_names,self.open_train,self.open_test,self.close_train,self.close_test, self.z_index ,self.ohlc, self.y_p_train_origin, self.y_p_test_origin= dataload.data_prepare_coarse_grain_rolling(
                    self.sym, self.freq, self.start_date_train, self.end_date_train,
                    self.start_date_test, self.end_date_test, 
                    coarse_grain_period=getattr(self, 'coarse_grain_period', '2h'),
                    feature_lookback_bars=getattr(self, 'feature_lookback_bars', 8),
                    rolling_step=getattr(self, 'rolling_step', '10min'),
                    y_train_ret_period=self.y_train_ret_period,
                    rolling_w=self.rolling_window, 
                    output_format='ndarry',
                    data_dir=self.data_dir, 
                    read_frequency=self.read_frequency, 
                    timeframe=self.timeframe,
                    file_path=self.file_path,
                    include_categories = getattr(self, 'include_categories', None))
            elif str(self.data_source).lower() == 'thick_rolling':
                self.X_all, self.X_train, self.y_train, self.ret_train, self.X_test, self.y_test, self.ret_test, self.feature_names,self.open_train,self.open_test,self.close_train,self.close_test, self.z_index ,self.ohlc= dataload.data_thick_rolling_prepare(
                    self.sym, self.freq, self.start_date_train, self.end_date_train,
                    self.start_date_test, self.end_date_test, y_train_ret_period=self.y_train_ret_period, rolling_w=self.rolling_window, data_dir=self.data_dir, read_frequency=self.read_frequency, timeframe=self.timeframe, file_path=self.file_path)
            else:
                self.X_all, self.X_train, self.y_train, self.ret_train, self.X_test, self.y_test, self.ret_test, self.feature_names,self.open_train,self.open_test,self.close_train,self.close_test, self.z_index ,self.ohlc= dataload.data_prepare(
                    self.sym, self.freq, self.start_date_train, self.end_date_train,
                    self.start_date_test, self.end_date_test, y_train_ret_period=self.y_train_ret_period, rolling_w=self.rolling_window, data_dir=self.data_dir, read_frequency=self.read_frequency, timeframe=self.timeframe, file_path=self.file_path)
            self.data_initialized = True
            self.test_index = self.z_index[(self.z_index >= pd.to_datetime(self.start_date_test)) & (
                        self.z_index <= pd.to_datetime(self.end_date_test))]
            self.train_index = self.z_index[(self.z_index >= pd.to_datetime(self.start_date_train)) & (
                        self.z_index < pd.to_datetime(self.end_date_train))]           
        else:
            print("Shared data already initialized. Skipping data loading.")



    def gp(self, X, y, feature_names, random_state):
        """
        æ‰§è¡Œé—ä¼ ç¼–ç¨‹è¿‡ç¨‹ã€‚

        Args:
            X: è®­ç»ƒæ•°æ®ç‰¹å¾ã€‚
            y: è®­ç»ƒæ•°æ®æ ‡ç­¾ã€‚
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨ã€‚
            random_state: éšæœºçŠ¶æ€ã€‚

        Returns:
            SymbolicTransformer: è®­ç»ƒå¥½çš„é—ä¼ ç¼–ç¨‹æ¨¡å‹ã€‚
        """
        func = list(_function_map.keys())

        # è½¬æ¢åˆ—è¡¨ä¸ºå…ƒç»„
        self.gp_settings['init_depth'] = tuple(self.gp_settings['init_depth'])
        

        ST_gplearn = SymbolicTransformer(
            population_size=self.gp_settings.get('population_size', 5),
            hall_of_fame=self.gp_settings.get('hall_of_fame', 2),
            n_components=self.gp_settings.get('n_components', 1),
            generations=self.gp_settings.get('generations', 2),
            tournament_size=self.gp_settings.get('tournament_size', 2),
            const_range=self.gp_settings.get('const_range', None),
            init_depth=self.gp_settings.get('init_depth', (2, 5)),
            function_set=func,
            metric=self.metric,
            parsimony_coefficient=self.gp_settings.get('parsimony_coefficient', 0),
            p_crossover=self.gp_settings.get('p_crossover', 0.9),
            p_subtree_mutation=self.gp_settings.get('p_subtree_mutation', 0.01),
            p_hoist_mutation=self.gp_settings.get('p_hoist_mutation', 0.01),
            p_point_mutation=self.gp_settings.get('p_point_mutation', 0.01),
            p_point_replace=self.gp_settings.get('p_point_replace', 0.4),
            feature_names=feature_names,
            n_jobs=self.gp_settings.get('n_jobs', -1),
            corrcoef_threshold=self.gp_settings.get('corrcoef_threshold', 0.9),
            random_state=random_state,
            verbose=2
            )

        ST_gplearn.fit(X, y)
        return ST_gplearn




    def run_genetic_programming(self, random_state=None):
        """
        fct_generateçš„ä¸€éƒ¨åˆ†
        æ‰§è¡Œé—ä¼ ç¼–ç¨‹è¿‡ç¨‹ã€‚
        
        âœ… æ”¹è¿›ï¼šç›´æ¥ä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¿æŒæ•°å€¼ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚
        æ ‡å‡†åŒ–çš„ä½œç”¨å°±æ˜¯è®©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ ï¼Œä¸åº”è¯¥åœ¨è®­ç»ƒå‰åæ ‡å‡†åŒ–ã€‚
        è¯„ä¼°æŒ‡æ ‡ï¼ˆICã€Sharpeç­‰ï¼‰åœ¨æ ‡å‡†åŒ–æ•°æ®ä¸Šè®¡ç®—æ˜¯åˆç†çš„ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç›¸å¯¹æŒ‡æ ‡ã€‚
        """
        # æ ¹æ® metric ç±»å‹é€‰æ‹©ä½¿ç”¨å“ªä¸ª label è¿›è¡Œè®­ç»ƒ
        print(f"\nğŸ“Š å¼€å§‹è®­ç»ƒé—ä¼ ç¼–ç¨‹æ¨¡å‹ (metric={self.metric})")
        print(f"   âœ… ä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆæ•°å€¼æ›´ç¨³å®šã€æ”¶æ•›æ›´å¿«ï¼‰")
        print(f"   - è®­ç»ƒæ•°æ®å½¢çŠ¶: X_train={self.X_train.shape}")
        print(f"   - X_trainç»Ÿè®¡: å‡å€¼={np.mean(self.X_train):.6f}, æ ‡å‡†å·®={np.std(self.X_train):.6f}")
        
        if self.metric in norm_y_list:
            # IC ç±»æŒ‡æ ‡ä½¿ç”¨ y_train
            print(f"   - ä½¿ç”¨ y_train ä½œä¸ºæ ‡ç­¾")
            print(f"   - y_trainç»Ÿè®¡: å‡å€¼={np.mean(self.y_train):.6f}, æ ‡å‡†å·®={np.std(self.y_train):.6f}")
            self.est_gp = self.gp(self.X_train, self.y_train, feature_names=self.feature_names, random_state=random_state)
        else:
            # Sharpe ç±»æŒ‡æ ‡ä½¿ç”¨ ret_train
            print(f"   - ä½¿ç”¨ ret_train ä½œä¸ºæ ‡ç­¾")
            print(f"   - ret_trainç»Ÿè®¡: å‡å€¼={np.mean(self.ret_train):.6f}, æ ‡å‡†å·®={np.std(self.ret_train):.6f}")
            self.est_gp = self.gp(self.X_train, self.ret_train, feature_names=self.feature_names, random_state=random_state)
        
        print(f"âœ“ é—ä¼ ç¼–ç¨‹è®­ç»ƒå®Œæˆ")
           
        
        

    def process_best_programs(self):
        """
        fct_generateçš„ä¸€éƒ¨åˆ†
        å¤„ç†æœ€ä½³ç¨‹åºå¹¶åˆ›å»ºæ•°æ®æ¡†ã€‚
        """
        best_programs = self.est_gp._best_programs
        best_programs_dict = {}
        fitness_key = f"fitness_{self.metric}"

        for p in best_programs:
            factor_expression = 'alpha_' + str(best_programs.index(p) + 1)
            best_programs_dict[factor_expression] = {fitness_key: p.fitness_, 'expression': str(p), 'depth': p.depth_,
                                               'length': p.length_}

        self.best_programs_df = pd.DataFrame(best_programs_dict).T
        self.best_programs_df['factor_order_in_model'] = self.best_programs_df.index
        self.best_programs_df = self.best_programs_df.sort_values(by=fitness_key, ascending=False)

    def save_initial_results(self):
        """
        fct_generateçš„ä¸€éƒ¨åˆ†
        ä¿å­˜åˆæ­¥ç»“æœåˆ°æ–‡ä»¶ã€‚
        """
        current_time = datetime.now().strftime('%Y%m%d%H%M%S')
        model_folder_name = f"{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}_{self.metric}_{current_time}"
        base_model_directory = Path.cwd() / 'gp_models'
        self.model_folder = base_model_directory / model_folder_name
        self.model_folder.mkdir(parents=True, exist_ok=True)

        logger.info(f'ä¿å­˜æœ¬è½®ä¿¡æ¯å’Œç»“æœåˆ°å¯¹åº”æ–‡ä»¶å¤¹ {self.model_folder}')

        yaml_file = Path(self.yaml_file_path)
        shutil.copy(yaml_file, self.model_folder / yaml_file.name)

        self.best_programs_df['sym'] = self.sym
        self.best_programs_df['freq'] = self.freq
        self.best_programs_df['coarse_grain_period'] = self.coarse_grain_period
        self.best_programs_df['feature_lookback_bars'] = self.feature_lookback_bars
        self.best_programs_df['rolling_step'] = self.rolling_step
        self.best_programs_df['y_train_ret_period'] = self.y_train_ret_period
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…é•¿åº¦ä¸åŒ¹é…é”™è¯¯
        self.best_programs_df['include_categories'] = str(self.include_categories) if self.include_categories else None
        self.best_programs_df['start_date_train'] = self.start_date_train
        self.best_programs_df['end_date_train'] = self.end_date_train
        self.best_programs_df['start_date_test'] = self.start_date_test
        self.best_programs_df['end_date_test'] = self.end_date_test
        self.best_programs_df['metric'] = self.metric
        self.best_programs_df['current_time'] = current_time

        self.best_programs_df.to_csv(self.model_folder / 'best_programs_df.csv.gz', index=False, compression='gzip')



    def load_total_factor_df(self):
        """
        åŠ è½½æ€»å› å­æ•°æ®æ¡†ã€‚
        """
        if not self.total_factor_file_path.exists():
            print(f"Factor file {self.total_factor_file_path} does not exist.")
            return None

        return pd.read_csv(self.total_factor_file_path, compression='gzip')
 
    def evaluate_single_factor(self, factor_expression, metric):
        """
        è¯„ä¼°å•ä¸ªå› å­çš„è¡¨ç°

        åªç”Ÿæˆä¸€ä¸ªevaluatorå¯¹è±¡ï¼Œç„¶åç”¨è¿™ä¸ªevaluatoræ¥è¯„ä¼°trainå’Œtestçš„è¡¨ç°(æˆªæ–­) -  å› ä¸ºè¦å…¼å®¹é‚£äº›å¯¹äºstart_dateæ•æ„Ÿçš„å› å­
        """
        # self.X_allå·²ç»æ˜¯æ ¹æ®train_start_dateå’Œtest_end_dateæˆªæ–­è¿‡çš„æ•°æ®
        evaluator = FeatureEvaluator(_function_map, self.feature_names, self.X_all)

        result = evaluator.evaluate(factor_expression)  # è§£æå› å­
        result = np.nan_to_num(result)
        
        # è°ƒè¯•ï¼šæ£€æŸ¥é•¿åº¦
        expected_len = len(self.y_train) + len(self.y_test)
        if len(result) != expected_len:
            print(f"âš ï¸  é•¿åº¦ä¸åŒ¹é…è­¦å‘Š:")
            print(f"   len(result) = {len(result)}")
            print(f"   len(X_all) = {len(self.X_all)}")
            print(f"   len(y_train) + len(y_test) = {len(self.y_train)} + {len(self.y_test)} = {expected_len}")
            print(f"   å·®å€¼ = {len(result) - expected_len}")
        
        # è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‹†åˆ†
        result_train, result_test = result[:len(self.y_train)], result[len(self.y_train):]
        
        if metric in norm_y_list :
            fitness_train = fitness._fitness_map[metric](self.y_train, pd.Series(result_train), np.ones(len(self.y_train)))
            fitness_test = fitness._fitness_map[metric](self.y_test, pd.Series(result_test), np.ones(len(self.y_test)))
        else:
            fitness_train = fitness._fitness_map[metric](self.ret_train, pd.Series(result_train), np.ones(len(self.y_train)))
            fitness_test = fitness._fitness_map[metric](self.ret_test, pd.Series(result_test), np.ones(len(self.y_test)))
       
        return fitness_train, fitness_test    
    
    def evaluate_single_factor_given_ic(self, factor_expression):
        """
        è¯„ä¼°å•ä¸ªå› å­çš„è¡¨ç°

        åªç”Ÿæˆä¸€ä¸ªevaluatorå¯¹è±¡ï¼Œç„¶åç”¨è¿™ä¸ªevaluatoræ¥è¯„ä¼°trainå’Œtestçš„è¡¨ç°(æˆªæ–­) -  å› ä¸ºè¦å…¼å®¹é‚£äº›å¯¹äºstart_dateæ•æ„Ÿçš„å› å­
        """
        # self.X_allå·²ç»æ˜¯æ ¹æ®train_start_dateå’Œtest_end_dateæˆªæ–­è¿‡çš„æ•°æ®
        evaluator = FeatureEvaluator(_function_map, self.feature_names, self.X_all)

        result = evaluator.evaluate(factor_expression)  # è§£æå› å­
        result = np.nan_to_num(result)
        # è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‹†åˆ†
        result_train, result_test = result[:len(self.y_train)], result[len(self.y_train):]

        max_ic_train, up_r, dn_r = fitness._fitness_map['max_ic_train'](self.y_train, pd.Series(result_train), np.ones(len(self.y_train)))
        given_ic_test  = fitness._fitness_map['given_ic_test'](self.y_test, pd.Series(result_test), np.ones(len(self.y_test)), up_r, dn_r)

        return max_ic_train, given_ic_test 
 
    def remove_duplicate_columns(self, df):
        """
        ç§»é™¤DataFrameä¸­çš„é‡å¤åˆ—ã€‚

        Args:
            df: è¾“å…¥çš„DataFrameã€‚

        Returns:
            DataFrame: ç§»é™¤é‡å¤åˆ—åçš„DataFrameã€‚
        """
        return df.loc[:, ~df.columns.duplicated()]

    def evaluate_single_factor_for_new_genes(self, factor_expression):
        """
        è¯„ä¼°å•ä¸ªæ–°ç”Ÿæˆå› å­çš„è¡¨ç°ã€‚
        """
        all_metrics = list(fitness._fitness_map.keys())
        all_metrics = [item for item in all_metrics if item not in ['max_ic_train', 'given_ic_test']]
        
        for metric in all_metrics:
            fitness_train, fitness_test = self.evaluate_single_factor(str(factor_expression), metric)

            print(f"metric={metric}, expression_fitness_train = {fitness_train}")
            print(f"metric={metric}, expression_fitness_test = {fitness_test}")

            self.best_programs_df_dedup.loc[
                self.best_programs_df_dedup['expression'] == factor_expression, f'fitness_{metric}_train'] = fitness_train
            self.best_programs_df_dedup.loc[
                self.best_programs_df_dedup['expression'] == factor_expression, f'fitness_{metric}_test'] = fitness_test
            
            
        max_ic_train_check, given_ic_test  = self.evaluate_single_factor_given_ic(factor_expression)  
        self.best_programs_df_dedup.loc[
            self.best_programs_df_dedup['expression'] == factor_expression, 'max_ic_train_check'] = max_ic_train_check
        self.best_programs_df_dedup.loc[
            self.best_programs_df_dedup['expression'] == factor_expression, 'given_ic_test'] = given_ic_test    
 
   
    def evaluate_factors(self):
        """
        è¯„ä¼°æ–°ç”Ÿæˆçš„å› å­å¹¶è®¡ç®—ä¸åŒæŒ‡æ ‡çš„è¡¨ç°ã€‚
        """
        self.best_programs_df_dedup = self.best_programs_df.drop_duplicates(subset=['expression'], keep='first')

        factor_expressions = [str(prog) for prog in self.est_gp._best_programs]
        
        # ===== è°ƒè¯•ä»£ç å¼€å§‹ =====
        print(f"\n{'='*80}")
        print(f"ğŸ” å®Œæ•´è¯Šæ–­ï¼šGP è¾“å‡ºä¸º 0 çš„åŸå› ")
        print(f"{'='*80}\n")
        
        # 1. æ£€æŸ¥è¾“å…¥æ•°æ®
        print(f"1. è¾“å…¥æ•°æ®æ£€æŸ¥:")
        print(f"   X_train å½¢çŠ¶: {self.X_train.shape}")
        print(f"   X_train ç»Ÿè®¡:")
        print(f"     - å‡å€¼: {np.mean(self.X_train):.6f}")
        print(f"     - æ ‡å‡†å·®: {np.std(self.X_train):.6f}")
        print(f"     - æœ€å°å€¼: {np.min(self.X_train):.6f}")
        print(f"     - æœ€å¤§å€¼: {np.max(self.X_train):.6f}")
        print(f"     - é›¶å€¼å æ¯”: {np.sum(self.X_train == 0) / self.X_train.size * 100:.2f}%")
        print(f"     - NaNå æ¯”: {np.sum(np.isnan(self.X_train)) / self.X_train.size * 100:.2f}%")
        print(f"     - Infå æ¯”: {np.sum(np.isinf(self.X_train)) / self.X_train.size * 100:.2f}%")
        
        # æ£€æŸ¥æ¯åˆ—ç‰¹å¾
        print(f"\n   å„ç‰¹å¾ç»Ÿè®¡ï¼ˆå‰ 10 ä¸ªï¼‰:")
        for i in range(min(10, self.X_train.shape[1])):
            col = self.X_train[:, i]
            feat_name = self.feature_names[i] if i < len(self.feature_names) else f'unknown_{i}'
            print(f"   ç‰¹å¾ {i} ({feat_name}):")
            print(f"     å‡å€¼={np.mean(col):.6f}, std={np.std(col):.6f}, "
                  f"min={np.min(col):.6f}, max={np.max(col):.6f}, "
                  f"é›¶å€¼å æ¯”={np.sum(col == 0)/len(col)*100:.1f}%")
        
        print(f"\n   X_train å‰ 3 è¡Œ, å‰ 5 åˆ—:")
        print(self.X_train[:3, :5])
        
        # 2. æ£€æŸ¥ GP ç¨‹åº
        print(f"\n2. GP ç¨‹åºæ£€æŸ¥:")
        print(f"   _best_programs æ•°é‡: {len(self.est_gp._best_programs)}")
        
        if len(self.est_gp._best_programs) > 0:
            for i, prog in enumerate(self.est_gp._best_programs[:5]):
                print(f"\n   ç¨‹åº {i+1}:")
                print(f"   - è¡¨è¾¾å¼: {str(prog)}")
                print(f"   - fitness: {prog.fitness_}")
                print(f"   - depth: {prog.depth_}")
                print(f"   - length: {prog.length_}")
                print(f"   - program ç»“æ„: {prog.program[:min(10, len(prog.program))]}...")
                
                # æ‰‹åŠ¨æ‰§è¡Œ
                try:
                    result = prog.execute(self.X_train)
                    if result is not None:
                        print(f"   - æ‰§è¡Œç»“æœ:")
                        print(f"     * ç±»å‹: {type(result)}")
                        print(f"     * å½¢çŠ¶: {result.shape if hasattr(result, 'shape') else 'N/A'}")
                        print(f"     * å‡å€¼: {np.mean(result):.6f}")
                        print(f"     * æ ‡å‡†å·®: {np.std(result):.6f}")
                        print(f"     * æœ€å°å€¼: {np.min(result):.6f}")
                        print(f"     * æœ€å¤§å€¼: {np.max(result):.6f}")
                        print(f"     * æ˜¯å¦å…¨ä¸º0: {np.all(result == 0)}")
                        print(f"     * NaNæ•°é‡: {np.sum(np.isnan(result))}")
                        print(f"     * éé›¶å€¼æ•°é‡: {np.sum(result != 0)}")
                        print(f"     * å‰ 20 ä¸ªå€¼: {result[:20]}")
                    else:
                        print(f"   - æ‰§è¡Œç»“æœ: None âŒ")
                except Exception as e:
                    print(f"   - æ‰§è¡Œå‡ºé”™: {type(e).__name__}: {e}")
                    import traceback
                    traceback.print_exc()
        else:
            print(f"âš ï¸  è­¦å‘Š: _best_programs ä¸ºç©ºï¼")
        
        # 3. æµ‹è¯• norm() å‡½æ•°
        # print(f"\n3. æµ‹è¯• norm() å‡½æ•°:")
        # from functions import norm
        
        # # æµ‹è¯•ç”¨ä¾‹ 1ï¼šæ­£å¸¸éšæœºæ•°æ®
        # test_data_1 = np.random.randn(1000) * 10 + 5
        # result_1 = norm(test_data_1, rolling_window=100)
        # print(f"   æµ‹è¯• 1 (æ­£å¸¸éšæœºæ•°æ®):")
        # print(f"     è¾“å…¥: å‡å€¼={np.mean(test_data_1):.6f}, std={np.std(test_data_1):.6f}")
        # print(f"     è¾“å‡º: å‡å€¼={np.mean(result_1):.6f}, std={np.std(result_1):.6f}, å…¨ä¸º0={np.all(result_1 == 0)}")
        
        # # æµ‹è¯•ç”¨ä¾‹ 2ï¼šå¸¸æ•°
        # test_data_2 = np.ones(1000) * 5
        # result_2 = norm(test_data_2, rolling_window=100)
        # print(f"   æµ‹è¯• 2 (å¸¸æ•°æ•°æ®):")
        # print(f"     è¾“å…¥: å‡å€¼={np.mean(test_data_2):.6f}, std={np.std(test_data_2):.6f}")
        # print(f"     è¾“å‡º: å‡å€¼={np.mean(result_2):.6f}, std={np.std(result_2):.6f}, å…¨ä¸º0={np.all(result_2 == 0)}")
        
        # # æµ‹è¯•ç”¨ä¾‹ 3ï¼šä½¿ç”¨å®é™…ç‰¹å¾
        # if self.X_train.shape[0] >= 100 and self.X_train.shape[1] > 0:
        #     test_data_3 = self.X_train[:, 0]
        #     result_3 = norm(test_data_3, rolling_window=min(100, len(test_data_3)//2))
        #     print(f"   æµ‹è¯• 3 (å®é™…ç‰¹å¾ 0):")
        #     print(f"     è¾“å…¥: å‡å€¼={np.mean(test_data_3):.6f}, std={np.std(test_data_3):.6f}")
        #     print(f"     è¾“å‡º: å‡å€¼={np.mean(result_3):.6f}, std={np.std(result_3):.6f}, å…¨ä¸º0={np.all(result_3 == 0)}")
        
        # # 4. æµ‹è¯• transform()
        # print(f"\n4. æµ‹è¯• transform() æ–¹æ³•:")
        # try:
        #     # æ‰‹åŠ¨æ‰§è¡Œæ¯ä¸ªç¨‹åº
        #     manual_results = []
        #     for i, prog in enumerate(self.est_gp._best_programs[:min(3, len(self.est_gp._best_programs))]):
        #         result = prog.execute(self.X_train)
        #         manual_results.append(result)
        #         print(f"   ç¨‹åº {i+1} æ‰§è¡Œç»“æœ: å‡å€¼={np.mean(result):.6f}, å…¨ä¸º0={np.all(result==0)}")
            
        #     # æ‰‹åŠ¨ç»„åˆ
        #     if len(manual_results) > 0:
        #         manual_transform = np.array(manual_results).T
        #         print(f"\n   æ‰‹åŠ¨ transform ç»“æœ:")
        #         print(f"     å½¢çŠ¶: {manual_transform.shape}")
        #         print(f"     å‡å€¼: {np.mean(manual_transform):.6f}")
        #         print(f"     æ˜¯å¦å…¨ä¸º0: {np.all(manual_transform == 0)}")
        # except Exception as e:
        #     print(f"   æµ‹è¯•å‡ºé”™: {type(e).__name__}: {e}")
        
        # print(f"\n{'='*80}\n")
        # ===== è°ƒè¯•ä»£ç ç»“æŸ =====
        
        factors_pred_train = self.est_gp.transform(self.X_train)
        factors_pred_test = self.est_gp.transform(self.X_test)
        
        # ===== ç»§ç»­è°ƒè¯• =====
        print(f"\n5. transform() çš„ç»“æœ:")
        print(f"   - factors_pred_train å½¢çŠ¶: {factors_pred_train.shape}")
        print(f"   - factors_pred_train ç»Ÿè®¡:")
        print(f"     * å‡å€¼: {np.mean(factors_pred_train):.6f}")
        print(f"     * æ ‡å‡†å·®: {np.std(factors_pred_train):.6f}")
        print(f"     * æœ€å°å€¼: {np.min(factors_pred_train):.6f}")
        print(f"     * æœ€å¤§å€¼: {np.max(factors_pred_train):.6f}")
        print(f"     * æ˜¯å¦å…¨ä¸º0: {np.all(factors_pred_train == 0)}")
        print(f"   - å‰3è¡Œæ•°æ®:")
        print(factors_pred_train[:3])
        print(f"=" * 80 + "\n")

        self.pred_data_df_train = pd.DataFrame(factors_pred_train, columns=factor_expressions)
        self.pred_data_df_test = pd.DataFrame(factors_pred_test, columns=factor_expressions)

        self.pred_data_df_train = self.remove_duplicate_columns(self.pred_data_df_train)
        self.pred_data_df_test = self.remove_duplicate_columns(self.pred_data_df_test)

        logger.info('ä½¿ç”¨evalè§£ææœ¬è½®çš„å› å­è¡¨è¾¾å¼ï¼Œå¹¶ä½¿ç”¨æ‰€æœ‰çš„metricæ–¹æ³•è·‘ä¸€è½®fitness')
        for factor_expression in self.pred_data_df_train.columns:
            self.evaluate_single_factor_for_new_genes(factor_expression)

        self.best_programs_df_dedup.to_csv(self.model_folder / 'best_programs_df.csv.gz', index=False,
                                           compression='gzip')



    def save_final_results(self):
        """
        fct_generateçš„ä¸€éƒ¨åˆ†
        ä¿å­˜æœ€ç»ˆç»“æœåˆ°æ€»è¡¨ä¸­ã€‚
        """

        if self.total_factor_file_path.exists():
            total_factor_df = pd.read_csv(self.total_factor_file_path, compression='gzip')
            total_factor_df = pd.concat([total_factor_df, self.best_programs_df_dedup], ignore_index=True).drop_duplicates(
                subset=['expression'], keep='first')
        else:
            total_factor_df = self.best_programs_df_dedup

        total_factor_df.to_csv(self.total_factor_file_path, index=False, compression='gzip')
        print(f"saved {self.total_factor_file_path}")
        print("Factor generation process completed.")

    def fct_generate(self, random_state=None):
        """
        ç”Ÿæˆå› å­å¹¶è¿›è¡Œåˆæ­¥è¯„ä¼°ã€‚
        """
        logger.info('----å¼€å§‹æ‰§è¡Œ----')

        self.run_genetic_programming(random_state)
        self.process_best_programs()
        self.save_initial_results()
        self.evaluate_factors()
        self.save_final_results()

    def execute_task(self, random_state=None):
        """
        æ‰§è¡Œå•ä¸ªé—ä¼ ç¼–ç¨‹ä»»åŠ¡ã€‚
        """
        try:
            self.fct_generate(random_state)
            print("Task completed successfully.")
        except Exception as e:
            print(f"An error occurred: {str(e)}")


    def run(self):
        """
        æ ¹æ®é…ç½®è¿è¡Œä»»åŠ¡ï¼Œæ”¯æŒå•æ¬¡æ‰§è¡Œå’Œå¾ªç¯æ‰§è¡Œæ¨¡å¼ã€‚
        """
        execution_mode = self.config.get('execution_mode', 'once')

        if execution_mode == 'once':
            self.execute_task()
        elif execution_mode == 'loop':
            interval = self.config.get('execution_interval', 30)  # é»˜è®¤30ç§’
            print(f"Starting loop mode with {interval} seconds interval.")
            schedule.every(interval).seconds.do(self.execute_task)
            while True:
                schedule.run_pending()
                time.sleep(1)
        else:
            print(f"Unknown execution mode: {execution_mode}")




 
    
    
    def save_total_factor_df(self, total_factor_df):
        """
        ä¿å­˜æ€»å› å­æ•°æ®æ¡†ã€‚
        """
        total_factor_df.to_csv(self.total_factor_file_path, index=False, compression='gzip')
        print(f"Updated factor evaluations saved to {self.total_factor_file_path}")    
    
    def evaluate_existing_factors(self):
        """
        ** ç‹¬ç«‹äºgplearnï¼Œå•ç‹¬è¿è¡Œ.
        ç›´æ¥è¯»å–å½“å‰å› å­åº“çš„total_factor_dfæ–‡ä»¶ï¼Œå¯¹æ‰€æœ‰å› å­è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ›´æ–°æ–‡ä»¶.
        è¯„ä¼°ç°æœ‰å› å­åº“ä¸­çš„æ‰€æœ‰å› å­
        """

        total_factor_df = self.load_total_factor_df()
        if total_factor_df is not None:

            all_metrics = list(fitness._fitness_map.keys())
            all_metrics = [item for item in all_metrics if item not in ['max_ic_train', 'given_ic_test']]
            for index, row in total_factor_df.iterrows():
                factor_expression = row['expression']
                for metric in all_metrics:
                    train_col = f'fitness_{metric}_train'
                    test_col = f'fitness_{metric}_test'

                    if train_col not in total_factor_df.columns or test_col not in total_factor_df.columns:
                        fitness_train, fitness_test = self.evaluate_single_factor(factor_expression, metric)
                        total_factor_df.loc[index, train_col] = fitness_train
                        total_factor_df.loc[index, test_col] = fitness_test
                max_ic_train_check, given_ic_test  = self.evaluate_single_factor_given_ic(factor_expression)
                total_factor_df.loc[index, 'max_ic_train_check'] = max_ic_train_check
                total_factor_df.loc[index, 'given_ic_test'] = given_ic_test

            self.save_total_factor_df(total_factor_df)

  

    def read_and_cal_metrics(self):
        '''
        è¯»å–ä¹‹å‰ç”Ÿæˆçš„å› å­å€¼ï¼Œå¹¶è®¡ç®—æ¯ä¸€ä¸ªå› å­å€¼çš„metric
        æ³¨æ„è¿™é‡Œè®¡ç®—çš„metricéƒ½æ˜¯è¿”å›ä¸€ä¸ªå€¼çš„'''
        z = pd.read_csv('/Users/aming/project/python/crypto-workstation/gp_models/ETHUSDT_15m_1_2025-01-01_2025-01-20_2025-01-20_2025-01-31.csv.gz')
        # z = pd.read_csv('/home/etern/crypto/gp-crypto/elite_pool/factor_selected.csv')
        z.drop_duplicates(inplace=True)
        
        all_metrics = list(fitness._fitness_map.keys())
        all_metrics = [item for item in all_metrics if item not in ['max_ic_train', 'given_ic_test']]
        
        
        for index, row in z.iterrows():
            factor_expression = row['expression']
            for metric in all_metrics:
                train_col = f'fitness_{metric}_train'
                test_col = f'fitness_{metric}_test'

                try:
                    print(f'æ­¤æ—¶è¦è®¡ç®—{self.sym}çš„metricå¯¹åº”çš„å› å­æ˜¯{factor_expression}')
                    fitness_train, fitness_test = self.evaluate_single_factor(factor_expression, metric)
                    z.loc[index, train_col] = fitness_train
                    z.loc[index, test_col] = fitness_test
                except Exception as e:
                    print(e)
            max_ic_train_check, given_ic_test  = self.evaluate_single_factor_given_ic(factor_expression)
            z.loc[index, 'max_ic_train_check'] = max_ic_train_check
            z.loc[index, 'given_ic_test'] = given_ic_test
                
        # ä¿å­˜åˆ°gp_modelsæ–‡ä»¶å¤¹ä¸‹
        self.save_total_factor_df(z)
        

    def read_and_pick(self):
        # éå†å› å­çš„æ€»è¡¨ï¼Œæ³¨æ„æ˜¯æ€»è¡¨
        elite_pool = []
        csv_path = f'gp_models/{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}.csv.gz'
        # è¯»å–å› å­å€¼
        z = pd.read_csv(csv_path)

        # éå†æ¯ä¸€ä¸ªå› å­ï¼Œå¦‚æœç¬¦åˆæ¡ä»¶ï¼Œå°±åŠ å…¥elite poolä¸­
        for index,row in z.iterrows():
            '''
            æ­¤å¤„å†™ç­›é€‰å› å­çš„é€»è¾‘
            
            '''
            cond1 = (row['fitness_sharp_train'] > 2)
            cond2 = (row['fitness_sharp_test'] > 2)  
            cond3 = (row['fitness_avg_pic_train'] > 0.005)
            cond4 = (row['fitness_avg_pic_test'] > 0.005)
            try:
                # è¿™é‡Œå†™å…¥ç­›é€‰å› å­çš„é€»è¾‘ï¼Œæ ¹æ®ä¸åŒçš„metricç­›é€‰å‡ºå› å­æ± ï¼Œè¿›è¡Œæ‹Ÿåˆ
                if cond1 and cond2 and cond3 and cond4:
                    # è®°å½•ç¬¦åˆæ ‡å‡†çš„expression
                    elite_pool.append(row['expression'])
            except Exception as e:
                    print(f'an error occurred with {e}')
        elite_pool_size = len(elite_pool)
        print(f'æŒ‘é€‰å‡ºæ¥{self.sym}çš„å› å­æ± æ•°é‡{elite_pool_size}')
        return elite_pool


    def plot_and_save_three_series(self,price_train, price_test, pnl_train, pnl_test, rs_train, rs_test ,title, index) -> None:
        fig, axs = plt.subplots(3, 2, figsize=(10, 12))
        fig.suptitle(f'{self.sym}_{index}_{title}', fontsize=16)

        axs[0, 0].plot(self.train_index,price_train, 'r-')
        axs[0, 0].set_title('price_train')
        axs[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[0, 0].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”

        axs[1, 0].plot(self.train_index,pnl_train, 'g-')
        axs[1, 0].set_title('pnl_train')
        axs[1, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[1, 0].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”

        axs[2, 0].plot(self.train_index,rs_train, 'b-')
        axs[2, 0].set_title('rolling sharp_train')
        axs[2, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[2, 0].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”

        axs[0, 1].plot(self.test_index,price_test, 'r-')
        axs[0, 1].set_title('price_test')
        axs[0, 1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[0, 1].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”

        axs[1, 1].plot(self.test_index,pnl_test, 'g-')
        axs[1, 1].set_title('pnl_test')
        axs[1, 1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[1, 1].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”

        axs[2, 1].plot(self.test_index,rs_test, 'b-')
        axs[2, 1].set_title('rolling sharp_test')
        axs[2, 1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[2, 1].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”

        plt.xticks(rotation=45)
        plt.tight_layout()
        

        # ç”Ÿæˆæ–‡ä»¶åå¹¶ä¿å­˜
        current_date = datetime.now().strftime("%Y%m%d")
        print('ä¿å­˜æ”¶ç›˜ä»·ã€å› å­çš„rolling sharpå’Œå› å­çš„pnl')
        output_dir = Path(f"{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}/factor_drawings")
        output_dir.mkdir(parents=True, exist_ok=True)
        filename = output_dir / f"factor_{index}_.png"
        plt.savefig(filename)
        # å…³é—­å›¾åƒçª—å£
        plt.close(fig)

        print(f"å› å­çš„pnlå’Œrolling sharpå·²æˆåŠŸä¿å­˜")


    def hist_draw(self,factor_series,index):
        '''ç»˜åˆ¶å› å­çš„åˆ†å¸ƒå›¾'''

        plt.figure(figsize=(10, 6))
        pd.Series(factor_series).hist(bins=100, edgecolor='green')
        plt.title('Distribution of single factor')
        plt.xlabel('Value')
        plt.ylabel('Frequency')
        output_dir = Path(f"{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}/factor_drawings")
        output_dir.mkdir(parents=True, exist_ok=True)
        file_path = output_dir / f"fcthist_{index}.png"
        plt.savefig(file_path)
        plt.close()
        print(f"-------------------å› å­{index}çš„åˆ†å¸ƒå›¾å·²ä¿å­˜æˆåŠŸï¼ï¼ï¼-----------------------")

    def backtest_single_factor(self, factor_expression, metric, index, df):
        """
        è¯„ä¼°å•ä¸ªå› å­çš„ç®€å•å›æµ‹ï¼Œè¿”å›åºåˆ—
        """
        # self.X_allå·²ç»æ˜¯æ ¹æ®train_start_dateå’Œtest_end_dateæˆªæ–­è¿‡çš„æ•°æ®
        evaluator = FeatureEvaluator(_function_map, self.feature_names, self.X_all)

        factor_series = evaluator.evaluate(factor_expression)  # è§£æå› å­
        #ç”»å› å­åˆ†å¸ƒ
        self.hist_draw(factor_series,index)
        #ç”»å› å­çš„ic_decay
        pd.Series({n:np.corrcoef(factor_series,df[f'return+{n}'].values)[0,1] for n in range(1,50,2)}).plot()
        plt.title(f"{factor_expression}")
        plt.xlabel("Lag (n)")
        plt.ylabel("ic")
        output_dir = Path(f"{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}/factor_drawings")
        output_dir.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_dir / f"ic_deacy_{index}.png")  # ä¿å­˜ä¸º PNG æ–‡ä»¶
        plt.close()
        
        # è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‹†åˆ†
        result_train, result_test = factor_series[:len(self.y_train)], factor_series[len(self.y_train):]
        fitness_train = fitness._backtest_map[metric](self.ret_train, pd.Series(result_train), np.ones(len(self.y_train)))
        fitness_test = fitness._backtest_map[metric](self.ret_test, pd.Series(result_test), np.ones(len(self.y_test)))

        return fitness_train, fitness_test
    
    
    def elite_factors_further_process(self,cal_new_metric=False):
        # ç¬¬ä¸€æ­¥è§£æä¹‹å‰ç”Ÿæˆçš„çš„å› å­ï¼Œå¹¶è®¡ç®—å„ä¸ªå› å­çš„metricsï¼Œæœ€åç”Ÿæˆä¸€ä¸ªè¡¨æ ¼ï¼Œåªç”¨è®¡ç®—ä¸€æ¬¡
        # è®¡ç®—æ¯ä¸ªå› å­çš„metricsï¼Œç”Ÿæˆä¸€ä¸ªè¡¨æ ¼ï¼Œç”¨äºåé¢çš„ç­›é€‰è¿‡ç¨‹
        if cal_new_metric:
            self.read_and_cal_metrics()
        # ç¬¬äºŒæ­¥è¯»å–ä¸Šä¸€æ­¥ç”Ÿæˆçš„è¡¨æ ¼ï¼Œå¹¶ç­›é€‰åˆé€‚çš„å› å­
        elite_pool = self.read_and_pick()
        z = pd.DataFrame()
        #å‡†å¤‡ic_decayçš„æ•°æ®
        space = range(1,50,2)
        # å…¼å®¹ä¸åŒçš„åˆ—åï¼š'c' æˆ– 'close'
        close_col = 'c' if 'c' in self.ohlc.columns else 'close'
        df = self.ohlc[close_col].to_frame()
        
        # éªŒè¯dfä¸X_allæ˜¯å¦å¯¹é½
        print(f"ohlc é•¿åº¦: {len(self.ohlc)}, X_all é•¿åº¦: {len(self.X_all)}")
        if len(df) != len(self.X_all):
            print(f"âš ï¸ è­¦å‘Šï¼šohlcä¸X_allé•¿åº¦ä¸ä¸€è‡´ï¼è¿™å¯èƒ½å¯¼è‡´ICè®¡ç®—é”™è¯¯ã€‚")
        
        # è®¡ç®—æœªæ¥æ”¶ç›Šï¼ˆç”¨äºIC decayåˆ†æï¼‰
        for i in space:
            df.loc[:,f'return+{i}'] = np.log(df.loc[:,close_col]).shift(-i) - np.log(df.loc[:,close_col])
            # df[f'return+{i}'] = np.where(df[f]>0,df[f'return+{i}']-fee, np.where(df[f]<0,df[f'return+{i}']+fee,0))
            df.replace([np.inf, -np.inf, np.nan], 0.0,inplace = True)      
            
        # éå†elite pool
        for index,i in enumerate(elite_pool):
            train_rs, test_rs = self.backtest_single_factor(i,'rolling_sharp',index,df)
            train_pnl,test_pnl = self.backtest_single_factor(i,'pnl',index,df)
            # TODOï¼šç”»å›¾å’Œç»Ÿè®¡å€¼åŒæ—¶å…·å¤‡
            close_train = pd.Series(self.close_train.reset_index(drop=True))
            close_test = pd.Series(self.close_test.reset_index(drop=True))
            # ä¸‹é¢å¼€å§‹ç”»å›¾
            try:
                self.plot_and_save_three_series(close_train,close_test,pd.Series(train_pnl),pd.Series(test_pnl), pd.Series(train_rs),pd.Series(test_rs),i, index)
                z = pd.DataFrame(elite_pool, columns=['expression'])
            except:
                pass
        # ä¿å­˜
        print('ä¸‹é¢ä¸€æ­¥ä¿å­˜ç­›é€‰å‡ºæ¥çš„å› å­å€¼')
        output_dir = Path(f'{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}')
        output_dir.mkdir(parents=True, exist_ok=True)
        z.to_csv(output_dir / f'factors_elite_{datetime.now().strftime("%Y%m%d-%H%M%S")}.csv')
        print('---------------------æ­¤è½®ç­›é€‰å‡ºæ¥çš„å› å­å€¼å·²ç»ä¿å­˜æˆåŠŸï¼ï¼----------------------------')
        return elite_pool


        
    def calculate_factors_values(self,factor_expression):
        evaluator = FeatureEvaluator(_function_map, self.feature_names, self.X_all)
        result = evaluator.evaluate(factor_expression)  # è§£æå› å­
        result_train, result_test = result[:len(self.y_train)], result[len(self.y_train):]
        return result_train,result_test

    def go_model(self,exp_pool):
        '''ç»„åˆå› å­ç”Ÿæˆmodel'''
        X_train,X_test = [],[]
        for i in exp_pool:
            X_train.append(self.calculate_factors_values(i)[0])
            X_test.append(self.calculate_factors_values(i)[1])
        X_train,X_test = np.array(X_train).T,np.array(X_test).T,
        model = LinearRegression()
        model.fit(X_train,self.ret_train.reshape(-1,1))
        # æ ¹æ®å†å²åˆ†ä½,å¹¶ç¡®å®šæ”¾å¤§ç¼©å°å€æ•°
        pos_train = model.predict(X_train).flatten()
        min_val = abs(np.percentile(pos_train, 99))
        max_val = abs(np.percentile(pos_train, 1))
        
        pos_ = model.predict(X_test).flatten()
        scale_n = 2/(min_val+max_val)
        # å¤§æ¦‚æ˜ å°„åˆ°åˆç†æ•´æ•°åŒºé—´
        pos_train = pos_train* scale_n
        pos_train = pos_train.clip(-5,5)
        
        pos = pos_ * scale_n
        pos = pos.clip(-5,5)
        output_dir = Path(f'{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}')
        output_dir.mkdir(parents=True, exist_ok=True)
        model_file = output_dir / 'model.pkl'
        with open(model_file, 'wb') as file:
            pickle.dump(model, file)
        print('æ¨¡å‹æ–‡ä»¶ä¿å­˜æˆåŠŸï¼')
        print(f'æ¨¡å‹ç³»æ•°{model.coef_},æ¨¡å‹çš„æˆªè·ä¸º{model.intercept_}')
        print(f'æŸ¥çœ‹pos{pos.shape}')
        return pos,pos_train

    def real_trading_simulator(self,pos:np.array, data_range = 'test', fee = 0.0005):
        '''æ¨¡æ‹ŸçœŸå®çš„äº¤æ˜“åœºæ™¯'''
        # è·å¾—ä¸‹ä¸€ä¸ªbarçš„open price,å’Œå½“å‰barçš„close price
        if data_range == 'train':
            next_open = np.concatenate((self.open_train[1:], np.array([0])))
            close = self.close_train
        elif data_range == 'test':
            next_open = np.concatenate((self.open_test[1:], np.array([0])))
            close = self.close_test
        else:
            open_all = pd.concat([self.open_train,self.open_test])
            next_open = np.concatenate((open_all[1:], np.array([0])))
            close =  pd.concat([self.close_train,self.close_test])  
        
        real_pos = pos   # å®é™…çš„å¼€ä»“ä»“ä½
        # è·å¾—æ¯æ¬¡ä»“ä½çš„å˜åŒ–
        pos_change = np.concatenate((np.array([0]), np.diff(real_pos)))
        # å†³å®šä»¥ä»€ä¹ˆä»·ä½å¼€ä»“ï¼Œå½“ä»“ä½å˜åŒ–å¤§äº0æ—¶ï¼Œéœ€è¦ä¹°è¿›ï¼Œæ›´å·®çš„ä»·æ ¼æ˜¯closeå’Œnext opençš„æœ€å¤§å€¼
        # å½“ä»“ä½å˜åŒ–å°äº0æ—¶ï¼Œéœ€è¦å–å‡ºï¼Œæ›´å·®çš„ä»·æ ¼æ˜¯closeå’Œnext opençš„æœ€å°å€¼
        which_price_to_trade = np.where(pos_change, np.maximum(close, next_open), np.minimum(close, next_open))

        next_trade_close = np.concatenate((which_price_to_trade[1:], np.array([which_price_to_trade[-1]])))
        rets = np.log(next_trade_close) - np.log(which_price_to_trade) 
        fee = fee  # ä¸‡5æ‰‹ç»­è´¹
        gain_loss = real_pos * rets - abs(pos_change) * fee
        copy_gain_loss = np.copy(gain_loss)
        # è®¡ç®—pnl
        pnl = copy_gain_loss.cumsum()

        # è®¡ç®—èƒœç‡
        win_rate_bar = np.sum(gain_loss > 0) / len(gain_loss)
        # è®¡ç®—ç›ˆäºæ¯”
        avg_gain_bar = np.mean(gain_loss[gain_loss > 0])
        avg_loss_bar = np.abs(np.mean(gain_loss[gain_loss < 0]))
        profit_loss_ratio_bar = avg_gain_bar / avg_loss_bar if avg_loss_bar != 0 else np.inf
        # è®¡ç®—æ€»äº¤æ˜“æ¬¡æ•°ï¼ˆä»“ä½å˜åŠ¨çš„æ¬¡æ•°ï¼‰
        # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡
        annual_return = np.mean(gain_loss)*self.annual_bars
        sharpe_ratio = annual_return / (np.std(gain_loss)*np.sqrt(self.annual_bars))
        #è®¡ç®—å›æ’¤å’Œå¡ç›
        peak_values = np.maximum.accumulate(pnl)
        drawdowns = (pnl - peak_values) / peak_values
        max_drawdown = np.min(drawdowns)
        Calmar_Ratio = annual_return / -max_drawdown if max_drawdown != 0 else np.inf
        
        
        return pnl,{"Win Rate_bar": win_rate_bar,
                "Profit/Loss Ratio_bar": profit_loss_ratio_bar,
                "Annual Return": annual_return,
                "MAX_Drawdown": max_drawdown,
                "Sharpe Ratio": sharpe_ratio,
                "Calmar Ratio": Calmar_Ratio
                }

    
    def real_trading_simulation_plot(self,pos,pos_train, fee=0.0005):
        
        net_values_train,metrics_train = self.real_trading_simulator(pos_train,'train', fee)
        pos_index_train = self.train_index
        close_train = self.close_train

        net_values,metrics = self.real_trading_simulator(pos,'test', fee)
        pos_index = self.test_index
        close_test = self.close_test
        
        fig, axs = plt.subplots(2, 2, figsize=(10, 12))
        fig.suptitle(f'{self.sym} real trading', fontsize=16)

        axs[0, 0].plot(pos_index_train,close_train, 'b-')
        axs[0, 0].set_title('price_train')
        axs[0, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[0, 0].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”


        axs[1, 0].plot(pos_index_train,net_values_train, 'r-')
        axs[1, 0].set_title('pnl_train')
        axs[1, 0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[1, 0].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”

        axs[0, 1].plot(pos_index,close_test, 'b-')
        axs[0, 1].set_title('price_test')
        axs[0, 1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[0, 1].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”


        axs[1, 1].plot(pos_index,net_values, 'r-')
        axs[1, 1].set_title('pnl_test')
        axs[1, 1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # è®¾ç½®æ—¥æœŸæ ¼å¼
        axs[1, 1].xaxis.set_major_locator(mdates.MonthLocator())  # è®¾ç½®æ—¥æœŸé—´éš”


        # Annotate metrics on the plot
        annotation_text = "\n".join([f"{key}: {value:.4f}" for key, value in metrics.items()])
        plt.annotate(annotation_text, xy=(0.05, 0.95), xycoords='axes fraction',
                     verticalalignment='top', horizontalalignment='left',
                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.grid(True)
        plt.tight_layout()
        dir_path = Path(
            f'{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}/real_trading')
        dir_path.mkdir(parents=True, exist_ok=True)
        # Save the plot
        plt.savefig(f'{self.sym}_{self.freq}_{self.y_train_ret_period}_{self.start_date_train}_{self.end_date_train}_{self.start_date_test}_{self.end_date_test}/real_trading/net_value_performance.png')
    
    



if __name__ == '__main__':
    # yaml_file_path = './parameters.yaml'
    yaml_file_path = './coarse_grain_parameters.yaml'
    analyzer = GPAnalyzer(yaml_file_path)

    # Option1 - è¿è¡Œé—ä¼ ç¼–ç¨‹ä»»åŠ¡ï¼Œä¸€æ‰¹æ‰¹çš„ç”Ÿæˆæ–°çš„å› å­
    analyzer.run()

    # # Option2 - ç›´æ¥è¯„ä¼°ç°æœ‰å› å­åº“ä¸­çš„æ‰€æœ‰å› å­ï¼Œ æ‰§è¡Œmetricæ‰“åˆ† ï¼ˆå¯ä»¥æ‰§è¡Œå¦å¤–çš„ä¸€ç»„metricï¼Œé‡æ–°å®šä¹‰å¦ä¸€ä¸ªmetric_dictå³å¯ï¼‰ã€‚ä¸éœ€è¦è¿è¡Œgplearn.
    # analyzer.evaluate_existing_factors()

    ## Option3 - å¯»æ‰¾å‡ºä¼˜ç§€çš„å› å­ï¼Œå¹¶ç»˜åˆ¶å‡ºæ»šåŠ¨å¤æ™®å’Œpnlæ›²çº¿,å†åŠ å·¥æ¨¡å‹æ¨¡å‹

    # analyzer.read_and_cal_metrics()
    # exp_pool = analyzer.elite_factors_further_process()
    # pos_test,pos_train = analyzer.go_model(exp_pool)
    # analyzer.real_trading_simulation_plot(pos_test,pos_train,0.000)

    # # Option4 - è§£ææ‰€æœ‰çš„å› å­è¡¨è¾¾å¼
    # # è¯»å–csv.gzæ–‡ä»¶
    # gz = pd.read_csv('factors_selected.csv')
    # use_historical_data = False
    # if use_historical_data:
    #     evaluator = FeatureEvaluator(_function_map, analyzer.feature_names, analyzer.X_all)
    #     factor_values = pd.DataFrame()
    #     for exp in gz['expression']:
    #         print(f'è¿™ä¸€æ¬¡è¦è§£æçš„å› å­å€¼æ˜¯{exp}')
    #         try:
    #             factor_values[exp] = evaluator.evaluate(exp) # è§£æå› å­å¼ä¸ºå› å­å€¼
    #             print('---------------æˆåŠŸï¼-----------------')
    #         except Exception as e:
    #             print(e)
    #             print('---------------å¤±è´¥ï¼-----------------')
    # else:
    #     import originalFeature
        
    #     mark_data_raw = pd.read_csv('C:/Users/Yidao/Desktop/raw_data_markdata_5m.csv',index_col=0)
    #     mark_data_raw['date'] = pd.to_datetime(mark_data_raw['date'])
    #     mark_data_raw.set_index('date', inplace=True)
        
    #     base_feature = originalFeature.BaseFeature(mark_data_raw)
    #     mark_data = base_feature.init_feature_df
    #     evaluator = FeatureEvaluator(_function_map, mark_data.columns, mark_data.values)
    #     factor_values = pd.DataFrame()
    #     for exp in gz['expression']:
    #         print(f'è¿™ä¸€æ¬¡è¦è§£æçš„å› å­å€¼æ˜¯{exp}')
    #         try:
    #             factor_values[exp] = evaluator.evaluate(exp) # è§£æå› å­å¼ä¸ºå› å­å€¼
    #             print('---------------æˆåŠŸï¼-----------------')
    #         except Exception as e:
    #             print(e)
    #             print('---------------å¤±è´¥ï¼-----------------')


    # Option5 - å¤šè¿›ç¨‹å¹¶è¡Œç­›é€‰å› å­
    # def process_file(yaml_file_path):
    #     analyzer = GPAnalyzer(yaml_file_path)
    #     exp_pool = analyzer.elite_factors_further_process(True)
    #     pos,pos_train = analyzer.go_model(exp_pool)
    #     analyzer.real_trading_simulation_plot(pos,pos_train)
    #
    # # æ–‡ä»¶ååˆ—è¡¨
    # file_names = ['parameters.yaml','parameters1.yaml','parameters2.yaml','parameters3.yaml',]
    #
    # # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†æ–‡ä»¶
    # with ProcessPoolExecutor() as executor:
    #     executor.map(process_file, file_names)
    
