'''
æ•°æ®è¯»å–ã€é™é¢‘å¤„ç†å’Œè®¡ç®—æ”¶ç›Šç‡æ¨¡å—
'''

import pandas as pd
import numpy as np
from pathlib import Path
import originalFeature
from scipy.optimize import minimize
import time
import talib as ta
from enum import Enum
import re
import os
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta

import sys
import matplotlib.pyplot as plt
from scipy.stats import zscore, kurtosis, skew, yeojohnson, boxcox
from scipy.stats import tukeylambda, mstats
from sklearn.preprocessing import RobustScaler
import zipfile
from io import BytesIO
from NormDataCheck import norm, inverse_norm


# å¾®ç»“æ„ï¼ˆbarçº§ï¼‰ç¨³å¥ä»£ç†å‡½æ•°
# from utils.microstructure_features import (
#     get_roll_measure,
#     get_corwin_schultz_estimator,
#     get_bekker_parkinson_vol,
#     get_bar_based_kyle_lambda,
#     get_bar_based_amihud_lambda,
#     get_bar_based_hasbrouck_lambda,
# )

# å¯é€‰ï¼šåŸºäºé€ç¬”æ•°æ®æ„å»º bars å¹¶æå– features/ ç›®å½•ä¸‹çš„å¾®ç»“æ„ç‰¹å¾
from data.time_bars import TimeBarBuilder
from data.dollar_bars import DollarBarBuilder
from data.trades_processor import TradesProcessor
from features.orderflow_features import OrderFlowFeatureExtractor
from features.impact_features import PriceImpactFeatureExtractor
from features.tail_features import TailFeatureExtractor
from features.bucketed_flow_features import BucketedFlowFeatureExtractor
from features.microstructure_extractor import MicrostructureFeatureExtractor
from pipeline.trading_pipeline import TradingPipeline
from multiprocessing import Pool, cpu_count
from functools import partial

# å°è¯•å¯¼å…¥ tqdm ç”¨äºè¿›åº¦æ¡æ˜¾ç¤º
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False


class DataFrequency(Enum):
    """æ•°æ®é¢‘ç‡æšä¸¾"""
    MONTHLY = 'monthly'  # æœˆåº¦æ•°æ®
    DAILY = 'daily'      # æ—¥åº¦æ•°æ®


def data_load(sym: str) -> pd.DataFrame:
    '''æ•°æ®è¯»å–æ¨¡å—ï¼ˆåŸç‰ˆï¼‰'''
    file_name = '/home/etern/crypto/data/merged/merged/' + sym + '-merged-without-rfr-1m.csv'  
    z = pd.read_csv(file_name, index_col=1)[
        ['o', 'h', 'l', 'c', 'vol', 'vol_ccy', 'trades',
               'oi', 'oi_ccy', 'toptrader_count_lsr', 'toptrader_oi_lsr', 'count_lsr',
               'taker_vol_lsr']]
    return z


def _generate_date_range(start_date: str, end_date: str, read_frequency: DataFrequency = DataFrequency.MONTHLY) -> List[str]:
    """
    ç”Ÿæˆæ—¥æœŸèŒƒå›´åˆ—è¡¨
    
    å‚æ•°:
    start_date: èµ·å§‹æ—¥æœŸ
        - æœˆåº¦æ ¼å¼: 'YYYY-MM' (å¦‚ '2020-01') æˆ– 'YYYY-MM-DD' (è‡ªåŠ¨è½¬æ¢ä¸º 'YYYY-MM')
        - æ—¥åº¦æ ¼å¼: 'YYYY-MM-DD' (å¦‚ '2020-01-01')
    end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼åŒä¸Š
    frequency: æ•°æ®é¢‘ç‡ï¼ˆæœˆåº¦æˆ–æ—¥åº¦ï¼‰
    
    è¿”å›:
    æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨
    """
    if read_frequency == DataFrequency.MONTHLY:
        # å…¼å®¹ 'YYYY-MM' å’Œ 'YYYY-MM-DD' ä¸¤ç§æ ¼å¼
        # å¦‚æœæ˜¯ 'YYYY-MM-DD' æ ¼å¼ï¼Œè‡ªåŠ¨æˆªå–ä¸º 'YYYY-MM'
        new_start_date = start_date
        new_end_date = end_date
        if len(start_date) == 10:  # 'YYYY-MM-DD' æ ¼å¼
            new_start_date = start_date[:7]
        if len(end_date) == 10:
            new_end_date = end_date[:7]
            
        start_dt = datetime.strptime(new_start_date, '%Y-%m')
        end_dt = datetime.strptime(new_end_date, '%Y-%m')
        
        date_list = []
        current_dt = start_dt
        while current_dt <= end_dt:
            date_list.append(current_dt.strftime('%Y-%m'))
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæœˆ
            if current_dt.month == 12:
                current_dt = current_dt.replace(year=current_dt.year + 1, month=1)
            else:
                current_dt = current_dt.replace(month=current_dt.month + 1)
        
        return date_list
    
    elif read_frequency == DataFrequency.DAILY:
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        date_list = []
        current_dt = start_dt
        while current_dt <= end_dt:
            date_list.append(current_dt.strftime('%Y-%m-%d'))
            current_dt += timedelta(days=1)
        
        return date_list
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é¢‘ç‡: {frequency}")


def _build_file_paths(sym: str, date_str: str, data_dir: str, timeframe: str = '1m', 
                      frequency: DataFrequency = DataFrequency.MONTHLY) -> Tuple[str, str, str]:
    """
    æ„å»ºæ–‡ä»¶è·¯å¾„
    
    å‚æ•°:
    sym: äº¤æ˜“å¯¹ç¬¦å·
    date_str: æ—¥æœŸå­—ç¬¦ä¸²
    data_dir: æ•°æ®ç›®å½•
    timeframe: æ—¶é—´å‘¨æœŸ (å¦‚ '1m', '5m', '1h')
    frequency: æ•°æ®é¢‘ç‡
    
    è¿”å›:
    (file_base_name, feather_path, zip_path) å…ƒç»„
    """
    if frequency == DataFrequency.MONTHLY:
        file_base_name = f"{sym}-{timeframe}-{date_str}"
    elif frequency == DataFrequency.DAILY:
        file_base_name = f"{sym}-{timeframe}-{date_str}"
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é¢‘ç‡: {frequency}")
    
    # /Volumes/Ext-Disk/data/futures/um/monthly/klines/ETHUSDT/15m/2025/ETHUSDT-15m-2025-01.feather
    year = date_str.split('-')[0]
    feather_path = os.path.join(f'{data_dir}/{year}', f"{file_base_name}.feather")
    zip_path = os.path.join(f'{data_dir}/{year}', f"{file_base_name}.zip")
    
    return file_base_name, feather_path, zip_path


def _read_feather_file(feather_path: str) -> Optional[pd.DataFrame]:
    """
    è¯»å– feather æ ¼å¼æ–‡ä»¶
    
    å‚æ•°:
    feather_path: feather æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    DataFrame æˆ– Noneï¼ˆå¦‚æœè¯»å–å¤±è´¥ï¼‰
    """
    if not os.path.exists(feather_path):
        return None
    
    try:
        df = pd.read_feather(feather_path)
        print(f"âœ“ æˆåŠŸè¯»å– feather: {os.path.basename(feather_path)}, è¡Œæ•°: {len(df)}")
        return df
    except Exception as e:
        print(f"âœ— è¯»å– feather æ–‡ä»¶å¤±è´¥: {os.path.basename(feather_path)}, é”™è¯¯: {str(e)}")
        return None


def _read_zip_file(zip_path: str, file_base_name: str, save_feather: bool = True) -> Optional[pd.DataFrame]:
    """
    è¯»å– zip æ ¼å¼æ–‡ä»¶ï¼ˆå†…å« CSVï¼‰
    
    å‚æ•°:
    zip_path: zip æ–‡ä»¶è·¯å¾„
    file_base_name: æ–‡ä»¶åŸºç¡€åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
    save_feather: æ˜¯å¦ä¿å­˜ä¸º feather æ ¼å¼ä»¥åŠ é€Ÿåç»­è¯»å–
    
    è¿”å›:
    DataFrame æˆ– Noneï¼ˆå¦‚æœè¯»å–å¤±è´¥ï¼‰
    """
    if not os.path.exists(zip_path):
        return None
    
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            # è·å– zip ä¸­çš„ csv æ–‡ä»¶å
            csv_filename = f"{file_base_name}.csv"
            
            if csv_filename not in zip_ref.namelist():
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ª csv æ–‡ä»¶
                csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]
                if csv_files:
                    csv_filename = csv_files[0]
                else:
                    print(f"âœ— åœ¨ {os.path.basename(zip_path)} ä¸­æ‰¾ä¸åˆ° CSV æ–‡ä»¶")
                    return None
            
            # è¯»å– CSV æ•°æ®
            with zip_ref.open(csv_filename) as csv_file:
                df = pd.read_csv(csv_file)
                print(f"âœ“ æˆåŠŸè¯»å– zip: {os.path.basename(zip_path)}, è¡Œæ•°: {len(df)}")
                
                # å¯é€‰ï¼šä¿å­˜ä¸º feather æ ¼å¼ä»¥åŠ é€Ÿåç»­è¯»å–
                if save_feather:
                    feather_path = zip_path.replace('.zip', '.feather')
                    try:
                        df.to_feather(feather_path)
                        print(f"  â†’ å·²ç¼“å­˜ä¸º feather æ ¼å¼")
                    except Exception as e:
                        print(f"  â†’ ä¿å­˜ feather æ–‡ä»¶å¤±è´¥: {str(e)}")
                
                return df
    
    except Exception as e:
        print(f"âœ— è¯»å– zip æ–‡ä»¶å¤±è´¥: {os.path.basename(zip_path)}, é”™è¯¯: {str(e)}")
        return None


def _read_single_period_data(sym: str, date_str: str, data_dir: str, timeframe: str = '1m',
                             frequency: DataFrequency = DataFrequency.MONTHLY) -> Optional[pd.DataFrame]:
    """
    è¯»å–å•ä¸ªæ—¶é—´æ®µçš„æ•°æ®ï¼ˆä¼˜å…ˆ featherï¼Œå…¶æ¬¡ zipï¼‰
    
    å‚æ•°:
    sym: äº¤æ˜“å¯¹ç¬¦å·
    date_str: æ—¥æœŸå­—ç¬¦ä¸²
    data_dir: æ•°æ®ç›®å½•
    timeframe: æ—¶é—´å‘¨æœŸ
    frequency: æ•°æ®é¢‘ç‡
    
    è¿”å›:
    DataFrame æˆ– None
    """
    file_base_name, feather_path, zip_path = _build_file_paths(sym, date_str, data_dir, timeframe, frequency)
    
    # ä¼˜å…ˆè¯»å– feather
    df = _read_feather_file(feather_path)
    if df is not None:
        return df
    
    # å¦‚æœ feather ä¸å­˜åœ¨ï¼Œè¯»å– zip
    df = _read_zip_file(zip_path, file_base_name, save_feather=True)
    if df is not None:
        return df
    
    # ä¸¤ç§æ–‡ä»¶éƒ½ä¸å­˜åœ¨
    print(f"âš  è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_base_name}")
    return None


def _read_tick_data_file(file_path: str) -> pd.DataFrame:
    """
    è¯»å– tick çº§åˆ«äº¤æ˜“æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒ feather / zip / csvï¼‰
    
    å‚æ•°:
    file_path: æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    Tick æ•°æ® DataFrameï¼ˆåŒ…å« time, price, quantity, side ç­‰åˆ—ï¼‰
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Tick æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_ext == '.feather':
            df = pd.read_feather(file_path)
            print(f"âœ“ æˆåŠŸè¯»å– tick feather æ–‡ä»¶ï¼Œè¡Œæ•°: {len(df):,}")
        elif file_ext == '.zip':
            df = pd.read_csv(file_path, compression='zip')
            print(f"âœ“ æˆåŠŸè¯»å– tick zip æ–‡ä»¶ï¼Œè¡Œæ•°: {len(df):,}")
        elif file_ext == '.csv':
            df = pd.read_csv(file_path)
            print(f"âœ“ æˆåŠŸè¯»å– tick csv æ–‡ä»¶ï¼Œè¡Œæ•°: {len(df):,}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ï¼Œä»…æ”¯æŒ .feather / .zip / .csv")
        
        # ç¡®ä¿æ—¶é—´åˆ—æ˜¯ datetime ç±»å‹
        if 'time' in df.columns:
            if not pd.api.types.is_datetime64_any_dtype(df['time']):
                df['time'] = pd.to_datetime(df['time'], unit='ms', errors='coerce')
        
        print(f"Tick æ•°æ®æ—¶é—´èŒƒå›´: {df['time'].min()} è‡³ {df['time'].max()}")
        
        return df
        
    except Exception as e:
        raise ValueError(f"è¯»å– Tick æ•°æ®æ–‡ä»¶å¤±è´¥: {file_path}\né”™è¯¯: {str(e)}")


def _read_direct_file(file_path: str) -> pd.DataFrame:
    """
    ç›´æ¥è¯»å–æŒ‡å®šè·¯å¾„çš„ Kçº¿æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒ feather / zip / csvï¼‰
    
    å‚æ•°:
    file_path: æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    æ ‡å‡†åŒ–åçš„ Kçº¿ DataFrame
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    print(f"\n{'='*60}")
    print(f"ç›´æ¥è¯»å– Kçº¿æ–‡ä»¶: {os.path.basename(file_path)}")
    print(f"{'='*60}\n")
    
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_ext == '.feather':
            df = pd.read_feather(file_path)
            print(f"âœ“ æˆåŠŸè¯»å– feather æ–‡ä»¶ï¼Œè¡Œæ•°: {len(df):,}")
        elif file_ext == '.zip':
            df = pd.read_csv(file_path, compression='zip')
            print(f"âœ“ æˆåŠŸè¯»å– zip æ–‡ä»¶ï¼Œè¡Œæ•°: {len(df):,}")
        elif file_ext == '.csv':
            df = pd.read_csv(file_path)
            print(f"âœ“ æˆåŠŸè¯»å– csv æ–‡ä»¶ï¼Œè¡Œæ•°: {len(df):,}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ï¼Œä»…æ”¯æŒ .feather / .zip / .csv")
        
        # æ ‡å‡†åŒ–åˆ—åå’Œç´¢å¼•
        standardized_df = _standardize_dataframe_columns(df)
        print(f"æ•°æ®æ—¶é—´èŒƒå›´: {standardized_df.index.min()} è‡³ {standardized_df.index.max()}")
        print(f"{'='*60}\n")
        
        return standardized_df
        
    except Exception as e:
        raise ValueError(f"è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}\né”™è¯¯: {str(e)}")


def _standardize_dataframe_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ ‡å‡†åŒ– DataFrame åˆ—åå¹¶è®¾ç½®ç´¢å¼•
    
    å‚æ•°:
    df: åŸå§‹ DataFrameï¼ˆåŒ…å« Binance æ ¼å¼çš„åˆ—åï¼‰
    
    è¿”å›:
    æ ‡å‡†åŒ–åçš„ DataFrame
    """
    # å°† open_time è½¬æ¢ä¸º datetime å¹¶è®¾ç½®ä¸ºç´¢å¼•
    df = df.copy()
    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
    df.set_index('open_time', inplace=True)

    # df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
    # df.set_index('close_time', inplace=True)
    
    # åˆ—åæ˜ å°„ï¼šæ–°åˆ—å -> æ—§åˆ—å
    # æ–°åˆ—å: open_time,open,high,low,close,volume,close_time,quote_volume,count,taker_buy_volume,taker_buy_quote_volume,ignore
    # æ—§åˆ—å: o, h, l, c, vol, vol_ccy, trades, oi, oi_ccy, toptrader_count_lsr, toptrader_oi_lsr, count_lsr, taker_vol_lsr
    column_mapping = {
        'open': 'o',
        'high': 'h',
        'low': 'l',
        'close': 'c',
        'volume': 'vol',
        'quote_volume': 'vol_ccy',
        'count': 'trades',
        'close_time': 'close_time',
    }
    
    df = df.rename(columns=column_mapping)
    
    # é€‰æ‹©éœ€è¦çš„åˆ—ï¼Œå¯¹äºç¼ºå¤±çš„åˆ—ç”¨ 0 å¡«å……
    required_columns = ['o', 'h', 'l', 'c', 'vol', 'vol_ccy', 'trades',
                    #    'oi', 'oi_ccy', 'toptrader_count_lsr', 'toptrader_oi_lsr', 'count_lsr',
                    #    'taker_vol_lsr', 
                       'close_time'
                       ]
    
    # ä¸ºç¼ºå¤±çš„åˆ—æ·»åŠ é»˜è®¤å€¼ 0
    for col in required_columns:
        if col not in df.columns:
            df[col] = 0
            print(f"âš  è­¦å‘Šï¼šåˆ— '{col}' ä¸å­˜åœ¨ï¼Œå·²å¡«å……ä¸º 0")
    
    return df[required_columns]


def data_load_v2(sym: str, data_dir: str, start_date: str, end_date: str, 
                 timeframe: str = '1h', read_frequency: str = 'monthly',
                 file_path: Optional[str] = None) -> pd.DataFrame:
    """
    æ•°æ®è¯»å–æ¨¡å— V2 - æ”¯æŒä»å¤šç§æ—¶é—´ç²’åº¦çš„æ•°æ®æ–‡ä»¶è¯»å–
    
    å‚æ•°:
    sym: äº¤æ˜“å¯¹ç¬¦å·ï¼Œä¾‹å¦‚ 'BTCUSDT'
    data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼Œä¾‹å¦‚ '/Volumes/Ext-Disk/data/futures/um/monthly/klines/BTCUSDT/1m'
    start_date: èµ·å§‹æ—¥æœŸ
        - æœˆåº¦æ ¼å¼: 'YYYY-MM' (å¦‚ '2020-01')
        - æ—¥åº¦æ ¼å¼: 'YYYY-MM-DD' (å¦‚ '2020-01-01')
    end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼åŒä¸Š
    timeframe: æ—¶é—´å‘¨æœŸï¼Œé»˜è®¤ '1m'ï¼Œå¯é€‰ '5m', '1h' ç­‰
    frequency: æ•°æ®é¢‘ç‡ï¼Œ'monthly'ï¼ˆæœˆåº¦ï¼‰æˆ– 'daily'ï¼ˆæ—¥åº¦ï¼‰
    file_path: ç›´æ¥æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .feather / .zip / .csvï¼‰ï¼ŒæŒ‡å®šåå°†å¿½ç•¥å…¶ä»–å‚æ•°
    
    è¿”å›:
    åŒ…å«æ ‡å‡†åŒ–åˆ—åçš„ DataFrame
    
    æ–‡ä»¶è¯»å–ä¼˜å…ˆçº§:
    1. å¦‚æœæŒ‡å®š file_pathï¼Œç›´æ¥è¯»å–è¯¥æ–‡ä»¶
    2. å¦åˆ™æŒ‰æ—¥æœŸèŒƒå›´è¯»å–ï¼Œä¼˜å…ˆè¯»å– .feather æ ¼å¼æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    3. å¦‚æœ .feather ä¸å­˜åœ¨ï¼Œåˆ™è¯»å– .zip æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨ç¼“å­˜ä¸º .feather
    
    ç¤ºä¾‹:
    # è¯»å–æœˆåº¦æ•°æ®
    df = data_load_v2('BTCUSDT', '/path/to/monthly', '2020-01', '2024-09', frequency='monthly')
    
    # è¯»å–æ—¥åº¦æ•°æ®
    df = data_load_v2('BTCUSDT', '/path/to/daily', '2020-01-01', '2020-01-31', frequency='daily')
    
    # ç›´æ¥è¯»å–å•ä¸ªæ–‡ä»¶
    df = data_load_v2('BTCUSDT', '', '', '', file_path='/path/to/data.feather')
    """
    
    # å¦‚æœæŒ‡å®šäº†ç›´æ¥æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥è¯»å–
    if file_path:
        return _read_direct_file(file_path)
    
    # è§£æé¢‘ç‡å‚æ•°
    try:
        freq_enum = DataFrequency(read_frequency.lower())
    except ValueError:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é¢‘ç‡: {read_frequency}ï¼Œä»…æ”¯æŒ 'monthly' æˆ– 'daily'")
    
    # ç”Ÿæˆæ—¥æœŸèŒƒå›´
    date_list = _generate_date_range(start_date, end_date, freq_enum)
    
    # è¯»å–æ‰€æœ‰æ—¶é—´æ®µçš„æ•°æ®
    df_list = []
    success_count = 0
    failed_count = 0
    
    for date_str in date_list:
        df = _read_single_period_data(sym, date_str, data_dir, timeframe, freq_enum)
        if df is not None:
            df_list.append(df)
            success_count += 1
        else:
            failed_count += 1
    
    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè¯»å–åˆ°æ•°æ®
    if not df_list:
        raise ValueError(f"æœªèƒ½æˆåŠŸè¯»å–ä»»ä½•æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œæ—¥æœŸèŒƒå›´\nè·¯å¾„: {data_dir}\næ—¥æœŸ: {start_date} ~ {end_date}")
    
    print(f"\n{'='*60}")
    print(f"è¯»å–å®Œæˆ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
    print(f"{'='*60}\n")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    merged_df = pd.concat(df_list, ignore_index=True)
    print(f"åˆå¹¶åæ€»è¡Œæ•°: {len(merged_df):,}")
    
    # æ ‡å‡†åŒ–åˆ—åå’Œç´¢å¼•
    standardized_df = _standardize_dataframe_columns(merged_df)
    
    print(f"æ•°æ®æ—¶é—´èŒƒå›´: {standardized_df.index.min()} è‡³ {standardized_df.index.max()}")
    print(f"{'='*60}\n")
    
    return standardized_df



def removed_zero_vol_dataframe(df):
    """
    æ‰“å°å¹¶ä¸”è¿”å›-
    1. volumeè¿™ä¸€åˆ—ä¸º0çš„è¡Œç»„æˆçš„df
    2. lowè¿™ä¸€åˆ—çš„æœ€å°å€¼
    3. volumeè¿™ä¸€åˆ—çš„æœ€å°å€¼
    5. å»é™¤æ‰volume=0çš„è¡Œçš„dataframe
    -------

    """
    # å°†DataFrameçš„ç´¢å¼•åˆ—è®¾ç½®ä¸º'datetime'
    df.index = pd.to_datetime(df.index)

    # 1. volumeè¿™ä¸€åˆ—ä¸º0çš„è¡Œç»„æˆçš„df
    volume_zero_df = df[df['vol'] == 0]
    print(f"Volumeä¸º0çš„è¡Œç»„æˆçš„DataFrame: {len(volume_zero_df)}")

    # 2. lowè¿™ä¸€åˆ—çš„æœ€å°å€¼
    min_low = df['l'].min()
    print(f"Lowè¿™ä¸€åˆ—çš„æœ€å°å€¼: {min_low}")

    # 3. volumeè¿™ä¸€åˆ—çš„æœ€å°å€¼
    min_volume = df['vol'].min()
    print(f"Volumeè¿™ä¸€åˆ—çš„æœ€å°å€¼: {min_volume}")

    # 5. å»é™¤æ‰volume=0çš„è¡Œçš„dataframe
    removed_zero_vol_df = df[df['vol'] != 0]
    print(f"å»é™¤æ‰Volumeä¸º0çš„è¡Œä¹‹å‰çš„DataFrame length: {len(df)}")
    print(f"å»é™¤æ‰Volumeä¸º0çš„è¡Œä¹‹åçš„DataFrame length: {len(removed_zero_vol_df)}")

    return removed_zero_vol_df


def resample(z: pd.DataFrame, freq: str) -> pd.DataFrame:
    '''
    è¿™æ˜¯ä¸æ”¯æŒvwapçš„ï¼Œé»˜è®¤è¯»å…¥çš„æ•°æ®æ˜¯æ²¡æœ‰turnoverä¿¡æ¯ï¼Œè‡ªç„¶ä¹Ÿæ²¡æœ‰vwapçš„ä¿¡æ¯ï¼Œä¸éœ€è¦è·å–symçš„ä¹˜æ•°
    '''
    if freq == '15m':
        return z
    
    if freq not in ('1min', '1m'):
        z.index = pd.to_datetime(z.index)
        # æ³¨æ„closedå’Œlabelå‚æ•°
        z = z.resample(freq, closed='left', label='left').agg({'o': 'first',
                                                               'h': 'max',
                                                               'l': 'min',
                                                               'c': 'last',
                                                               'vol': 'sum',
                                                               'vol_ccy': 'sum',
                                                               'trades': 'sum'
                                                            #    'oi': 'last', 
                                                            #    'oi_ccy': 'last', 
                                                            #    'toptrader_count_lsr':'last', 
                                                            #    'toptrader_oi_lsr':'last', 
                                                            #    'count_lsr':'last',
                                                            #    'taker_vol_lsr':'last'
                                                               })
        # æ³¨æ„resampleå,æ¯”å¦‚ä»¥10minä¸ºresampleçš„freqï¼Œ9:00çš„æ•°æ®æ˜¯æŒ‡9:00åˆ°9:10çš„æ•°æ®~~
        z = z.fillna(method='ffill')   
        # z.columns = ['o', 'h', 'l', 'c', 'vol', 'vol_ccy','trades',
        #        'oi', 'oi_ccy', 'toptrader_count_lsr', 'toptrader_oi_lsr', 'count_lsr',
        #        'taker_vol_lsr']
        z.columns = ['o', 'h', 'l', 'c', 'vol', 'vol_ccy','trades']

        # é‡è¦ï¼Œè¿™ä¸ªåˆ æ‰0æˆäº¤çš„æ“ä½œï¼Œä¸èƒ½ç»™5åˆ†é’Ÿä»¥å†…çš„freqè¿›è¡Œæ“ä½œï¼Œå› ä¸ºè¿™ç§æƒ…å†µè¿˜æ˜¯æŒºå®¹æ˜“å‡ºç°æ²¡æœ‰æˆäº¤çš„ï¼Œè¿™ä¼šæ”¹å˜æœ¬èº«çš„åˆ†å¸ƒ
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¼€å¤´çš„æ•°å€¼éƒ¨åˆ†, åˆ¤æ–­freqçš„å‘¨æœŸ
        match = re.match(r"(\d+)", freq)
        if match:
            int_freq = int(match.group(1))
        if int_freq > 5:
            z = removed_zero_vol_dataframe(z)
    return z


def data_prepare(sym, freq, start_date_train, end_date_train, start_date_test, end_date_test, y_train_ret_period=1,
                 rolling_w=2000, output_format='ndarry', _compute_transformed_series=False, _check_zscore_window_series=False, data_dir='', read_frequency='', timeframe='', file_path=None):
    '''
    å†…ç½®äº†ä¸€äº›å¯¹äºlabelçš„åˆ†æï¼Œæ¯”è¾ƒå…³é”®, ä½†åªéœ€è¦ç ”ç©¶å’Œå¯¹æ¯”æ—¶æ‰ä¼šå¼€å¯

        # å¯¹äºLabelçš„åˆ†æçš„æŒ‡å¯¼ç›®æ ‡ï¼Œæ˜¯å¸Œæœ›å®ƒèƒ½å¤Ÿæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œååº¦ï¼Œå³°åº¦æ¥è¿‘äº0
    # æ–¹æ¡ˆ1ï¼Œå…ˆå¯¹log_returnåšclipï¼Œå®Œå…¨å»é™¤äº†outlierï¼Œå†çœ‹ååº¦å³°åº¦ï¼Œå†³å®šåç»­æ˜¯å¦rolling_zscore.
    # æ–¹æ¡ˆ2ï¼Œå…ˆå¯¹log_returnåšrolling_zscore,
    # (rollingçª—å£å€¼æ˜¯2000ï¼Œæš‚æ—¶å½“åšç»éªŒæ€§çš„å‚æ•°ï¼Œå–å€¼çš„è‡ªç”±åº¦æ¥æºäºè©¹æ£®ä¸ç­‰å¼å’Œå¤§æ•°å®šç†ï¼Œéƒ½æ˜¯ç”¨æ•°æ®ç®—å‡ºæ¥çš„)
        # 1. å‚æ•°å’Œå•ç‚¹å¤æ™®çš„å…³ç³»ä¸æ˜ç¡®ï¼Œä½†æ˜¯å’Œå‡ ä¸‡ä¸ªå› å­çš„å¤æ™®åªå’Œï¼Œä»–ä»¬çš„å…³ç³»åº”è¯¥å­˜åœ¨ä¸€å®šçš„å‡¸æ€§ï¼›
        # 2. å‚æ•°çš„è®¾ç½®åº”è¯¥åœ¨ç»´æŒ1çš„å‰æä¸‹å…¼é¡¾å¤§æ•°å®šç†ï¼›
        # 3. samplesæ‹†åˆ†æˆä¸ºå‡ ä¸ªclassåæ ·æœ¬é‡ä»ç„¶ç¬¦åˆå¤§æ•°å®šç†ï¼›
    # å¦‚ä¸Šä¸¤ç§æ–¹æ¡ˆçš„å¯¹æ¯”ï¼Œå½“å‰è®¤ä¸ºæ˜¯åº”è¯¥ç¬¬äºŒç§æ–¹å¼ï¼Œåº”è¯¥æ˜¯èƒ½å¤Ÿä¿ç•™ä¸€éƒ¨åˆ†outlierçš„ä¿¡æ¯ï¼Œç›¸å¯¹å¹³è¡¡çš„å‡è½»outlierçš„å½±å“

    Note - æœ€ç»ˆè¦æŠŠçª—å£è¿˜æ²¡ç§¯ç´¯å®Œå…¨çš„éƒ¨åˆ†ï¼Œåˆ é™¤æ‰è¿™äº›æ ·æœ¬ï¼Œå¦åˆ™ä¼šå½±å“è®­ç»ƒçš„ç»“æœã€‚å¾€å¾€æ˜¯éƒ½ç”Ÿæˆäº†featureä¹‹åï¼Œæœ€åå¤„ç†å¥½labelï¼Œå†åšåˆ‡å‰²ã€‚
    '''

    # -----------------------------------------
    # z = data_load(sym)
    z = data_load_v2(sym, data_dir=data_dir, start_date=start_date_train, end_date=end_date_test, timeframe=timeframe, read_frequency=read_frequency, file_path=file_path)
    # åˆ‡åˆ†æ•°æ®ï¼Œåªå–éœ€è¦çš„éƒ¨åˆ† - train and test
    z.index = pd.to_datetime(z.index)
    print(f'å¼€å§‹å¤„ç† {sym} çš„å†å²æ•°æ®')   
    print(f'len of z before select = {len(z)}')
    z = z[(z.index >= pd.to_datetime(start_date_train)) & (
        z.index <= pd.to_datetime(end_date_test))]  # åªæˆªå–å‚æ•°æŒ‡å®šéƒ¨åˆ†dataframe
    print(f'len of z after select = {len(z)}')
    ohlcva_df = resample(z, freq)

    print(f'len of resample_z = {len(ohlcva_df)}')
    # --------------------------------------------------------
    if _compute_transformed_series:
        # åˆ†ælabelçš„åˆ†å¸ƒï¼Œç”»å‡ºlabelçš„å„ç±»å¤„ç†åçš„ ä¸‰é˜¶çŸ©ï¼Œå››é˜¶çŸ©
        compute_transformed_series(z.c)
    if _check_zscore_window_series:
        # ç”»å›¾ï¼Œå±•ç°å„ç§çª—å£ä¸‹çš„labelçš„log_return
        check_zscore_window_series(z, 'c')
    # --------------------------------------------------------
    print('å¼€å§‹ç”Ÿæˆåˆå§‹ç‰¹å¾')
    base_feature = originalFeature.BaseFeature(ohlcva_df.copy())
    z = base_feature.init_feature_df

    # -----------------------------------------

    # å…³é”® - ç”Ÿæˆretè¿™ä¸€åˆ—ï¼Œè¿™æ˜¯labelæ•°å€¼ï¼Œæ•´ä¸ªå› å­è¯„ä¼°ä½“ç³»çš„åŸºç¡€ï¼Œè¦æ³¨æ„åˆ†ælabelåˆ†å¸ƒçš„skewness, kurtosisç­‰.
    # note - éœ€è¦æŠŠç©ºå€¼å¤„ç†æ‰ï¼Œå› ä¸ºæµ‹è¯•é›†ä¸­çš„æœ€åçš„å‡ ä¸ªç©ºå€¼å¯èƒ½åˆšå¥½å½±å“æµ‹è¯•çš„æŒä»“æ•ˆæœ.
    # æ³¨æ„ä½¿ç”¨æ»‘åŠ¨çª—å£æ—¶ï¼Œå¯¹äºæ²¡å¡«æ»¡çš„åŒºåŸŸï¼Œå’Œæœ€åç©ºç©ºå€¼åŒºåŸŸï¼Œä¹Ÿè¦æœ‰ç±»ä¼¼çš„è€ƒé‡ï¼Œé˜²æ­¢åˆšå¥½ç¢°åˆ°æå€¼labelå¼•èµ·å¤±çœŸå½±å“ã€‚
    print('å¼€å§‹ç”Ÿæˆret')
    z['return_f'] = np.log(z['c']).diff(
        y_train_ret_period).shift(-y_train_ret_period)
    z['return_f'] = z['return_f'].fillna(0)
    z['r'] = np.log(z['c']).diff()
    z['r'] = z['r'].fillna(0)


    # ---æ–¹æ¡ˆ2ï¼Œ å…ˆå¯¹labelåšrolling_zscore---------------
    def norm_ret(x, window=rolling_w):  # ä¸å†ç”¨L2 normï¼Œæ¢å¤åˆ°ä¹‹å‰çš„zscoreï¼Œç„¶åè¿™é‡Œéœ€è¦åšçš„æ˜¯ç»™ä»–å¢åŠ ä¸€ä¸ªå‘¨æœŸ

        # æ³¨æ„è¿™ä¸ªå‡½æ•°æ˜¯ç”¨åœ¨returnä¸Šé¢çš„ï¼Œlog1pæœ€å°çš„æ•°å€¼æ˜¯-1ï¼Œç”¨äºreturnåˆé€‚
        x = np.log1p(np.asarray(x))
        factors_data = pd.DataFrame(x, columns=['factor'])
        factors_data = factors_data.replace([np.inf, -np.inf, np.nan], 0.0)
        # factors_mean = factors_data.rolling(
        #     window=window, min_periods=1).mean()
        factors_std = factors_data.rolling(window=window, min_periods=1).std()
        factor_value = factors_data / factors_std
        factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
        # factor_value = factor_value.clip(-6, 6)
        return np.nan_to_num(factor_value).flatten()


    # Note - å…ˆå¼ºè¡Œä½¿ç”¨norm_retçœ‹æ•ˆæœ
    z['ret_rolling_zscore'] = norm_ret(z['return_f'])

    # æ­¤æ—¶ï¼Œæ‰€æœ‰çš„featureså’Œlabelï¼Œéƒ½ç”¨ç›¸åŒçª—å£åšå®Œäº†rollingå¤„ç†ï¼Œä¸ºäº†è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥å¼€å§‹åˆ é™¤æ‰è¿˜æ²¡æœ‰å­˜æ»¡çª—å£çš„é‚£äº›è¡Œäº†ã€‚å»é™¤å‰windowè¡Œ
    # é‡è¦ï¼ï¼ å¦‚æœæ‰§è¡Œå¦‚ä¸‹è¿™å¥ï¼Œä¼šzä¸ä¸Šé¢çš„ohlcva_dfä¸ä¸€è‡´ï¼Œå¯¼è‡´originalFeature.BaseFeature(ohlcva_df)åˆå§‹åŒ–çš„ohlcva_dfä¸åševalçš„feature_dataä¸ä¸€è‡´
    # z = z.iloc[window-1:]


    kept_index = z.index
    ohlcva_df = ohlcva_df.loc[kept_index]
    open_train = ohlcva_df['o'][(ohlcva_df['o'].index >= pd.to_datetime(start_date_train)) & (
            ohlcva_df['o'].index < pd.to_datetime(end_date_train))]
    open_test = ohlcva_df['o'][(ohlcva_df['o'].index >= pd.to_datetime(start_date_test)) & (
            ohlcva_df['o'].index <= pd.to_datetime(end_date_test))]
    close_train = ohlcva_df['c'][(ohlcva_df['c'].index >= pd.to_datetime(start_date_train)) & (
            ohlcva_df['c'].index < pd.to_datetime(end_date_train))]
    close_test = ohlcva_df['c'][(ohlcva_df['c'].index >= pd.to_datetime(start_date_test)) & (
            ohlcva_df['c'].index <= pd.to_datetime(end_date_test))]

    print('ret_rolling_zscore skew = {}'.format(
        z['ret_rolling_zscore'].skew()))
    print('ret_rolling_zscore kurtosis = {}'.format(
        z['ret_rolling_zscore'].kurtosis()))

    print('return skew = {}'.format(z['return_f'].skew()))
    print('return kurtosis = {}'.format(z['return_f'].kurtosis()))


    # åˆ‡åˆ†ä¸ºtrainå’Œtestä¸¤ä¸ªæ•°æ®é›†ï¼Œä½†æ˜¯æ³¨æ„ï¼Œtestæ•°æ®é›†å…¶å®å¸¦å…¥äº†ä¹‹å‰çš„æ•°æ®çš„çª—å£, æ˜¯è¦ç‰¹æ„è¿™ä¹ˆåšçš„ã€‚
    z_train = z[(z.index >= pd.to_datetime(start_date_train)) & (
        z.index < pd.to_datetime(end_date_train))]  # åªæˆªå–å‚æ•°æŒ‡å®šéƒ¨åˆ†dataframe
    z_test = z[(z.index >= pd.to_datetime(start_date_test)) & (
        z.index <= pd.to_datetime(end_date_test))]
    # ------------<label åˆ†æ>-------------------------

    # å¯¹äºLabelçš„åˆ†æçš„æŒ‡å¯¼ç›®æ ‡ï¼Œæ˜¯å¸Œæœ›å®ƒèƒ½å¤Ÿæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œååº¦ï¼Œå³°åº¦æ¥è¿‘äº0
    # æ–¹æ¡ˆ1ï¼Œå…ˆå¯¹log_returnåšclipï¼Œå®Œå…¨å»é™¤äº†outlierï¼Œå†çœ‹ååº¦å³°åº¦ï¼Œå†³å®šåç»­æ˜¯å¦rolling_zscore.
    # æ–¹æ¡ˆ2ï¼Œå…ˆå¯¹log_returnåšrolling_zscore,
    # (rollingçª—å£å€¼æ˜¯2000ï¼Œæš‚æ—¶å½“åšç»éªŒæ€§çš„å‚æ•°ï¼Œå–å€¼çš„è‡ªç”±åº¦æ¥æºäºè©¹æ£®ä¸ç­‰å¼å’Œå¤§æ•°å®šç†ï¼Œéƒ½æ˜¯ç”¨æ•°æ®ç®—å‡ºæ¥çš„)
    # 1. å‚æ•°å’Œå•ç‚¹å¤æ™®çš„å…³ç³»ä¸æ˜ç¡®ï¼Œä½†æ˜¯å’Œå‡ ä¸‡ä¸ªå› å­çš„å¤æ™®åªå’Œï¼Œä»–ä»¬çš„å…³ç³»åº”è¯¥å­˜åœ¨ä¸€å®šçš„å‡¸æ€§ï¼›
    # 2. å‚æ•°çš„è®¾ç½®åº”è¯¥åœ¨ç»´æŒ1çš„å‰æä¸‹å…¼é¡¾å¤§æ•°å®šç†ï¼›
    # 3. samplesæ‹†åˆ†æˆä¸ºå‡ ä¸ªclassåæ ·æœ¬é‡ä»ç„¶ç¬¦åˆå¤§æ•°å®šç†ï¼›
    # å¦‚ä¸Šä¸¤ç§æ–¹æ¡ˆçš„å¯¹æ¯”ï¼Œå½“å‰è®¤ä¸ºæ˜¯åº”è¯¥ç¬¬äºŒç§æ–¹å¼ï¼Œåº”è¯¥æ˜¯èƒ½å¤Ÿä¿ç•™ä¸€éƒ¨åˆ†outlierçš„ä¿¡æ¯ï¼Œç›¸å¯¹å¹³è¡¡çš„å‡è½»outlierçš„å½±å“



    if output_format == 'ndarry':
        y_dataset_train = z_train['ret_rolling_zscore'].values
        y_dataset_test = z_test['ret_rolling_zscore'].values
        ret_dataset_train = z_train['return_f'].values
        ret_dataset_test = z_test['return_f'].values
        # é‡è¦ï¼è¦åˆ é™¤æ‰åŒ…å«æœªæ¥ä¿¡æ¯çš„å­—æ®µï¼Œretï¼Œret_rolling_zscore
        z_train.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z_test.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)

        X_all = np.where(np.isnan(z), 0, z)
        X_dataset_train = np.where(np.isnan(z_train), 0, z_train)
        X_dataset_test = np.where(np.isnan(z_test), 0, z_test)

    elif output_format == 'dataframe':
        y_dataset_train = z_train['ret_rolling_zscore']
        y_dataset_test = z_test['ret_rolling_zscore']
        ret_dataset_train = z_train['return_f']
        ret_dataset_test = z_test['return_f']
        # é‡è¦ï¼è¦åˆ é™¤æ‰åŒ…å«æœªæ¥ä¿¡æ¯çš„å­—æ®µï¼Œretï¼Œret_rolling_zscore
        z_train.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z_test.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)

        X_all = z.fillna(0)
        X_dataset_train = z_train.fillna(0)
        X_dataset_test = z_test.fillna(0)

    else:
        print(
            'output_format of data_prepare should be "ndarry" or "dataframe", pls check it ')
        exit(1)

    feature_names = z_train.columns

    print('æ£€æŸ¥x allæ˜¯ä¸æ˜¯ç­‰äº x trainå’Œy trainç›¸åŠ ï¼Œå†æ£€æŸ¥trianå’Œtestä»¥åŠcloseå’Œopençš„å½¢çŠ¶æ˜¯å¦ä¸€è‡´')
    # X_all ä¸“é—¨æ˜¯ä¸ºåšbatch predictionçš„æ—¶å€™ï¼Œè¦ç”¨X_allç”Ÿæˆtesté›†è¦ç”¨åˆ°çš„factor_df, å› ä¸ºfactorçš„è®¡ç®—éœ€è¦ä¹‹å‰ä¸€æ®µwindowä¸­çš„featureå€¼
    print(f'æ£€æŸ¥X_allçš„å½¢çŠ¶ {X_all.shape}')
    print(f'æ£€æŸ¥x dataset trainçš„å½¢çŠ¶ {X_dataset_train.shape}')
    print(f'æ£€æŸ¥y dataset trainçš„å½¢çŠ¶ {y_dataset_train.shape}')
    print(f'æ£€æŸ¥x allæ˜¯ä¸æ˜¯ç­‰äºtrainå’Œtestç›¸åŠ  {len(X_all)},{len(X_dataset_test)+len(X_dataset_train)}')
    print(f'æ£€æŸ¥open trainçš„å½¢çŠ¶ {open_train.shape}')
    print(f'æ£€æŸ¥close trainçš„å½¢çŠ¶ {close_train.shape}')
    print(f'æ£€æŸ¥x dataset testçš„å½¢çŠ¶ {X_dataset_test.shape}')
    print(f'æ£€æŸ¥y dataset testçš„å½¢çŠ¶ {y_dataset_test.shape}')
    print(f'æ£€æŸ¥open testçš„å½¢çŠ¶ {open_test.shape}')
    print(f'æ£€æŸ¥close testçš„å½¢çŠ¶ {close_test.shape}')

    # X_all ä¸“é—¨æ˜¯ä¸ºåšbatch predictionçš„æ—¶å€™ï¼Œè¦ç”¨X_allç”Ÿæˆtesté›†è¦ç”¨åˆ°çš„factor_df, å› ä¸ºfactorçš„è®¡ç®—éœ€è¦ä¹‹å‰ä¸€æ®µwindowä¸­çš„featureå€¼
    return X_all, X_dataset_train, y_dataset_train,ret_dataset_train, X_dataset_test, y_dataset_test,ret_dataset_test, feature_names,open_train,open_test,close_train,close_test, z.index ,ohlcva_df


def data_thick_rolling_prepare(sym, freq, start_date_train, end_date_train, start_date_test, end_date_test, y_train_ret_period=1,
                 rolling_w=2000, output_format='ndarry', _compute_transformed_series=False, _check_zscore_window_series=False, data_dir='', read_frequency='', timeframe='', file_path=None):
    '''
    å†…ç½®äº†ä¸€äº›å¯¹äºlabelçš„åˆ†æï¼Œæ¯”è¾ƒå…³é”®, ä½†åªéœ€è¦ç ”ç©¶å’Œå¯¹æ¯”æ—¶æ‰ä¼šå¼€å¯

        # å¯¹äºLabelçš„åˆ†æçš„æŒ‡å¯¼ç›®æ ‡ï¼Œæ˜¯å¸Œæœ›å®ƒèƒ½å¤Ÿæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œååº¦ï¼Œå³°åº¦æ¥è¿‘äº0
    # æ–¹æ¡ˆ1ï¼Œå…ˆå¯¹log_returnåšclipï¼Œå®Œå…¨å»é™¤äº†outlierï¼Œå†çœ‹ååº¦å³°åº¦ï¼Œå†³å®šåç»­æ˜¯å¦rolling_zscore.
    # æ–¹æ¡ˆ2ï¼Œå…ˆå¯¹log_returnåšrolling_zscore,
    # (rollingçª—å£å€¼æ˜¯2000ï¼Œæš‚æ—¶å½“åšç»éªŒæ€§çš„å‚æ•°ï¼Œå–å€¼çš„è‡ªç”±åº¦æ¥æºäºè©¹æ£®ä¸ç­‰å¼å’Œå¤§æ•°å®šç†ï¼Œéƒ½æ˜¯ç”¨æ•°æ®ç®—å‡ºæ¥çš„)
        # 1. å‚æ•°å’Œå•ç‚¹å¤æ™®çš„å…³ç³»ä¸æ˜ç¡®ï¼Œä½†æ˜¯å’Œå‡ ä¸‡ä¸ªå› å­çš„å¤æ™®åªå’Œï¼Œä»–ä»¬çš„å…³ç³»åº”è¯¥å­˜åœ¨ä¸€å®šçš„å‡¸æ€§ï¼›
        # 2. å‚æ•°çš„è®¾ç½®åº”è¯¥åœ¨ç»´æŒ1çš„å‰æä¸‹å…¼é¡¾å¤§æ•°å®šç†ï¼›
        # 3. samplesæ‹†åˆ†æˆä¸ºå‡ ä¸ªclassåæ ·æœ¬é‡ä»ç„¶ç¬¦åˆå¤§æ•°å®šç†ï¼›
    # å¦‚ä¸Šä¸¤ç§æ–¹æ¡ˆçš„å¯¹æ¯”ï¼Œå½“å‰è®¤ä¸ºæ˜¯åº”è¯¥ç¬¬äºŒç§æ–¹å¼ï¼Œåº”è¯¥æ˜¯èƒ½å¤Ÿä¿ç•™ä¸€éƒ¨åˆ†outlierçš„ä¿¡æ¯ï¼Œç›¸å¯¹å¹³è¡¡çš„å‡è½»outlierçš„å½±å“

    Note - æœ€ç»ˆè¦æŠŠçª—å£è¿˜æ²¡ç§¯ç´¯å®Œå…¨çš„éƒ¨åˆ†ï¼Œåˆ é™¤æ‰è¿™äº›æ ·æœ¬ï¼Œå¦åˆ™ä¼šå½±å“è®­ç»ƒçš„ç»“æœã€‚å¾€å¾€æ˜¯éƒ½ç”Ÿæˆäº†featureä¹‹åï¼Œæœ€åå¤„ç†å¥½labelï¼Œå†åšåˆ‡å‰²ã€‚
    '''

    # -----------------------------------------
    # z = data_load(sym)
    z = data_load_v2(sym, data_dir=data_dir, start_date=start_date_train, end_date=end_date_test, timeframe=timeframe, read_frequency=read_frequency, file_path=file_path)
    # åˆ‡åˆ†æ•°æ®ï¼Œåªå–éœ€è¦çš„éƒ¨åˆ† - train and test
    z.index = pd.to_datetime(z.index)
    print(f'å¼€å§‹å¤„ç† {sym} çš„å†å²æ•°æ®')   
    print(f'len of z before select = {len(z)}')
    z = z[(z.index >= pd.to_datetime(start_date_train)) & (
        z.index <= pd.to_datetime(end_date_test))]  # åªæˆªå–å‚æ•°æŒ‡å®šéƒ¨åˆ†dataframe
    print(f'len of z after select = {len(z)}')
    ohlcva_df = resample(z, freq)

    print(f'len of resample_z = {len(ohlcva_df)}')
    # --------------------------------------------------------
    if _compute_transformed_series:
        # åˆ†ælabelçš„åˆ†å¸ƒï¼Œç”»å‡ºlabelçš„å„ç±»å¤„ç†åçš„ ä¸‰é˜¶çŸ©ï¼Œå››é˜¶çŸ©
        compute_transformed_series(z.c)
    if _check_zscore_window_series:
        # ç”»å›¾ï¼Œå±•ç°å„ç§çª—å£ä¸‹çš„labelçš„log_return
        check_zscore_window_series(z, 'c')
    # --------------------------------------------------------
    print('å¼€å§‹ç”Ÿæˆåˆå§‹ç‰¹å¾')
    base_feature = originalFeature.BaseFeature(ohlcva_df.copy())
    z = base_feature.init_feature_df

    # -----------------------------------------

    # å…³é”® - ç”Ÿæˆretè¿™ä¸€åˆ—ï¼Œè¿™æ˜¯labelæ•°å€¼ï¼Œæ•´ä¸ªå› å­è¯„ä¼°ä½“ç³»çš„åŸºç¡€ï¼Œè¦æ³¨æ„åˆ†ælabelåˆ†å¸ƒçš„skewness, kurtosisç­‰.
    # note - éœ€è¦æŠŠç©ºå€¼å¤„ç†æ‰ï¼Œå› ä¸ºæµ‹è¯•é›†ä¸­çš„æœ€åçš„å‡ ä¸ªç©ºå€¼å¯èƒ½åˆšå¥½å½±å“æµ‹è¯•çš„æŒä»“æ•ˆæœ.
    # æ³¨æ„ä½¿ç”¨æ»‘åŠ¨çª—å£æ—¶ï¼Œå¯¹äºæ²¡å¡«æ»¡çš„åŒºåŸŸï¼Œå’Œæœ€åç©ºç©ºå€¼åŒºåŸŸï¼Œä¹Ÿè¦æœ‰ç±»ä¼¼çš„è€ƒé‡ï¼Œé˜²æ­¢åˆšå¥½ç¢°åˆ°æå€¼labelå¼•èµ·å¤±çœŸå½±å“ã€‚
    print('å¼€å§‹ç”Ÿæˆret')
    z['return_f'] = np.log(z['c']).diff(
        y_train_ret_period).shift(-y_train_ret_period)
    z['return_f'] = z['return_f'].fillna(0)
    z['r'] = np.log(z['c']).diff()
    z['r'] = z['r'].fillna(0)


    # ---æ–¹æ¡ˆ2ï¼Œ å…ˆå¯¹labelåšrolling_zscore---------------
    def norm_ret(x, window=rolling_w):  # ä¸å†ç”¨L2 normï¼Œæ¢å¤åˆ°ä¹‹å‰çš„zscoreï¼Œç„¶åè¿™é‡Œéœ€è¦åšçš„æ˜¯ç»™ä»–å¢åŠ ä¸€ä¸ªå‘¨æœŸ

        # æ³¨æ„è¿™ä¸ªå‡½æ•°æ˜¯ç”¨åœ¨returnä¸Šé¢çš„ï¼Œlog1pæœ€å°çš„æ•°å€¼æ˜¯-1ï¼Œç”¨äºreturnåˆé€‚
        x = np.log1p(np.asarray(x))
        factors_data = pd.DataFrame(x, columns=['factor'])
        factors_data = factors_data.replace([np.inf, -np.inf, np.nan], 0.0)
        # factors_mean = factors_data.rolling(
        #     window=window, min_periods=1).mean()
        factors_std = factors_data.rolling(window=window, min_periods=1).std()
        factor_value = factors_data / factors_std
        factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
        # factor_value = factor_value.clip(-6, 6)
        return np.nan_to_num(factor_value).flatten()


    # Note - å…ˆå¼ºè¡Œä½¿ç”¨norm_retçœ‹æ•ˆæœ
    z['ret_rolling_zscore'] = norm_ret(z['return_f'])

    # æ­¤æ—¶ï¼Œæ‰€æœ‰çš„featureså’Œlabelï¼Œéƒ½ç”¨ç›¸åŒçª—å£åšå®Œäº†rollingå¤„ç†ï¼Œä¸ºäº†è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¯ä»¥å¼€å§‹åˆ é™¤æ‰è¿˜æ²¡æœ‰å­˜æ»¡çª—å£çš„é‚£äº›è¡Œäº†ã€‚å»é™¤å‰windowè¡Œ
    # é‡è¦ï¼ï¼ å¦‚æœæ‰§è¡Œå¦‚ä¸‹è¿™å¥ï¼Œä¼šzä¸ä¸Šé¢çš„ohlcva_dfä¸ä¸€è‡´ï¼Œå¯¼è‡´originalFeature.BaseFeature(ohlcva_df)åˆå§‹åŒ–çš„ohlcva_dfä¸åševalçš„feature_dataä¸ä¸€è‡´
    # z = z.iloc[window-1:]


    kept_index = z.index
    ohlcva_df = ohlcva_df.loc[kept_index]
    open_train = ohlcva_df['o'][(ohlcva_df['o'].index >= pd.to_datetime(start_date_train)) & (
            ohlcva_df['o'].index < pd.to_datetime(end_date_train))]
    open_test = ohlcva_df['o'][(ohlcva_df['o'].index >= pd.to_datetime(start_date_test)) & (
            ohlcva_df['o'].index <= pd.to_datetime(end_date_test))]
    close_train = ohlcva_df['c'][(ohlcva_df['c'].index >= pd.to_datetime(start_date_train)) & (
            ohlcva_df['c'].index < pd.to_datetime(end_date_train))]
    close_test = ohlcva_df['c'][(ohlcva_df['c'].index >= pd.to_datetime(start_date_test)) & (
            ohlcva_df['c'].index <= pd.to_datetime(end_date_test))]

    print('ret_rolling_zscore skew = {}'.format(
        z['ret_rolling_zscore'].skew()))
    print('ret_rolling_zscore kurtosis = {}'.format(
        z['ret_rolling_zscore'].kurtosis()))

    print('return skew = {}'.format(z['return_f'].skew()))
    print('return kurtosis = {}'.format(z['return_f'].kurtosis()))


    # åˆ‡åˆ†ä¸ºtrainå’Œtestä¸¤ä¸ªæ•°æ®é›†ï¼Œä½†æ˜¯æ³¨æ„ï¼Œtestæ•°æ®é›†å…¶å®å¸¦å…¥äº†ä¹‹å‰çš„æ•°æ®çš„çª—å£, æ˜¯è¦ç‰¹æ„è¿™ä¹ˆåšçš„ã€‚
    z_train = z[(z.index >= pd.to_datetime(start_date_train)) & (
        z.index < pd.to_datetime(end_date_train))]  # åªæˆªå–å‚æ•°æŒ‡å®šéƒ¨åˆ†dataframe
    z_test = z[(z.index >= pd.to_datetime(start_date_test)) & (
        z.index <= pd.to_datetime(end_date_test))]
    # ------------<label åˆ†æ>-------------------------

    # å¯¹äºLabelçš„åˆ†æçš„æŒ‡å¯¼ç›®æ ‡ï¼Œæ˜¯å¸Œæœ›å®ƒèƒ½å¤Ÿæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œååº¦ï¼Œå³°åº¦æ¥è¿‘äº0
    # æ–¹æ¡ˆ1ï¼Œå…ˆå¯¹log_returnåšclipï¼Œå®Œå…¨å»é™¤äº†outlierï¼Œå†çœ‹ååº¦å³°åº¦ï¼Œå†³å®šåç»­æ˜¯å¦rolling_zscore.
    # æ–¹æ¡ˆ2ï¼Œå…ˆå¯¹log_returnåšrolling_zscore,
    # (rollingçª—å£å€¼æ˜¯2000ï¼Œæš‚æ—¶å½“åšç»éªŒæ€§çš„å‚æ•°ï¼Œå–å€¼çš„è‡ªç”±åº¦æ¥æºäºè©¹æ£®ä¸ç­‰å¼å’Œå¤§æ•°å®šç†ï¼Œéƒ½æ˜¯ç”¨æ•°æ®ç®—å‡ºæ¥çš„)
    # 1. å‚æ•°å’Œå•ç‚¹å¤æ™®çš„å…³ç³»ä¸æ˜ç¡®ï¼Œä½†æ˜¯å’Œå‡ ä¸‡ä¸ªå› å­çš„å¤æ™®åªå’Œï¼Œä»–ä»¬çš„å…³ç³»åº”è¯¥å­˜åœ¨ä¸€å®šçš„å‡¸æ€§ï¼›
    # 2. å‚æ•°çš„è®¾ç½®åº”è¯¥åœ¨ç»´æŒ1çš„å‰æä¸‹å…¼é¡¾å¤§æ•°å®šç†ï¼›
    # 3. samplesæ‹†åˆ†æˆä¸ºå‡ ä¸ªclassåæ ·æœ¬é‡ä»ç„¶ç¬¦åˆå¤§æ•°å®šç†ï¼›
    # å¦‚ä¸Šä¸¤ç§æ–¹æ¡ˆçš„å¯¹æ¯”ï¼Œå½“å‰è®¤ä¸ºæ˜¯åº”è¯¥ç¬¬äºŒç§æ–¹å¼ï¼Œåº”è¯¥æ˜¯èƒ½å¤Ÿä¿ç•™ä¸€éƒ¨åˆ†outlierçš„ä¿¡æ¯ï¼Œç›¸å¯¹å¹³è¡¡çš„å‡è½»outlierçš„å½±å“



    if output_format == 'ndarry':
        y_dataset_train = z_train['ret_rolling_zscore'].values
        y_dataset_test = z_test['ret_rolling_zscore'].values
        ret_dataset_train = z_train['return_f'].values
        ret_dataset_test = z_test['return_f'].values
        # é‡è¦ï¼è¦åˆ é™¤æ‰åŒ…å«æœªæ¥ä¿¡æ¯çš„å­—æ®µï¼Œretï¼Œret_rolling_zscore
        z_train.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z_test.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)

        X_all = np.where(np.isnan(z), 0, z)
        X_dataset_train = np.where(np.isnan(z_train), 0, z_train)
        X_dataset_test = np.where(np.isnan(z_test), 0, z_test)

    elif output_format == 'dataframe':
        y_dataset_train = z_train['ret_rolling_zscore']
        y_dataset_test = z_test['ret_rolling_zscore']
        ret_dataset_train = z_train['return_f']
        ret_dataset_test = z_test['return_f']
        # é‡è¦ï¼è¦åˆ é™¤æ‰åŒ…å«æœªæ¥ä¿¡æ¯çš„å­—æ®µï¼Œretï¼Œret_rolling_zscore
        z_train.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z_test.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)
        z.drop(['return_f', 'ret_rolling_zscore'], axis=1, inplace=True)

        X_all = z.fillna(0)
        X_dataset_train = z_train.fillna(0)
        X_dataset_test = z_test.fillna(0)

    else:
        print(
            'output_format of data_prepare should be "ndarry" or "dataframe", pls check it ')
        exit(1)

    feature_names = z_train.columns

    print('æ£€æŸ¥x allæ˜¯ä¸æ˜¯ç­‰äº x trainå’Œy trainç›¸åŠ ï¼Œå†æ£€æŸ¥trianå’Œtestä»¥åŠcloseå’Œopençš„å½¢çŠ¶æ˜¯å¦ä¸€è‡´')
    # X_all ä¸“é—¨æ˜¯ä¸ºåšbatch predictionçš„æ—¶å€™ï¼Œè¦ç”¨X_allç”Ÿæˆtesté›†è¦ç”¨åˆ°çš„factor_df, å› ä¸ºfactorçš„è®¡ç®—éœ€è¦ä¹‹å‰ä¸€æ®µwindowä¸­çš„featureå€¼
    print(f'æ£€æŸ¥X_allçš„å½¢çŠ¶ {X_all.shape}')
    print(f'æ£€æŸ¥x dataset trainçš„å½¢çŠ¶ {X_dataset_train.shape}')
    print(f'æ£€æŸ¥y dataset trainçš„å½¢çŠ¶ {y_dataset_train.shape}')
    print(f'æ£€æŸ¥x allæ˜¯ä¸æ˜¯ç­‰äºtrainå’Œtestç›¸åŠ  {len(X_all)},{len(X_dataset_test)+len(X_dataset_train)}')
    print(f'æ£€æŸ¥open trainçš„å½¢çŠ¶ {open_train.shape}')
    print(f'æ£€æŸ¥close trainçš„å½¢çŠ¶ {close_train.shape}')
    print(f'æ£€æŸ¥x dataset testçš„å½¢çŠ¶ {X_dataset_test.shape}')
    print(f'æ£€æŸ¥y dataset testçš„å½¢çŠ¶ {y_dataset_test.shape}')
    print(f'æ£€æŸ¥open testçš„å½¢çŠ¶ {open_test.shape}')
    print(f'æ£€æŸ¥close testçš„å½¢çŠ¶ {close_test.shape}')

    # X_all ä¸“é—¨æ˜¯ä¸ºåšbatch predictionçš„æ—¶å€™ï¼Œè¦ç”¨X_allç”Ÿæˆtesté›†è¦ç”¨åˆ°çš„factor_df, å› ä¸ºfactorçš„è®¡ç®—éœ€è¦ä¹‹å‰ä¸€æ®µwindowä¸­çš„featureå€¼
    return X_all, X_dataset_train, y_dataset_train,ret_dataset_train, X_dataset_test, y_dataset_test,ret_dataset_test, feature_names,open_train,open_test,close_train,close_test, z.index ,ohlcva_df


def _compute_vectorized_labels(timestamps, z_raw, prediction_horizon_td):
    """
    å‘é‡åŒ–è®¡ç®—æ ‡ç­¾ï¼Œé¿å…é€ä¸ªæŸ¥è¯¢ä»·æ ¼
    
    è¿”å›: DataFrame with columns ['timestamp', 't_price', 't_future_price', 'return_f']
    """
    # å°†z_rawçš„æ”¶ç›˜ä»·è½¬æ¢ä¸ºSeriesï¼Œæ–¹ä¾¿reindex
    close_prices = z_raw['c']
    
    # æ‰¹é‡è·å–å½“å‰æ—¶åˆ»å’Œæœªæ¥æ—¶åˆ»çš„ä»·æ ¼
    timestamps_series = pd.Series(timestamps)
    future_timestamps = timestamps_series + prediction_horizon_td
    
    # reindexæ¥åŒ¹é…æ—¶é—´ç‚¹ï¼ˆmethod='ffill'ç¡®ä¿æ‰¾åˆ°æœ€è¿‘çš„ä»·æ ¼ï¼‰
    t_prices = close_prices.reindex(timestamps, method='ffill')
    t_future_prices = close_prices.reindex(future_timestamps, method='ffill')
    
    # è®¡ç®—å¯¹æ•°æ”¶ç›Š
    log_returns = np.log(t_future_prices.values / t_prices.values)
    
    # æ„å»ºDataFrame
    labels_df = pd.DataFrame({
        'timestamp': timestamps,
        't_price': t_prices.values,
        't_future_price': t_future_prices.values,
        'return_f': log_returns
    })
    
    return labels_df


def _process_single_timestamp(args):
    """
    å¤„ç†å•ä¸ªæ—¶é—´ç‚¹çš„ç‰¹å¾æå–å’Œæ ‡ç­¾è®¡ç®—ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰
    
    è¿”å›: (sample_dict, success_flag)
    """
    (t, z_raw, coarse_grain_period, feature_window_timedelta, 
     feature_lookback_bars, prediction_horizon_td) = args
    
    try:
        # ========== æ»‘åŠ¨çª—å£ç‰¹å¾æå– ==========
        feature_window_start = t - feature_window_timedelta
        feature_window_end = t
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´
        if feature_window_start < z_raw.index.min():
            return None, False
        
        if feature_window_end > z_raw.index.max():
            return None, False
        
        if t + prediction_horizon_td >= z_raw.index.max():
            return None, False
        
        # ä»åŸå§‹æ•°æ®ä¸­æå–è¿™ä¸ªæ—¶é—´ç‚¹ä¸“å±çš„çª—å£æ•°æ®
        window_raw_data = z_raw[(z_raw.index >= feature_window_start) & 
                                (z_raw.index < feature_window_end)]
        
        if len(window_raw_data) < 10:
            return None, False
        
        # å¯¹çª—å£æ•°æ®è¿›è¡Œç²—ç²’åº¦é‡é‡‡æ ·
        window_coarse_bars = resample(window_raw_data, coarse_grain_period)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç²—ç²’åº¦æ¡¶
        if len(window_coarse_bars) < feature_lookback_bars * 0.5:
            return None, False
        
        # ä¸ºè¿™ä¸ªçª—å£çš„ç²—ç²’åº¦æ¡¶æå–ç‰¹å¾
        base_feature = originalFeature.BaseFeature(window_coarse_bars.copy())
        window_features_df = base_feature.init_feature_df
        
        # å¯¹çª—å£å†…çš„ç‰¹å¾è¿›è¡Œèšåˆï¼ˆå¤šç§ç»Ÿè®¡é‡ï¼‰
        feature_dict = {}
        for col in window_features_df.columns:
            if col in ['c', 'v', 'o', 'h', 'l', 'vol']:
                continue
            if pd.api.types.is_numeric_dtype(window_features_df[col]):
                col_data = window_features_df[col]
                n = len(col_data)
                
                # åŸºç¡€ç»Ÿè®¡é‡ï¼ˆä½¿ç”¨numpyåŠ é€Ÿï¼‰
                feature_dict[f'{col}_mean'] = np.mean(col_data)
                feature_dict[f'{col}_std'] = np.std(col_data)
                feature_dict[f'{col}_max'] = np.max(col_data)
                feature_dict[f'{col}_min'] = np.min(col_data)
                feature_dict[f'{col}_last'] = col_data.iloc[-1] if n > 0 else 0
                
                # é«˜é˜¶ç»Ÿè®¡é‡
                feature_dict[f'{col}_skew'] = col_data.skew() if n > 2 else 0
                feature_dict[f'{col}_kurt'] = col_data.kurtosis() if n > 3 else 0
                
                # åˆ†ä½æ•°
                feature_dict[f'{col}_median'] = np.median(col_data)
                feature_dict[f'{col}_q25'] = np.percentile(col_data, 25) if n > 0 else 0
                feature_dict[f'{col}_q75'] = np.percentile(col_data, 75) if n > 0 else 0
        
        # è®¡ç®—æ ‡ç­¾ï¼ˆä»·æ ¼å’Œæ”¶ç›Šï¼‰
        t_price = z_raw.loc[t, 'c']
        t_future = t + prediction_horizon_td
        t_future_price = z_raw.loc[t_future, 'c']
        log_return = np.log(t_future_price / t_price)
        
        # è®°å½•æ ·æœ¬
        sample = {
            'timestamp': t,
            't_price': t_price,
            't_future_price': t_future_price,
            'return_f': log_return,
            **feature_dict
        }
        
        return sample, True
        
    except Exception as e:
        # ç‰¹å¾æå–å¤±è´¥
        return None, False


def _process_timestamp_with_multi_offset_precompute_v2(args):
    """
    æ­£ç¡®çš„ä¼˜åŒ–æ–¹æ¡ˆï¼šå¤šç»„åç§»çš„ç²—ç²’åº¦é¢„è®¡ç®—
    
    ä¼˜åŒ–æ€è·¯ï¼š
    1. é¢„è®¡ç®—Nç»„ä¸åŒåç§»çš„2hæ¡¶ï¼ˆN = 2h/rolling_stepï¼‰
    2. æ¯ä¸ªæ—¶é—´ç‚¹æ ¹æ®å…¶åˆ†é’Ÿåç§»é€‰æ‹©å¯¹åº”çš„ç»„
    3. ä»é€‰å®šç»„çš„é¢„è®¡ç®—ç‰¹å¾ä¸­é€‰æ‹©çª—å£æ¡¶
    
    å…³é”®ï¼š
    - 9:15æ—¶åˆ» â†’ é€‰æ‹©offset=15minçš„ç»„ â†’ ä½¿ç”¨è¾¹ç•Œ[1:15, 3:15, 5:15, 7:15, 9:15]çš„æ¡¶
    - 9:30æ—¶åˆ» â†’ é€‰æ‹©offset=30minçš„ç»„ â†’ ä½¿ç”¨è¾¹ç•Œ[1:30, 3:30, 5:30, 7:30, 9:30]çš„æ¡¶
    - å®Œç¾å¯¹é½ï¼Œæ— ç²¾åº¦æŸå¤±ï¼
    
    è¿”å›: (sample_dict, success_flag)
    """
    (t, z_raw, coarse_features_dict, rolling_step_minutes,
     feature_window_timedelta, feature_lookback_bars, prediction_horizon_td) = args
    
    try:
        # è®¡ç®—å½“å‰æ—¶é—´ç‚¹çš„åˆ†é’Ÿåç§»ï¼ˆç›¸å¯¹äºæ•´ç‚¹ï¼‰
        step = int(rolling_step_minutes)
        offset_minutes = (t.minute // step) * step  # 0, 15, 30, 45...
        offset = pd.Timedelta(minutes=offset_minutes)
        
        coarse_features_df = coarse_features_dict[offset]
        
        # ğŸš€ ä¼˜åŒ–ï¼šreturn_få·²ç»åœ¨é¢„è®¡ç®—é˜¶æ®µè®¡ç®—å¥½äº†ï¼Œç›´æ¥ä½¿ç”¨å³å¯
        # ä¸éœ€è¦é‡å¤è®¡ç®—ï¼Œå¤§å¹…æå‡æ€§èƒ½ï¼
        
        # æ’é™¤åŸºç¡€ä»·æ ¼åˆ—
        numeric_cols = coarse_features_df.select_dtypes(include=[np.number]).columns
        exclude_cols = {'c', 'v', 'o', 'h', 'l', 'vol'}
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        # é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆåŒ…å«é¢„è®¡ç®—çš„return_fç­‰æ ‡ç­¾ï¼‰
        window_coarse_features = coarse_features_df[feature_cols].copy()
        
        # æ·»åŠ offsetæ ‡è¯†
        window_coarse_features['feature_offset'] = offset_minutes
        
        # ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨row_timestampsä½œä¸ºindexï¼Œé¿å…åç»­concatæ—¶çš„é¢å¤–å¤„ç†
        # window_coarse_featuresçš„indexå·²ç»æ˜¯row_timestamps
        
        # print(f'window_coarse_features offset_minutes={offset_minutes}, len = {len(window_coarse_features)}')
        return window_coarse_features, True
        
    except Exception as e:
        return None, False
    

# def _process_timestamp_with_multi_offset_precompute(args):
#     """
#     æ­£ç¡®çš„ä¼˜åŒ–æ–¹æ¡ˆï¼šå¤šç»„åç§»çš„ç²—ç²’åº¦é¢„è®¡ç®—
    
#     ä¼˜åŒ–æ€è·¯ï¼š
#     1. é¢„è®¡ç®—Nç»„ä¸åŒåç§»çš„2hæ¡¶ï¼ˆN = 2h/rolling_stepï¼‰
#     2. æ¯ä¸ªæ—¶é—´ç‚¹æ ¹æ®å…¶åˆ†é’Ÿåç§»é€‰æ‹©å¯¹åº”çš„ç»„
#     3. ä»é€‰å®šç»„çš„é¢„è®¡ç®—ç‰¹å¾ä¸­é€‰æ‹©çª—å£æ¡¶
    
#     å…³é”®ï¼š
#     - 9:15æ—¶åˆ» â†’ é€‰æ‹©offset=15minçš„ç»„ â†’ ä½¿ç”¨è¾¹ç•Œ[1:15, 3:15, 5:15, 7:15, 9:15]çš„æ¡¶
#     - 9:30æ—¶åˆ» â†’ é€‰æ‹©offset=30minçš„ç»„ â†’ ä½¿ç”¨è¾¹ç•Œ[1:30, 3:30, 5:30, 7:30, 9:30]çš„æ¡¶
#     - å®Œç¾å¯¹é½ï¼Œæ— ç²¾åº¦æŸå¤±ï¼
    
#     è¿”å›: (sample_dict, success_flag)
#     """
#     (t, z_raw, coarse_features_dict, rolling_step_minutes,
#      feature_window_timedelta, feature_lookback_bars, prediction_horizon_td) = args
    
#     try:
#         # è®¡ç®—å½“å‰æ—¶é—´ç‚¹çš„åˆ†é’Ÿåç§»ï¼ˆç›¸å¯¹äºæ•´ç‚¹ï¼‰
#         step = int(rolling_step_minutes)
#         offset_minutes = (t.minute // step) * step  # 0, 15, 30, 45...
#         offset = pd.Timedelta(minutes=offset_minutes)
        
#         # é€‰æ‹©å¯¹åº”çš„é¢„è®¡ç®—ç‰¹å¾ç»„
#         # if offset not in coarse_features_dict:
#         #     # æ‰¾æœ€æ¥è¿‘çš„offset
#         #     available_offsets = sorted(coarse_features_dict.keys())
#         #     offset = min(available_offsets, key=lambda x: abs((x - offset).total_seconds()))
        
#         coarse_features_df = coarse_features_dict[offset]
        
#         window_coarse_features = coarse_features_df

#         # è®¡ç®—çª—å£èŒƒå›´
#         # feature_window_start = t - feature_window_timedelta
#         # feature_window_end = t
        
#         # # è¾¹ç•Œæ£€æŸ¥
#         # if (feature_window_start < coarse_features_df.index.min() or 
#         #     feature_window_end > coarse_features_df.index.max() or
#         #     t + prediction_horizon_td >= z_raw.index.max()):
#         #     return None, False
        
#         # # ========== å…³é”®ï¼šä»é€‰å®šç»„çš„é¢„è®¡ç®—ç‰¹å¾ä¸­é€‰æ‹©çª—å£ ==========
#         # window_coarse_features = coarse_features_df[
#         #     (coarse_features_df.index >= feature_window_start) & 
#         #     (coarse_features_df.index < feature_window_end)
#         # ]
        
#         # # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ¡¶
#         # if len(window_coarse_features) < feature_lookback_bars * 0.5:
#         #     return None, False
        

#         # ========== å¯¹çª—å£å†…çš„ç²—ç²’åº¦æ¡¶ç‰¹å¾è¿›è¡Œèšåˆç»Ÿè®¡ ==========
#         feature_dict = {}
        
#         # æ’é™¤åŸºç¡€ä»·æ ¼åˆ—
#         numeric_cols = window_coarse_features.select_dtypes(include=[np.number]).columns
#         exclude_cols = {'c', 'v', 'o', 'h', 'l', 'vol'}
#         feature_cols = [col for col in numeric_cols if col not in exclude_cols]
        
#         # feature_dict[f'feature_{offset_minutes}'] = window_coarse_features

#         for col in feature_cols:
#             col_data = window_coarse_features[col]
#             n = len(col_data)
#             if n > 0:
#                 feature_dict[f'{col}'] = col_data
        

#         # è®¡ç®—æ ‡ç­¾
#         t_price = z_raw.loc[t, 'c']
#         t_future = t + prediction_horizon_td
#         t_future_price = z_raw.loc[t_future, 'c']
        
#         # norm_return = norm(t_future_price / t_price, window = 200, clip = 6)
#         # log_return = np.log(t_future_price / t_price)

#         return_f = np.log(t_future_price / t_price)
#         # t_future_price / t_price
#         return_p = t_future_price / t_price


#         sample = {
#             'timestamp': t,
#             't_price': t_price,
#             't_future_price': t_future_price,
#             'return_p' : return_p,
#             'feature_offset': offset_minutes,
#             'return_f': return_f,
#             **feature_dict
#         }
        
#         return sample, True
        
#     except Exception as e:
#         return None, False


def data_prepare_coarse_grain_rolling(
        sym: str, 
        freq: str,  # é¢„æµ‹å‘¨æœŸï¼Œä¾‹å¦‚ '2h' è¡¨ç¤ºé¢„æµ‹æœªæ¥2å°æ—¶æ”¶ç›Š
        start_date_train: str, 
        end_date_train: str,
        start_date_test: str, 
        end_date_test: str,
        coarse_grain_period: str = '2h',  # ç²—ç²’åº¦ç‰¹å¾æ¡¶å‘¨æœŸ
        feature_lookback_bars: int = 8,    # ç‰¹å¾å›æº¯æ¡¶æ•°ï¼ˆ8ä¸ª2h = 16å°æ—¶ï¼‰
        rolling_step: str = '15min',       # æ»šåŠ¨æ­¥é•¿
        y_train_ret_period: int = 8,       # é¢„æµ‹å‘¨æœŸï¼ˆä»¥coarse_grainä¸ºå•ä½ï¼Œ1è¡¨ç¤º1ä¸ª2hï¼‰
        rolling_w: int = 2000,
        output_format: str = 'ndarry',
        data_dir: str = '',
        read_frequency: str = '',
        timeframe: str = '',
        file_path: Optional[str] = None,
        use_parallel: bool = True,  # æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
        n_jobs: int = -1,  # å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        use_fine_grain_precompute: bool = True,  # æ˜¯å¦ä½¿ç”¨ç»†ç²’åº¦é¢„è®¡ç®—ä¼˜åŒ–
        include_categories: List[str] = None
    ):
    """
    ç²—ç²’åº¦ç‰¹å¾ + ç»†ç²’åº¦æ»šåŠ¨çš„æ•°æ®å‡†å¤‡æ–¹æ³•ï¼ˆæ»‘åŠ¨çª—å£ç‰ˆæœ¬ï¼‰
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - ç‰¹å¾ä½¿ç”¨ç²—ç²’åº¦å‘¨æœŸï¼ˆå¦‚2å°æ—¶ï¼‰èšåˆï¼Œå‡å°‘å™ªå£°
    - ç‰¹å¾çª—å£ä½¿ç”¨å›ºå®šæ—¶é—´é•¿åº¦ï¼ˆå¦‚8ä¸ª2å°æ—¶ = 16å°æ—¶ï¼‰
    - é¢„æµ‹èµ·ç‚¹ä»¥ç»†ç²’åº¦æ­¥é•¿æ»šåŠ¨ï¼ˆå¦‚15åˆ†é’Ÿï¼‰ï¼Œäº§ç”Ÿé«˜é¢‘æ ·æœ¬
    - **å…³é”®æ”¹è¿›**ï¼šæ¯ä¸ªæ»šåŠ¨æ—¶é—´ç‚¹éƒ½ç‹¬ç«‹è®¡ç®—å…¶ä¸“å±çš„æ»‘åŠ¨çª—å£ç‰¹å¾ï¼Œé¿å…å¤šä¸ªæ ·æœ¬é‡å¤ä½¿ç”¨ç›¸åŒçš„ç²—ç²’åº¦æ¡¶
    - é¢„æµ‹ç›®æ ‡æ˜¯æœªæ¥Nä¸ªç²—ç²’åº¦å‘¨æœŸçš„æ”¶ç›Šï¼ˆå¦‚æœªæ¥2å°æ—¶ï¼‰
    
    å‚æ•°è¯´æ˜ï¼š
    - sym: äº¤æ˜“å¯¹ç¬¦å·
    - freq: ç”¨äºå…¼å®¹ï¼Œå®é™…é¢„æµ‹å‘¨æœŸç”± y_train_ret_period * coarse_grain_period å†³å®š
    - coarse_grain_period: ç²—ç²’åº¦ç‰¹å¾æ¡¶å‘¨æœŸï¼Œå¦‚ '2h', '1h', '30min'
    - feature_lookback_bars: ç‰¹å¾å›æº¯çš„ç²—ç²’åº¦æ¡¶æ•°é‡ï¼ˆå¦‚8è¡¨ç¤º8ä¸ª2hæ¡¶ï¼‰
    - rolling_step: æ»šåŠ¨æ­¥é•¿ï¼Œå¦‚ '15min', '10min', '5min'
    - y_train_ret_period: é¢„æµ‹å‘¨æœŸæ•°ï¼ˆä»¥rolling_stepä¸ºå•ä½ï¼‰
    
    ç¤ºä¾‹åœºæ™¯ï¼ˆæ»‘åŠ¨çª—å£ï¼‰ï¼š
    - coarse_grain_period='2h', feature_lookback_bars=8, rolling_step='15min'
    - åœ¨9:00æ—¶åˆ»ï¼šä»åŸå§‹æ•°æ®æå– [å‰ä¸€å¤©17:00, 9:00] çš„æ•°æ®ï¼Œé‡é‡‡æ ·ä¸º2hæ¡¶ï¼Œè®¡ç®—ç‰¹å¾ï¼Œé¢„æµ‹9:00-11:00æ”¶ç›Š
    - åœ¨9:15æ—¶åˆ»ï¼šä»åŸå§‹æ•°æ®æå– [å‰ä¸€å¤©17:15, 9:15] çš„æ•°æ®ï¼Œé‡é‡‡æ ·ä¸º2hæ¡¶ï¼Œè®¡ç®—ç‰¹å¾ï¼Œé¢„æµ‹9:15-11:15æ”¶ç›Š
    - åœ¨9:30æ—¶åˆ»ï¼šä»åŸå§‹æ•°æ®æå– [å‰ä¸€å¤©17:30, 9:30] çš„æ•°æ®ï¼Œé‡é‡‡æ ·ä¸º2hæ¡¶ï¼Œè®¡ç®—ç‰¹å¾ï¼Œé¢„æµ‹9:30-11:30æ”¶ç›Š
    
    ä¼˜åŠ¿ï¼š
    - æ¯ä¸ªæ—¶é—´ç‚¹çš„ç‰¹å¾çª—å£éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œé¿å…äº†æ•°æ®æ³„éœ²å’Œæ ·æœ¬ç›¸å…³æ€§é—®é¢˜
    - æ»šåŠ¨æ­¥é•¿å¯ä»¥ä»»æ„è®¾ç½®ï¼Œä¸å—ç²—ç²’åº¦å‘¨æœŸé™åˆ¶
    - ç‰¹å¾æ›´åŠ ç²¾ç»†ï¼Œæ›´èƒ½åæ˜ å®æ—¶å¸‚åœºçŠ¶æ€
    
    è¿”å›ä¸ data_prepare ç›¸åŒçš„æ¥å£
    """
    
    print(f"\n{'='*60}")
    print(f"ç²—ç²’åº¦ç‰¹å¾ + ç»†ç²’åº¦æ»šåŠ¨æ•°æ®å‡†å¤‡ï¼ˆæ»‘åŠ¨çª—å£ç‰ˆæœ¬ï¼‰")
    print(f"å“ç§: {sym}")
    print(f"ç²—ç²’åº¦å‘¨æœŸ: {coarse_grain_period}")
    print(f"ç‰¹å¾çª—å£: {feature_lookback_bars} Ã— {coarse_grain_period} = {feature_lookback_bars * pd.Timedelta(coarse_grain_period).total_seconds() / 3600:.1f}å°æ—¶")
    print(f"æ»šåŠ¨æ­¥é•¿: {rolling_step}")
    print(f"é¢„æµ‹å‘¨æœŸ: {y_train_ret_period} Ã— {rolling_step} = {y_train_ret_period * pd.Timedelta(rolling_step).total_seconds() / 3600:.1f}å°æ—¶")
    print(f"æ³¨æ„ï¼šæ¯ä¸ªæ—¶é—´ç‚¹éƒ½ä¼šç‹¬ç«‹è®¡ç®—å…¶æ»‘åŠ¨çª—å£ç‰¹å¾ï¼Œé¿å…é‡å¤ä½¿ç”¨ç›¸åŒçš„ç²—ç²’åº¦æ¡¶")
    print(f"{'='*60}\n")
    
    # ========== ç¬¬ä¸€æ­¥ï¼šè¯»å–åŸå§‹æ•°æ®ï¼ˆç»†ç²’åº¦ï¼‰ ==========
    z_raw = data_load_v2(sym, data_dir=data_dir, start_date=start_date_train, end_date=end_date_test,
                         timeframe=timeframe, read_frequency=read_frequency, file_path=file_path)
    z_raw.index = pd.to_datetime(z_raw.index)
    
    # æ‰©å±•æ•°æ®èŒƒå›´ä»¥å®¹çº³ç‰¹å¾çª—å£
    feature_window_timedelta = pd.Timedelta(coarse_grain_period) * feature_lookback_bars
    # extended_start = pd.to_datetime(start_date_train) - feature_window_timedelta - pd.Timedelta('1d')  # å¤šç•™1å¤©buffer
    
    # z_raw = z_raw[(z_raw.index >= extended_start) & (z_raw.index <= pd.to_datetime(end_date_test))]
    
    z_raw = z_raw[(z_raw.index >= pd.to_datetime(start_date_train)) 
                  & (z_raw.index <= pd.to_datetime(end_date_test))]  # åªæˆªå–å‚æ•°æŒ‡å®šéƒ¨åˆ†dataframe
    
    print(f"è¯»å–åŸå§‹æ•°æ®: {len(z_raw)} è¡Œï¼Œæ—¶é—´èŒƒå›´ {z_raw.index.min()} è‡³ {z_raw.index.max()}")
    
    # ========== ç¬¬äºŒæ­¥ï¼šé¢„è®¡ç®—ç²—ç²’åº¦æ¡¶ç‰¹å¾ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰ ==========
    coarse_features_dict = {}  # å­—å…¸ï¼š{offset: features_df}
    
    if use_fine_grain_precompute:
        print(f"\nğŸš€ å¯ç”¨ç²—ç²’åº¦é¢„è®¡ç®—ä¼˜åŒ–")
        
        # è®¡ç®—éœ€è¦å¤šå°‘ç»„ä¸åŒåç§»çš„resample
        coarse_period_minutes = pd.Timedelta(coarse_grain_period).total_seconds() / 60
        rolling_step_minutes = pd.Timedelta(rolling_step).total_seconds() / 60
        num_offsets = int(coarse_period_minutes / rolling_step_minutes)
        
        print(f"ç²—ç²’åº¦å‘¨æœŸ: {coarse_grain_period} ({coarse_period_minutes}åˆ†é’Ÿ)")
        print(f"æ»šåŠ¨æ­¥é•¿: {rolling_step} ({rolling_step_minutes}åˆ†é’Ÿ)")
        print(f"éœ€è¦é¢„è®¡ç®— {num_offsets} ç»„ä¸åŒåç§»çš„ç²—ç²’åº¦æ¡¶")
        
        samples = []
        prediction_horizon_td = pd.Timedelta(rolling_step) * y_train_ret_period
        
        for i in range(num_offsets):
            offset = pd.Timedelta(minutes=i * rolling_step_minutes)
            print(f"\nç»„{i}: åç§» {offset} ...")
            
            # å¯¹æ•°æ®è¿›è¡Œåç§»ï¼Œç„¶åresample
            z_raw_offset = z_raw.copy()
            z_raw_offset.index = z_raw_offset.index - offset
            
            # Resampleï¼ˆè¾¹ç•Œä¼šè‡ªåŠ¨å¯¹é½åˆ°0:00, 2:00, 4:00...ï¼‰
            coarse_bars = resample(z_raw_offset, coarse_grain_period)
            
            # æ¢å¤åŸå§‹æ—¶é—´
            coarse_bars.index = coarse_bars.index + offset
            
            # ğŸ”§ ä¿®å¤ï¼šè¿‡æ»¤æ‰è¶…å‡ºåŸå§‹æ•°æ®èŒƒå›´çš„æ¡¶
            original_start = z_raw.index.min()
            original_end = z_raw.index.max()
            coarse_bars = coarse_bars[
                (coarse_bars.index >= original_start) & 
                (coarse_bars.index <= original_end)
            ]

            print(f"  - æ¡¶æ•°é‡: {len(coarse_bars)}")
            print(f"  - è®¡ç®—BaseFeature...")
            
            # è®¡ç®—ç‰¹å¾
            base_feature = originalFeature.BaseFeature(coarse_bars.copy(), include_categories = include_categories, rolling_zscore_window = rolling_w)
            features_df = base_feature.init_feature_df
            
            # ğŸš€ ä¼˜åŒ–ï¼šé¢„è®¡ç®—return_fï¼Œé¿å…æ¯ä¸ªæ—¶é—´ç‚¹é‡å¤è®¡ç®—
            print(f"  - é¢„è®¡ç®—return_f...")
            row_timestamps = features_df.index
            
            # å‘é‡åŒ–è·å–å½“å‰æ—¶åˆ»çš„ä»·æ ¼
            t_prices = z_raw['c'].reindex(row_timestamps)
            
            # å‘é‡åŒ–è®¡ç®—æœªæ¥æ—¶åˆ»
            future_timestamps = row_timestamps + prediction_horizon_td
            
            # å‘é‡åŒ–è·å–æœªæ¥æ—¶åˆ»çš„ä»·æ ¼ï¼ˆè¶Šç•Œè‡ªåŠ¨ä¸ºnanï¼‰
            t_future_prices = z_raw['c'].reindex(future_timestamps)
            
            # å‘é‡åŒ–è®¡ç®—æ”¶ç›Šç‡
            return_p = (t_future_prices.values / t_prices.values)
            return_f = np.log(return_p)
            
            # å°†æ ‡ç­¾æ·»åŠ åˆ°features_df
            features_df['t_price'] = t_prices.values
            features_df['t_future_price'] = t_future_prices.values
            features_df['return_p'] = return_p
            features_df['return_f'] = return_f
            
            # å­˜å‚¨            
            features_df['feature_offset'] = offset.total_seconds() / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ

            coarse_features_dict[offset] = features_df
            samples.append(features_df)

            print(f"  âœ“ ç»„{i}å®Œæˆ: {len(features_df)} ä¸ªæ¡¶, {len(features_df.columns)} ä¸ªç‰¹å¾")
        
        print(f"\nâœ“ é¢„è®¡ç®—å®Œæˆ: {num_offsets} ç»„ç²—ç²’åº¦ç‰¹å¾")
        print(f"ä¼˜åŒ–ç­–ç•¥: æ¯ä¸ªæ—¶é—´ç‚¹æ ¹æ®å…¶offseté€‰æ‹©å¯¹åº”ç»„çš„é¢„è®¡ç®—ç‰¹å¾")
    else:
        print(f"\nä½¿ç”¨åŸå§‹æ–¹æ¡ˆï¼ˆæ¯ä¸ªæ—¶é—´ç‚¹ç‹¬ç«‹è®¡ç®—ï¼‰")
    
    # ========== ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç»†ç²’åº¦æ»šåŠ¨æ—¶é—´ç½‘æ ¼ ==========
    print(f"\nç”Ÿæˆç»†ç²’åº¦æ»šåŠ¨æ—¶é—´ç½‘æ ¼ï¼ˆæ­¥é•¿={rolling_step}ï¼‰...")
    
    # ä»è®­ç»ƒé›†å¼€å§‹åˆ°æµ‹è¯•é›†ç»“æŸï¼ŒæŒ‰rolling_stepç”Ÿæˆæ—¶é—´ç‚¹
    # grid_start = pd.to_datetime(start_date_train)
    # grid_end = pd.to_datetime(end_date_test)
    
    # # ç”Ÿæˆæ—¶é—´ç½‘æ ¼
    # fine_grain_timestamps = pd.date_range(start=grid_start, end=grid_end, freq=rolling_step)
    # print(f"ç”Ÿæˆ {len(fine_grain_timestamps)} ä¸ªæ—¶é—´ç‚¹")
    
    # ========== ç¬¬å››æ­¥ï¼šä¸ºæ¯ä¸ªç»†ç²’åº¦æ—¶é—´ç‚¹æå–æ»‘åŠ¨çª—å£ç‰¹å¾å’Œæ ‡ç­¾ ==========
    print(f"\nä¸ºæ¯ä¸ªæ—¶é—´ç‚¹æå–æ»‘åŠ¨çª—å£ç‰¹å¾å’Œæ ‡ç­¾...")
    
    # coarse_period_td = pd.Timedelta(coarse_grain_period)
    
    # ========== ç¬¬äº”æ­¥ï¼šå¤„ç†æ ·æœ¬ ==========
    # if use_fine_grain_precompute:
    #     # ğŸš€ è¶…çº§ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„15ç»„æ•°æ®ï¼Œä¸éœ€è¦å†éå†æ—¶é—´ç‚¹ï¼
    #     print(f"\nâœ¨ ä½¿ç”¨è¶…çº§ä¼˜åŒ–æ–¹æ¡ˆ: ç›´æ¥ä½¿ç”¨é¢„è®¡ç®—æ•°æ®")
    #     print(f"è·³è¿‡æ—¶é—´ç‚¹éå†ï¼Œç›´æ¥åˆå¹¶{len(coarse_features_dict)}ç»„é¢„è®¡ç®—ç‰¹å¾")
        
    #     samples = []
    #     for offset, features_df in coarse_features_dict.items():
    #         # ä¸ºæ¯ç»„æ•°æ®æ·»åŠ offsetæ ‡è¯†
    #         df_copy = features_df.copy()
    #         df_copy['feature_offset'] = offset.total_seconds() / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
    #         samples.append(df_copy)
        
    #     valid_count = sum(len(s) for s in samples)
    #     skipped_count = 0
    #     print(f"âœ“ ç›´æ¥ä½¿ç”¨é¢„è®¡ç®—æ•°æ®: {len(samples)}ç»„, æ€»è®¡ {valid_count} è¡Œ")
        
    # else:
    #     # åŸå§‹æ–¹æ¡ˆï¼šéœ€è¦éå†æ¯ä¸ªæ—¶é—´ç‚¹
    #     process_func = _process_single_timestamp
    #     print(f"\nä½¿ç”¨åŸå§‹æ–¹æ¡ˆ: æ¯ä¸ªæ—¶é—´ç‚¹ç‹¬ç«‹è®¡ç®—")
    
    #     # é€‰æ‹©å¤„ç†æ¨¡å¼ï¼šå¹¶è¡Œæˆ–ä¸²è¡Œ
    #     if use_parallel:
    #         # ========== å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼ˆä¼˜åŒ–chunksizeï¼‰ ==========
    #         # n_cores = cpu_count() if n_jobs == -1 else n_jobs
    #         n_cores = 1

    #         # åŠ¨æ€ä¼˜åŒ– chunksizeï¼ˆä¿ç•™è¿™ä¸ªä¼˜åŒ–ï¼‰
    #         # optimal_chunksize = max(1, len(fine_grain_timestamps) // (n_cores * 4))
    #         # optimal_chunksize = min(optimal_chunksize, 1)
    #         optimal_chunksize = 1

    #         print(f"ğŸš€ ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼Œè¿›ç¨‹æ•°: {n_cores}, ä¼˜åŒ–chunksize: {optimal_chunksize}")
            
    #         # å‡†å¤‡å‚æ•°åˆ—è¡¨ï¼ˆåŸå§‹æ–¹æ¡ˆä½¿ç”¨å•ä¸ªæ—¶é—´ç‚¹å¤„ç†ï¼‰
    #         args_list = [
    #             (t, z_raw, coarse_grain_period, feature_window_timedelta, 
    #              feature_lookback_bars, prediction_horizon_td)
    #             for t in fine_grain_timestamps
    #         ]
            
    #         # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
    #         with Pool(processes=n_cores) as pool:
    #             results = []
    #             # ä½¿ç”¨imap_unorderedæé«˜æ•ˆç‡ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦
    #             if HAS_TQDM:
    #                 # ä½¿ç”¨tqdmè¿›åº¦æ¡ï¼ˆæ›´å‹å¥½ï¼‰
    #                 iterator = tqdm(
    #                     pool.imap_unordered(process_func, args_list, 
    #                                       chunksize=optimal_chunksize),
    #                     total=len(fine_grain_timestamps),
    #                     desc="å¹¶è¡Œå¤„ç†",
    #                     unit="æ ·æœ¬"
    #                 )
    #                 for result in iterator:
    #                     results.append(result)
    #             else:
    #                 # é™çº§ä¸ºç®€å•çš„ç™¾åˆ†æ¯”æ˜¾ç¤º
    #                 for idx, result in enumerate(pool.imap_unordered(
    #                     process_func, args_list, 
    #                     chunksize=optimal_chunksize)):
    #                     results.append(result)
    #                     if idx % 100 == 0:
    #                         print(f"  å¤„ç†è¿›åº¦: {idx}/{len(fine_grain_timestamps)} ({100*idx/len(fine_grain_timestamps):.1f}%)")
            
    #         # æ”¶é›†æˆåŠŸçš„æ ·æœ¬
    #         samples = [sample for sample, success in results if success]
    #         valid_count = len(samples)
    #         skipped_count = len(results) - valid_count
            
    #         print(f"\nâœ“ å¹¶è¡Œå¤„ç†å®Œæˆ: æœ‰æ•ˆ {valid_count} ä¸ªï¼Œè·³è¿‡ {skipped_count} ä¸ª")
        
    #     else:
    #         # ========== ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼ˆç”¨äºè°ƒè¯•ï¼‰==========
    #         print(f"ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼ˆå•çº¿ç¨‹ï¼‰")
            
    #         samples = []
    #         valid_count = 0
    #         skipped_count = 0

    #         # é€‰æ‹©è¿›åº¦æ˜¾ç¤ºæ–¹å¼
    #         iterator = tqdm(fine_grain_timestamps, desc="ä¸²è¡Œå¤„ç†", unit="æ ·æœ¬") if HAS_TQDM else fine_grain_timestamps
            
    #         for idx, t in enumerate(iterator):
    #             # å¦‚æœæ²¡æœ‰tqdmï¼Œæ˜¾ç¤ºç®€å•è¿›åº¦
    #             if not HAS_TQDM and idx % 50 == 0:
    #                 print(f"  å¤„ç†è¿›åº¦: {idx}/{len(fine_grain_timestamps)} ({100*idx/len(fine_grain_timestamps):.1f}%)")
                
    #             # åŸå§‹æ–¹æ¡ˆçš„å‚æ•°
    #             args = (t, z_raw, coarse_grain_period, feature_window_timedelta, 
    #                    feature_lookback_bars, prediction_horizon_td)
                
    #             sample, success = process_func(args)
                
    #             if success:
    #                 samples.append(sample)
    #                 valid_count += 1
    #             else:
    #                 skipped_count += 1
            
    #         print(f"\nâœ“ ä¸²è¡Œå¤„ç†å®Œæˆ: æœ‰æ•ˆ {valid_count} ä¸ªï¼Œè·³è¿‡ {skipped_count} ä¸ª")
    
    # ========== ç¬¬å…­æ­¥ï¼šæ„å»ºDataFrameå¹¶å¤„ç† ==========
    print(f"\nåˆå¹¶æ ·æœ¬æ•°æ®...")
    
    # æ£€æŸ¥samplesçš„ç±»å‹ï¼Œä½¿ç”¨ä¸åŒçš„åˆå¹¶ç­–ç•¥
    if len(samples) > 0 and isinstance(samples[0], pd.DataFrame):
        # ä¼˜åŒ–è·¯å¾„ï¼šsamplesæ˜¯DataFrameåˆ—è¡¨ï¼ˆæ¥è‡ª_process_timestamp_with_multi_offset_precompute_v2ï¼‰
        # ä½¿ç”¨pd.concatä¼šæ¯”pd.DataFrameå¿«å¾ˆå¤š
        print(f"  ä½¿ç”¨pd.concatåˆå¹¶{len(samples)}ä¸ªDataFrame...")
        df_samples = pd.concat(samples, axis=0, ignore_index=False, copy=False)
        df_samples.sort_index(inplace=True)
        df_samples.dropna(inplace=True)
    else:
        # ä¼ ç»Ÿè·¯å¾„ï¼šsamplesæ˜¯dictåˆ—è¡¨ï¼ˆæ¥è‡ª_process_single_timestampï¼‰
        print(f"  ä½¿ç”¨pd.DataFrameåˆå¹¶{len(samples)}ä¸ªæ ·æœ¬...")
        df_samples = pd.DataFrame(samples)
        df_samples.set_index('timestamp', inplace=True)
        df_samples.sort_index(inplace=True)
    
    print(f"âœ“ åˆå¹¶å®Œæˆ")
    
    print(f"æ ·æœ¬æ—¶é—´èŒƒå›´: {df_samples.index.min()} è‡³ {df_samples.index.max()}")
    print(f"æ ·æœ¬æ•°é‡: {len(df_samples)}")
    print(f"ç‰¹å¾ç»´åº¦: {len([c for c in df_samples.columns if c not in ['t_price', 't_future_price', 'return_f']])}")
    
    # åº”ç”¨æ»šåŠ¨æ ‡å‡†åŒ–åˆ°æ ‡ç­¾
    def norm_ret(x, window=rolling_w):
        x = np.log1p(np.asarray(x))
        factors_data = pd.DataFrame(x, columns=['factor'])
        factors_data = factors_data.replace([np.inf, -np.inf, np.nan], 0.0)
        factors_std = factors_data.rolling(window=window, min_periods=1).std()
        factor_value = factors_data / factors_std
        factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
        return np.nan_to_num(factor_value).flatten()
    
    # ä½¿ç”¨ä¸ rolling_w ä¸€è‡´çš„ windowï¼Œç¡®ä¿åç»­ inverse_norm èƒ½æ­£ç¡®åŒ¹é…
    # norm_window = rolling_w  # ä½¿ç”¨é…ç½®çš„ rolling_w

    #  return_f = np.log(t_future_price / t_price)
    #  

    # df_samples['ret_rolling_zscore'] = norm_ret(df_samples['return_f'].values, window=norm_window)
    df_samples['ret_rolling_zscore'] = norm(df_samples['return_p'].values, window=rolling_w, clip=6)
    # df_samples['return_f'] = df_samples['ret_rolling_zscore']
    
    print(f"âœ“ ä½¿ç”¨ norm(window={rolling_w}) è¿›è¡Œæ ‡å‡†åŒ–")
    # df_samples['ret_rolling_zscore'] = norm(df_samples['return_f'].values, window = 200, clip = 6)
    # df_samples['ret_rolling_zscore'] = norm_ret(df_samples['return_f'].values)
    
    print(f"\næ ‡ç­¾ç»Ÿè®¡:")
    print(f"return_f - ååº¦: {df_samples['return_f'].skew():.4f}, å³°åº¦: {df_samples['return_f'].kurtosis():.4f}")
    print(f"ret_rolling_zscore - ååº¦: {df_samples['ret_rolling_zscore'].skew():.4f}, å³°åº¦: {df_samples['ret_rolling_zscore'].kurtosis():.4f}")
    
    # ========== ç¬¬ä¸ƒæ­¥ï¼šåˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† ==========
    train_mask = (df_samples.index >= pd.to_datetime(start_date_train)) & \
                 (df_samples.index < pd.to_datetime(end_date_train))
    test_mask = (df_samples.index >= pd.to_datetime(start_date_test)) & \
                (df_samples.index <= pd.to_datetime(end_date_test))
    
    # æå–ç‰¹å¾åˆ—
    feature_cols = [c for c in df_samples.columns if c not in ['t_price', 't_future_price', 'return_f', 'ret_rolling_zscore', 'return_p']]
    
    X_all = df_samples[feature_cols].fillna(0)
    X_train = X_all[train_mask]
    X_test = X_all[test_mask]
    
    y_train = df_samples.loc[train_mask, 'ret_rolling_zscore'].fillna(0).values
    y_test = df_samples.loc[test_mask, 'ret_rolling_zscore'].fillna(0).values
    ret_train = df_samples.loc[train_mask, 'return_f'].fillna(0).values
    ret_test = df_samples.loc[test_mask, 'return_f'].fillna(0).values

    y_p_train_origin = df_samples.loc[train_mask, 'return_p'].fillna(0).values
    y_p_test_origin = df_samples.loc[test_mask, 'return_p'].fillna(0).values

    
    # ä»·æ ¼æ•°æ®ï¼ˆç”¨äºå›æµ‹ï¼‰
    open_train = df_samples.loc[train_mask, 't_price']
    close_train = df_samples.loc[train_mask, 't_price']  # ç®€åŒ–ï¼šå¼€ç›˜ä»·=å½“å‰ä»·
    open_test = df_samples.loc[test_mask, 't_price']
    close_test = df_samples.loc[test_mask, 't_price']
    
    feature_names = feature_cols
    
    # æ ¼å¼è½¬æ¢
    if output_format == 'ndarry':
        X_all = X_all.values
        X_train = X_train.values
        X_test = X_test.values
    elif output_format == 'dataframe':
        pass  # ä¿æŒDataFrameæ ¼å¼
    else:
        raise ValueError(f"output_format åº”ä¸º 'ndarry' æˆ– 'dataframe'ï¼Œå½“å‰ä¸º {output_format}")
    
    print(f"\n{'='*60}")
    print(f"æ•°æ®åˆ†å‰²å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    print(f"  ç‰¹å¾æ•°: {len(feature_names)}")
    print(f"{'='*60}\n")
    
    # æ„å»º ohlc DataFrameï¼ˆç”¨äºåç»­åˆ†æï¼Œå¦‚IC decayï¼‰
    # åŒ…å«æ¯ä¸ªæ ·æœ¬çš„ä»·æ ¼ä¿¡æ¯ï¼Œä¸ X_all å¯¹é½
    ohlc_aligned = pd.DataFrame({
        'c': df_samples['t_price'],  # å½“å‰ä»·æ ¼
        'close': df_samples['t_price']  # å…¼å®¹æ€§åˆ«å
    }, index=df_samples.index)
    
    # è¿”å›æ¥å£ä¸ data_prepare ä¿æŒä¸€è‡´
    return (X_all, X_train, y_train, ret_train, X_test, y_test, ret_test,
            feature_names, open_train, open_test, close_train, close_test,
            df_samples.index, ohlc_aligned, y_p_train_origin, y_p_test_origin)


# def data_prepare_micro(sym: str, freq: str,
#                        start_date_train: str, end_date_train: str,
#                        start_date_test: str, end_date_test: str,
#                        y_train_ret_period: int = 1, rolling_w: int = 2000,
#                        output_format: str = 'ndarry', data_dir: str = '',
#                        read_frequency: str = '', timeframe: str = '',
#                        use_feature_extractors: bool = False,
#                        trades_dir: str = '',
#                        bar_builder: str = 'time',
#                        dollar_threshold: float = 1e6,
#                        feature_window_bars: int = 10,
#                        trades_load_mode: str = 'auto',
#                        prefer_trades_feather: bool = True,
#                        daily_data_template: str = '',
#                        monthly_data_template: str = '',
#                        active_family: str = '',
#                        feature_family_include: list = None,
#                        feature_family_exclude: list = None,
#                        file_path: Optional[str] = None,
#                        kline_file_path: Optional[str] = None):
#     """
#     å¾®ç»“æ„ç‰ˆæ•°æ®å‡†å¤‡ï¼š
    
#     æ•°æ®æµç¨‹ï¼š
#     1. è¯»å– tick çº§åˆ«äº¤æ˜“æ•°æ®ï¼ˆtradesï¼‰
#     2. ä» trades æ„å»º barsï¼ˆTimeBar æˆ– DollarBarï¼‰å¾—åˆ° OHLCV
#     3. ä» bars çš„ OHLCV ç”Ÿæˆæ ‡ç­¾ï¼ˆreturn_fï¼‰
#     4. æå–å¾®è§‚ç»“æ„ç‰¹å¾ï¼ˆåŸºäº trades å’Œ barsï¼‰
    
#     å‚æ•°è¯´æ˜ï¼š
#     - file_path: tickäº¤æ˜“æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
#     - kline_file_path: Kçº¿æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ ‡ç­¾ç”Ÿæˆçš„å¤‡ç”¨æ–¹æ¡ˆï¼Œå½“æ— tickæ•°æ®æ—¶ï¼‰
#     - daily_data_template/monthly_data_template: tickæ•°æ®æ¨¡æ¿è·¯å¾„
    
#     è¿”å›ç­¾åä¿æŒä¸ data_prepare å®Œå…¨ä¸€è‡´ï¼Œä¾¿äº GPAnalyzer ç›´æ¥æ›¿æ¢ä½¿ç”¨
#     """
    
#     # ========== ç¬¬ä¸€æ­¥ï¼šè¯»å– Tick äº¤æ˜“æ•°æ® ==========
#     trades_df = None
    
#     # ä¼˜å…ˆçº§1: ç›´æ¥æŒ‡å®šçš„ tick æ–‡ä»¶è·¯å¾„
#     if file_path:
#         print(f"\n{'='*60}")
#         print(f"ä»æŒ‡å®šè·¯å¾„è¯»å– Tick æ•°æ®: {file_path}")
#         print(f"{'='*60}\n")
#         trades_df = _read_tick_data_file(file_path)
    
#     # ä¼˜å…ˆçº§2: ä½¿ç”¨ TradingPipeline ä»æ¨¡æ¿è¯»å–
#     elif daily_data_template or monthly_data_template:
#         try:
#             tp_config = {
#                 'data': {
#                     'load_mode': trades_load_mode or 'auto',
#                     'prefer_feather': bool(prefer_trades_feather),
#                 }
#             }
#             tp = TradingPipeline(tp_config)
            
#             # è§„èŒƒåŒ–æ—¥æœŸä¸º YYYY-MM-DD
#             def _normalize_date(s: str) -> str:
#                 if len(str(s)) == 7 and '-' in s:  # 'YYYY-MM'
#                     return f"{s}-01"
#                 return str(s)
            
#             start_norm = _normalize_date(start_date_train)
#             end_norm = _normalize_date(end_date_test)
            
#             trades_df = tp.load_data(
#                 trades_data=None,
#                 date_range=(start_norm, end_norm),
#                 daily_data_template=daily_data_template if daily_data_template else None,
#                 monthly_data_template=monthly_data_template if monthly_data_template else None,
#             )
#             print(f"âœ“ æˆåŠŸä»æ¨¡æ¿è¯»å– {len(trades_df):,} æ¡ Tick æ•°æ®")
#         except Exception as e:
#             print(f"âš ï¸  ä»æ¨¡æ¿è¯»å– Tick æ•°æ®å¤±è´¥: {e}")
    
#     # å¦‚æœæ²¡æœ‰ tick æ•°æ®ï¼Œå›é€€åˆ° Kçº¿æ•°æ®
#     if trades_df is None or trades_df.empty:
#         print(f"\n{'='*60}")
#         print("âš ï¸  æœªæ‰¾åˆ° Tick æ•°æ®ï¼Œå›é€€åˆ° Kçº¿æ•°æ®æ¨¡å¼")
#         print(f"{'='*60}\n")
        
#         # ä» Kçº¿æ•°æ®è¯»å–
#         z_raw = data_load_v2(sym, data_dir=data_dir, start_date=start_date_train, end_date=end_date_test,
#                             timeframe=timeframe, read_frequency=read_frequency, file_path=kline_file_path)
#         z_raw.index = pd.to_datetime(z_raw.index)
#         z_raw = z_raw[(z_raw.index >= pd.to_datetime(start_date_train)) & (z_raw.index <= pd.to_datetime(end_date_test))]
#         ohlcva_df = resample(z_raw, freq)
#         bars = None  # æ ‡è®°æ²¡æœ‰æ„å»º bars
#     else:
#         # ========== ç¬¬äºŒæ­¥ï¼šä» Tick æ•°æ®æ„å»º Bars ==========
#         print(f"\n{'='*60}")
#         print(f"ä» Tick æ•°æ®æ„å»º Bars (builder={bar_builder}, freq={freq})")
#         print(f"{'='*60}\n")
        
#         if str(bar_builder).lower() == 'dollar':
#             db = DollarBarBuilder(dollar_threshold=float(dollar_threshold))
#             bars = db.process(trades_df)
#             print(f"âœ“ æ„å»ºäº† {len(bars)} ä¸ª Dollar Bars (threshold={dollar_threshold:,.0f})")
#         else:
#             try:
#                 tb = TimeBarBuilder(freq=freq)
#                 bars = tb.process(trades_df)
#             except Exception as e:
#                 print(f"âš ï¸  æ„å»º Time Bars å¤±è´¥: {e}")
#             print(f"âœ“ æ„å»ºäº† {len(bars)} ä¸ª Time Bars (freq={freq})")
        
#         # è¿‡æ»¤æ—¶é—´èŒƒå›´
#         # bars = bars[(pd.to_datetime(bars['end_time']) >= pd.to_datetime(start_date_train)) &
#         #             (pd.to_datetime(bars['end_time']) <= pd.to_datetime(end_date_test))]
        
#         # è®¾ç½®æ—¶é—´ç´¢å¼•
#         bars.index = pd.to_datetime(bars['end_time'])
#         ohlcva_df = None  # æ ‡è®°ä½¿ç”¨ bars
        
#         print(f"âœ“ æ„å»ºäº† {len(bars)} ä¸ª Bars")
#         print(f"æ—¶é—´èŒƒå›´: {bars.index.min()} è‡³ {bars.index.max()}\n")

#     # ========== ç¬¬ä¸‰æ­¥ï¼šç»Ÿä¸€æ•°æ®æ¥å£ï¼ˆbars æˆ– ohlcva_dfï¼‰==========
#     # å¦‚æœæœ‰ barsï¼Œç›´æ¥ä½¿ç”¨ barsï¼›å¦åˆ™ä½¿ç”¨ ohlcva_dfï¼ˆKçº¿å›é€€æ¨¡å¼ï¼‰
#     if bars is not None:
#         data_df = bars
#         close = data_df['close'].astype(float)
#     else:
#         data_df = ohlcva_df
#         close = data_df['c'].astype(float)
    
#     eps = 1e-12

#     # å®¶æ—æ„å›¾è§£æï¼ˆæ§åˆ¶â€œåªè®¡ç®—å¯ç”¨å®¶æ—â€ï¼‰
#     def _to_list(v):
#         if v is None:
#             return []
#         return v if isinstance(v, list) else [v]

#     include_keys = _to_list(feature_family_include)
#     exclude_keys = _to_list(feature_family_exclude)

#     fam = str(active_family).strip().lower() if isinstance(active_family, str) else ''
#     family_map = {
#         'momentum': ['bar_logret', 'bar_abs_logret', 'micro_dp_short', 'micro_dp_zscore'],
#         'liquidity': ['amihud', 'kyle', 'hasbrouck', 'cs_spread', 'impact_roll', 'bar_amihud'],
#         'impact': ['amihud', 'kyle', 'hasbrouck', 'impact_'],
#     }

#     enabled_families: set = set()
#     if fam in family_map:
#         enabled_families.add(fam)
#     elif include_keys:
#         for fam_name, keys in family_map.items():
#             if any(any(k in key or key in k for key in keys) for k in include_keys):
#                 enabled_families.add(fam_name)

#     # å¾®ç»“æ„ä»£ç†ç‰¹å¾ï¼ˆbarçº§ï¼‰- æš‚æ—¶ä¸ºç©ºDataFrameï¼Œåç»­ç”±ç‰¹å¾æå–å™¨å¡«å……
#     f = pd.DataFrame(index=data_df.index)

#     # ========== ç¬¬å››æ­¥ï¼šæå–å¾®è§‚ç»“æ„ç‰¹å¾ï¼ˆå¦‚æœæœ‰ tick æ•°æ®å’Œ barsï¼‰==========
#     extracted_df = None
#     if use_feature_extractors and trades_df is not None and bars is not None and not bars.empty:
#         print(f"\n{'='*60}")
#         print("å¼€å§‹æå–å¾®è§‚ç»“æ„ç‰¹å¾")
#         print(f"{'='*60}\n")
        
#         try:
#             # æ„å»ºäº¤æ˜“ä¸Šä¸‹æ–‡
#             tp = TradesProcessor()
#             ctx = tp.build_context(trades_df)
            
#             # ä»¥ trading_pipeline çš„çª—å£æ–¹å¼æå–ï¼šå¯¹æ¯ä¸ªç›®æ ‡bar iï¼Œç”¨ [i-feature_window_bars, i-1] ä½œä¸ºç‰¹å¾çª—å£
#             bars = bars.reset_index(drop=True)
#             bars['end_time'] = pd.to_datetime(bars['end_time'])
            
#             # æ„é€ ä»…å¯ç”¨å®¶æ—çš„æå–å™¨é…ç½®
#             extractor_cfg: Dict[str, Any] = {}
#             if enabled_families:
#                 # å…ˆå…¨éƒ¨ç¦ç”¨ï¼Œå†æŒ‰éœ€å¯ç”¨
#                 for k in ['basic','volatility','momentum','orderflow','impact','tail','path_shape','bucketed_flow']:
#                     extractor_cfg[k] = {'enabled': False}
#                 for fam_name in enabled_families:
#                     if fam_name in extractor_cfg:
#                         extractor_cfg[fam_name] = {'enabled': True}
            
#             extractor = MicrostructureFeatureExtractor(extractor_cfg if extractor_cfg else None)
            
#             feat_rows = []
#             feat_index = []
#             total_bars = len(bars)
#             print(f"å¼€å§‹é€ bar æå–ç‰¹å¾ï¼Œå…± {total_bars} ä¸ª bars...")
            
#             for i in range(total_bars):
#                 if i % 100 == 0 and i > 0:
#                     print(f"  è¿›åº¦: {i}/{total_bars} ({i/total_bars*100:.1f}%)")
                
#                 start_idx = i - int(feature_window_bars)
#                 end_idx = i - 1
#                 if start_idx < 0 or end_idx < 0 or end_idx < start_idx:
#                     continue
                
#                 st = pd.to_datetime(bars.loc[start_idx, 'start_time'])
#                 et = pd.to_datetime(bars.loc[end_idx, 'end_time'])
                
#                 try:
#                     feats = extractor.extract_from_context(
#                         ctx=ctx,
#                         start_ts=st,
#                         end_ts=et,
#                         bars=bars,
#                         bar_window_start_idx=start_idx,
#                         bar_window_end_idx=end_idx,
#                     )
#                 except Exception:
#                     feats = {}
                
#                 feat_rows.append(feats)
#                 feat_index.append(pd.to_datetime(bars.loc[i, 'end_time']))
            
#             print(f"âœ“ ç‰¹å¾æå–å®Œæˆï¼Œå…±æå– {len(feat_rows)} ä¸ªæ ·æœ¬")
            
#             extracted_df = pd.DataFrame(feat_rows, index=pd.to_datetime(feat_index))
            
#             # ç¼ºå¤±ä¸æå€¼å¤„ç†
#             print(f"å¤„ç†ç¼ºå¤±å€¼å’Œæå€¼...")
#             extracted_df = extracted_df.replace([np.inf, -np.inf], np.nan)
#             try:
#                 lower2 = extracted_df.quantile(0.01)
#                 upper2 = extracted_df.quantile(0.99)
#                 extracted_df = extracted_df.clip(lower=lower2, upper=upper2, axis=1)
#             except Exception:
#                 pass
#             extracted_df = extracted_df.fillna(0.0)
#             print(f"âœ“ æå–äº† {len(extracted_df.columns)} ä¸ªå¾®è§‚ç»“æ„ç‰¹å¾\n")
            
#         except Exception as e:
#             print(f"âš ï¸  åŸºäº features/ ç‰¹å¾æå–å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨ bar çº§ä»£ç†ç‰¹å¾ï¼š{e}\n")

#     # ========== ç¬¬äº”æ­¥ï¼šç”Ÿæˆæ ‡ç­¾ ==========
#     z_lab = pd.DataFrame(index=data_df.index)
#     z_lab['return_f'] = np.log(close).diff(y_train_ret_period).shift(-y_train_ret_period)
#     z_lab['return_f'] = z_lab['return_f'].fillna(0)

#     def _norm_ret(x, window=rolling_w):
#         x = np.log1p(np.asarray(x))
#         x = np.nan_to_num(x)
#         std = pd.Series(x).rolling(window=window, min_periods=1).std().replace(0, np.nan)
#         val = pd.Series(x) / std
#         return np.nan_to_num(val).values

#     z_lab['ret_rolling_zscore'] = _norm_ret(z_lab['return_f'])

#     # ========== ç¬¬å…­æ­¥ï¼šåˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† ==========
#     # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©åˆ—å
#     if bars is not None:
#         open_col, close_col = 'open', 'close'
#     else:
#         open_col, close_col = 'o', 'c'
    
#     open_train = data_df[open_col][(data_df[open_col].index >= pd.to_datetime(start_date_train)) & (data_df[open_col].index < pd.to_datetime(end_date_train))]
#     open_test = data_df[open_col][(data_df[open_col].index >= pd.to_datetime(start_date_test)) & (data_df[open_col].index <= pd.to_datetime(end_date_test))]
#     close_train = data_df[close_col][(data_df[close_col].index >= pd.to_datetime(start_date_train)) & (data_df[close_col].index < pd.to_datetime(end_date_train))]
#     close_test = data_df[close_col][(data_df[close_col].index >= pd.to_datetime(start_date_test)) & (data_df[close_col].index <= pd.to_datetime(end_date_test))]

#     # åˆ‡åˆ† train/test
#     z_train = pd.concat([f, z_lab], axis=1)
#     z_test = z_train.copy()
#     z_train = z_train[(z_train.index >= pd.to_datetime(start_date_train)) & (z_train.index < pd.to_datetime(end_date_train))]
#     z_test = z_test[(z_test.index >= pd.to_datetime(start_date_test)) & (z_test.index <= pd.to_datetime(end_date_test))]

#     # y/ret
#     y_dataset_train = z_train['ret_rolling_zscore'].values
#     y_dataset_test = z_test['ret_rolling_zscore'].values
#     ret_dataset_train = z_train['return_f'].values
#     ret_dataset_test = z_test['return_f'].values

#     # åˆ é™¤åŒ…å«æœªæ¥ä¿¡æ¯çš„åˆ—
#     z_train = z_train.drop(['return_f', 'ret_rolling_zscore'], axis=1)
#     z_test = z_test.drop(['return_f', 'ret_rolling_zscore'], axis=1)
#     f_all = pd.concat([f], axis=1)

#     # X_all/X_train/X_test
#     if output_format == 'ndarry':
#         X_all = np.where(np.isnan(f_all), 0, f_all)
#         X_dataset_train = np.where(np.isnan(z_train), 0, z_train)
#         X_dataset_test = np.where(np.isnan(z_test), 0, z_test)
#     elif output_format == 'dataframe':
#         X_all = f_all.fillna(0)
#         X_dataset_train = z_train.fillna(0)
#         X_dataset_test = z_test.fillna(0)
#     else:
#         print('output_format of data_prepare_micro should be "ndarry" or "dataframe"')
#         exit(1)

#     feature_names = z_train.columns

#     # ä¸ºäº†å…¼å®¹æ€§ï¼Œå¦‚æœä½¿ç”¨ barsï¼Œéœ€è¦åˆ›å»º ohlcva_df æ ¼å¼çš„è¿”å›å€¼
#     if bars is not None:
#         # åˆ›å»ºå…¼å®¹æ ¼å¼çš„ ohlcva_df
#         ohlcva_df = data_df[['open', 'high', 'low', 'close', 'volume', 'dollar_value', 'trades']].copy()
#         ohlcva_df.columns = ['o', 'h', 'l', 'c', 'vol', 'vol_ccy', 'trades']
#         for col in ['oi', 'oi_ccy', 'toptrader_count_lsr', 'toptrader_oi_lsr', 'count_lsr', 'taker_vol_lsr']:
#             ohlcva_df[col] = 0

    return (X_all, X_dataset_train, y_dataset_train, ret_dataset_train,
            X_dataset_test, y_dataset_test, ret_dataset_test,
            feature_names, open_train, open_test, close_train, close_test,
            z_lab.index, ohlcva_df)


def _filter_features_by_type_and_keywords(feature_cols: List[str], 
                                        feature_types: Optional[List[str]] = None,
                                        feature_keywords: Optional[List[str]] = None) -> List[str]:
    """
    æ ¹æ®ç‰¹å¾ç±»å‹å’Œå…³é”®è¯ç­›é€‰ç‰¹å¾åˆ—
    
    å‚æ•°:
    - feature_cols: æ‰€æœ‰ç‰¹å¾åˆ—ååˆ—è¡¨
    - feature_types: ç‰¹å¾ç±»å‹ç­›é€‰ï¼Œå¦‚['momentum', 'tail', 'orderflow', 'impact', 'volatility', 'basic', 'path_shape']
    - feature_keywords: å…³é”®è¯ç­›é€‰ï¼Œå¦‚['lambda', 'large', 'signed', 'imbalance']ç­‰
    
    è¿”å›:
    - ç­›é€‰åçš„ç‰¹å¾åˆ—ååˆ—è¡¨
    """
    if not feature_types and not feature_keywords:
        return feature_cols
    
    # å®šä¹‰ç‰¹å¾ç±»å‹æ˜ å°„
    feature_type_mapping = {
        'momentum': ['mr_', 'momentum', 'reversion', 'dp_short', 'dp_zscore'],
        'tail': ['large_', 'q90', 'q95', 'q99', 'sweep', 'burstiness'],
        'orderflow': ['ofi_', 'gof_', 'signed_', 'imbalance', 'flow'],
        'impact': ['lambda', 'amihud', 'kyle', 'hasbrouck', 'impact'],
        'volatility': ['rv', 'bpv', 'jump', 'volatility', 'std'],
        'basic': ['int_trade_', 'vwap', 'volume', 'dollar', 'intensity'],
        'path_shape': ['vwap_deviation', 'corr_', 'path', 'shape', 'deviation']
    }
    
    filtered_cols = []
    
    for col in feature_cols:
        col_lower = col.lower()

        # if col_lower != 'bar_gof_by_count':
        #     continue
        
        include_col = True
        
        # ç‰¹å¾ç±»å‹ç­›é€‰
        if feature_types:
            type_match = False
            for feature_type in feature_types:
                if feature_type in feature_type_mapping:
                    keywords = feature_type_mapping[feature_type]
                    if any(keyword in col_lower for keyword in keywords):
                        type_match = True
                        break
            if not type_match:
                include_col = False
        
        # å…³é”®è¯ç­›é€‰
        if include_col and feature_keywords:
            keyword_match = any(keyword.lower() in col_lower for keyword in feature_keywords)
            if not keyword_match:
                include_col = False
        
        if include_col and not col.startswith('cs_'):
            filtered_cols.append(col)
    
    return filtered_cols


def data_prepare_rolling(sym: str, freq: str,
                        start_date_train: str, end_date_train: str,
                        start_date_test: str, end_date_test: str,
                        y_train_ret_period: int = 1, rolling_w: int = 2000,
                        feature_window_bars: int = 10,
                        output_format: str = 'ndarry', data_dir: str = '',
                        read_frequency: str = '', timeframe: str = '',
                        file_path: Optional[str] = None,
                        rolling_windows: Optional[List[int]] = None,
                        use_rolling_aggregator: bool = True,
                        feature_types: Optional[List[str]] = None,
                        feature_keywords: Optional[List[str]] = None):
    """
    æ»šåŠ¨ç»Ÿè®¡ç‰ˆæ•°æ®å‡†å¤‡ï¼š
    
    æ•°æ®æµç¨‹ï¼š
    1. è¯»å–å·²èšåˆçš„barçº§æ•°æ®ï¼ˆåŒ…å«OHLCVå’Œå¾®è§‚ç»“æ„å› å­ï¼‰
    2. ä»baræ•°æ®ç”Ÿæˆæ ‡ç­¾ï¼ˆreturn_fï¼‰
    3. ä½¿ç”¨RollingAggregatorå¯¹barçº§ç‰¹å¾è¿›è¡Œæ»šåŠ¨ç»Ÿè®¡
    4. è¿”å›ä¸data_prepare_microå®Œå…¨ä¸€è‡´çš„æ¥å£
    
    å‚æ•°è¯´æ˜ï¼š
    - file_path: å·²èšåˆçš„baræ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«OHLCVå’Œå¾®è§‚ç»“æ„å› å­ï¼‰
    - rolling_windows: æ»šåŠ¨çª—å£åˆ—è¡¨ï¼Œå¦‚[5, 10, 20]è¡¨ç¤º5barã€10barã€20barçª—å£
    - use_rolling_aggregator: æ˜¯å¦ä½¿ç”¨RollingAggregatorè¿›è¡Œæ»šåŠ¨ç»Ÿè®¡
    - feature_types: ç‰¹å¾ç±»å‹ç­›é€‰ï¼Œå¦‚['momentum', 'tail', 'orderflow', 'impact', 'volatility', 'basic', 'path_shape']
    - feature_keywords: å…³é”®è¯ç­›é€‰ï¼Œå¦‚['lambda', 'large', 'signed', 'imbalance']ç­‰
    
    è¿”å›ç­¾åä¿æŒä¸data_prepare_microå®Œå…¨ä¸€è‡´ï¼Œä¾¿äºGPAnalyzerç›´æ¥æ›¿æ¢ä½¿ç”¨
    """
    
    print(f"\n{'='*60}")
    print(f"æ»šåŠ¨ç»Ÿè®¡ç‰ˆæ•°æ®å‡†å¤‡: {sym}")
    print(f"æ—¶é—´èŒƒå›´: {start_date_train} è‡³ {end_date_test}")
    print(f"æ»šåŠ¨çª—å£: {rolling_windows or [rolling_w]}")
    if feature_types:
        print(f"ç‰¹å¾ç±»å‹ç­›é€‰: {feature_types}")
    if feature_keywords:
        print(f"å…³é”®è¯ç­›é€‰: {feature_keywords}")
    print(f"{'='*60}\n")
    
    # ========== ç¬¬ä¸€æ­¥ï¼šè¯»å–å·²èšåˆçš„Baræ•°æ® ==========
    if file_path:
        print(f"ä»æŒ‡å®šè·¯å¾„è¯»å–Baræ•°æ®: {file_path}")
        try:
            if file_path.endswith('.feather'):
                bars_df = pd.read_feather(file_path)
            elif file_path.endswith('.csv'):
                bars_df = pd.read_csv(file_path)
            elif file_path.endswith('.parquet'):
                bars_df = pd.read_parquet(file_path)
            else:
                # å°è¯•è‡ªåŠ¨æ£€æµ‹æ ¼å¼
                bars_df = pd.read_feather(file_path)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            raise
    else:
        # å›é€€åˆ°ä¼ ç»ŸKçº¿æ•°æ®è¯»å–
        print("æœªæŒ‡å®šfile_pathï¼Œå›é€€åˆ°ä¼ ç»ŸKçº¿æ•°æ®è¯»å–")
        z_raw = data_load_v2(sym, data_dir=data_dir, start_date=start_date_train, end_date=end_date_test,
                            timeframe=timeframe, read_frequency=read_frequency)
        z_raw.index = pd.to_datetime(z_raw.index)
        z_raw = z_raw[(z_raw.index >= pd.to_datetime(start_date_train)) & (z_raw.index <= pd.to_datetime(end_date_test))]
        bars_df = resample(z_raw, freq)
        # é‡å‘½ååˆ—ä»¥åŒ¹é…æ ‡å‡†æ ¼å¼
        if 'o' in bars_df.columns:
            bars_df = bars_df.rename(columns={'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'vol': 'volume'})
    
    # ç¡®ä¿æ—¶é—´ç´¢å¼•æ­£ç¡®
    if 'timestamp' in bars_df.columns:
        bars_df.index = pd.to_datetime(bars_df['timestamp'])
    elif 'end_time' in bars_df.columns:
        bars_df.index = pd.to_datetime(bars_df['end_time'])
    elif 'start_time' in bars_df.columns:
        bars_df.index = pd.to_datetime(bars_df['start_time'])
    else:
        bars_df.index = pd.to_datetime(bars_df.index)
    
    # è¿‡æ»¤æ—¶é—´èŒƒå›´
    bars_df = bars_df[(bars_df.index >= pd.to_datetime(start_date_train)) & 
                     (bars_df.index <= pd.to_datetime(end_date_test))]
    
    print(f"âœ“ è¯»å–äº† {len(bars_df)} ä¸ªBaræ•°æ®")
    print(f"æ—¶é—´èŒƒå›´: {bars_df.index.min()} è‡³ {bars_df.index.max()}")
    print(f"æ•°æ®åˆ—: {list(bars_df.columns)}")
    
    # ========== ç¬¬äºŒæ­¥ï¼šç”Ÿæˆæ ‡ç­¾ ==========
    print(f"\nå¼€å§‹ç”Ÿæˆæ ‡ç­¾ (ret_period={y_train_ret_period})")
    
    # ç¡®ä¿æœ‰closeåˆ—
    if 'close' not in bars_df.columns and 'c' in bars_df.columns:
        bars_df['close'] = bars_df['c']
    elif 'close' not in bars_df.columns:
        raise ValueError("æ•°æ®ä¸­å¿…é¡»åŒ…å«closeæˆ–cåˆ—")
    
    close = bars_df['close'].astype(float)
    
    # ç”Ÿæˆæ”¶ç›Šç‡æ ‡ç­¾
    bars_df['return_f'] = np.log(close).diff(y_train_ret_period).shift(-y_train_ret_period)
    bars_df['return_f'] = bars_df['return_f'].fillna(0)
    bars_df['r'] = np.log(close).diff()
    bars_df['r'] = bars_df['r'].fillna(0)
    
    # åº”ç”¨æ»šåŠ¨æ ‡å‡†åŒ–ï¼ˆä¸data_prepareä¿æŒä¸€è‡´ï¼‰
    def norm_ret(x, window=rolling_w):
        x = np.log1p(np.asarray(x))
        factors_data = pd.DataFrame(x, columns=['factor'])
        factors_data = factors_data.replace([np.inf, -np.inf, np.nan], 0.0)
        factors_std = factors_data.rolling(window=window, min_periods=1).std()
        factor_value = factors_data / factors_std
        factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
        return np.nan_to_num(factor_value).flatten()
    
    bars_df['ret_rolling_zscore'] = norm_ret(bars_df['return_f'])
    
    print(f"âœ“ æ ‡ç­¾ç”Ÿæˆå®Œæˆ")
    print(f"return_f skew = {bars_df['return_f'].skew():.4f}")
    print(f"return_f kurtosis = {bars_df['return_f'].kurtosis():.4f}")
    print(f"ret_rolling_zscore skew = {bars_df['ret_rolling_zscore'].skew():.4f}")
    print(f"ret_rolling_zscore kurtosis = {bars_df['ret_rolling_zscore'].kurtosis():.4f}")
    
    # ========== ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡ç‰¹å¾æ•°æ® ==========
    print(f"\nå¼€å§‹å‡†å¤‡ç‰¹å¾æ•°æ®")
    
    # è¯†åˆ«å¾®è§‚ç»“æ„ç‰¹å¾åˆ—ï¼ˆæ’é™¤OHLCVå’Œæ ‡ç­¾åˆ—ï¼‰
    exclude_cols = {'open', 'high', 'low', 'close', 'volume', 'vol', 'o', 'h', 'l', 'c', 
                   'return_f', 'r', 'ret_rolling_zscore', 'timestamp', 'start_time', 'end_time'}
    
    # è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾åˆ—
    all_feature_cols = [col for col in bars_df.columns 
                       if col not in exclude_cols and 
                       pd.api.types.is_numeric_dtype(bars_df[col])]
    
    print(f"è¯†åˆ«åˆ° {len(all_feature_cols)} ä¸ªåŸå§‹ç‰¹å¾åˆ—")
    
    # ========== ç‰¹å¾ç±»å‹ç­›é€‰ ==========
    feature_cols = _filter_features_by_type_and_keywords(
        all_feature_cols, feature_types, feature_keywords
    )
    
    print(f"ç­›é€‰åä¿ç•™ {len(feature_cols)} ä¸ªç‰¹å¾åˆ—: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}")
    
    # åˆ›å»ºç‰¹å¾DataFrame
    feature_df = bars_df[feature_cols].copy()
    
    # ========== ç¬¬å››æ­¥ï¼šæ»šåŠ¨ç»Ÿè®¡ç‰¹å¾æå– ==========
    if use_rolling_aggregator and feature_cols:
        print(f"\nå¼€å§‹æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾æå–")
        
        # å¯¼å…¥RollingAggregator
        from features.rolling_aggregator import RollingAggregator
        
        # è®¾ç½®æ»šåŠ¨çª—å£
        windows = [feature_window_bars]
        print(f"ä½¿ç”¨æ»šåŠ¨çª—å£: {windows}")
        
        # åˆ›å»ºRollingAggregatorå®ä¾‹
        rolling_agg = RollingAggregator(windows=windows)
        
        # ä¸ºæ¯ä¸ªç‰¹å¾æ·»åŠ bar_å‰ç¼€ï¼ˆæ¨¡æ‹Ÿbarçº§ç‰¹å¾ï¼‰
        bar_feature_df = feature_df.copy()
        # bar_feature_df.columns = [f'bar_{col}' for col in bar_feature_df.columns]
        
        # ç­›é€‰å‡ºå¸¦bar_å‰ç¼€çš„åˆ—
        bar_feature_df = bar_feature_df[[col for col in bar_feature_df.columns if col.startswith('bar_')]]
        
        # æ”¶é›†æ‰€æœ‰æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        all_rolling_features = []
        
        for window in windows:
            print(f"  å¤„ç†çª—å£ {window}...")
            window_features = []
            
            for i in range(len(bar_feature_df)):
                if i < window:
                    # çª—å£ä¸è¶³ï¼Œè·³è¿‡
                    window_features.append({})
                    continue
                
                # æå–æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
                rolling_stats = rolling_agg.extract_rolling_statistics(
                    bar_feature_df, window=window, current_idx=i
                )
                window_features.append(rolling_stats)
            
            # è½¬æ¢ä¸ºDataFrame
            window_df = pd.DataFrame(window_features, index=bar_feature_df.index)
            all_rolling_features.append(window_df)
        
        # åˆå¹¶æ‰€æœ‰çª—å£çš„ç‰¹å¾
        if all_rolling_features:
            final_feature_df = pd.concat(all_rolling_features, axis=1)
            print(f"âœ“ ç”Ÿæˆäº† {len(final_feature_df.columns)} ä¸ªæ»šåŠ¨ç»Ÿè®¡ç‰¹å¾")
        else:
            final_feature_df = pd.DataFrame(index=feature_df.index)
            print("âš ï¸  æœªç”Ÿæˆä»»ä½•æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾")
    else:
        # ä¸ä½¿ç”¨æ»šåŠ¨ç»Ÿè®¡ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾
        print("ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼ˆæœªè¿›è¡Œæ»šåŠ¨ç»Ÿè®¡ï¼‰")
        final_feature_df = feature_df
    
    # ========== ç¬¬äº”æ­¥ï¼šæ•°æ®åˆ†å‰²å’Œæ ¼å¼åŒ– ==========
    print(f"\nå¼€å§‹æ•°æ®åˆ†å‰²å’Œæ ¼å¼åŒ–")
    
    # ç¡®ä¿ç´¢å¼•ä¸€è‡´
    final_feature_df = final_feature_df.loc[bars_df.index]
    
    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_mask = (final_feature_df.index >= pd.to_datetime(start_date_train)) & \
                 (final_feature_df.index < pd.to_datetime(end_date_train))
    test_mask = (final_feature_df.index >= pd.to_datetime(start_date_test)) & \
                (final_feature_df.index <= pd.to_datetime(end_date_test))
    
    X_train = final_feature_df[train_mask].fillna(0)
    X_test = final_feature_df[test_mask].fillna(0)
    y_train = bars_df.loc[train_mask, 'ret_rolling_zscore'].fillna(0).values
    y_test = bars_df.loc[test_mask, 'ret_rolling_zscore'].fillna(0).values
    ret_train = bars_df.loc[train_mask, 'return_f'].fillna(0).values
    ret_test = bars_df.loc[test_mask, 'return_f'].fillna(0).values
    
    # OHLCæ•°æ®
    ohlc_cols = ['open', 'high', 'low', 'close']
    if all(col in bars_df.columns for col in ohlc_cols):
        open_train = bars_df.loc[train_mask, 'open'].fillna(0)
        open_test = bars_df.loc[test_mask, 'open'].fillna(0)
        close_train = bars_df.loc[train_mask, 'close'].fillna(0)
        close_test = bars_df.loc[test_mask, 'close'].fillna(0)
    else:
        # å›é€€åˆ°cåˆ—
        open_train = bars_df.loc[train_mask, 'close'].fillna(0)  # ç”¨closeä½œä¸ºopençš„è¿‘ä¼¼
        open_test = bars_df.loc[test_mask, 'close'].fillna(0)
        close_train = bars_df.loc[train_mask, 'close'].fillna(0)
        close_test = bars_df.loc[test_mask, 'close'].fillna(0)
    
    # ç‰¹å¾åç§°
    feature_names = list(final_feature_df.columns)
    
    # å®Œæ•´æ•°æ®é›†ï¼ˆç”¨äºæŸäº›åˆ†æï¼‰- å¤„ç†NaNå€¼
    X_all = final_feature_df.fillna(0)
    
    print(f"âœ“ æ•°æ®åˆ†å‰²å®Œæˆ")
    print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬, {len(feature_names)} ç‰¹å¾")
    print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬, {len(feature_names)} ç‰¹å¾")
    
    # ========== ç¬¬å…­æ­¥ï¼šè¿”å›ç»“æœ ==========
    return (X_all, X_train, y_train, ret_train, X_test, y_test, ret_test,
            feature_names, open_train, open_test, close_train, close_test,
            final_feature_df.index, bars_df)


def compute_transformed_series(column):
    """
    è¾“å…¥çš„æ˜¯ä¸€ä¸ªdataframeçš„ä¸€åˆ—ï¼Œseries.
    è®¡ç®—å¾—åˆ°å¦‚ä¸‹å‡ ä¸ªndarry-

    1. log_return: å–log returnã€‚
    2. log_log_returnï¼šå¯¹log_returnå†åšä¸€æ¬¡log.
    3. boxcox_transformed: ä½¿ç”¨Box-Coxå˜æ¢ã€‚
    4. yeo_johnson_transformed: ä½¿ç”¨Yeo-Johnsonå˜æ¢ã€‚
    5. winsorized_log_return: å¯¹log_returnè¿›è¡ŒWinsorizingã€‚
    6. scaled_log_return: å¯¹log_returnè¿›è¡ŒRobustScalerç¼©æ”¾ã€‚

    plotä¸€ä¸ªç›´æ–¹å›¾ï¼Œä¸Šé¢4ç§é¢œè‰²æ˜¾ç¤ºå¦‚ä¸Šå››ç±»æ•°å€¼å„è‡ªçš„ç›´æ–¹åˆ†å¸ƒå›¾.
    å¹¶ä¸”åœ¨å›¾ä¸Šç”»å‡ºå¦‚ä¸Š4ä¸ªåºåˆ—å„è‡ªçš„skewnesså’Œkurtosis
    -------

    """
    log_return = (np.log(column).diff(1).fillna(0)*1).shift(-1)
    log_return = np.where(np.isnan(log_return), 0, log_return)

    # ---------å°è¯•å¯¹log returnåšæ»šåŠ¨æ ‡å‡†åŒ–--------
    log_return = _rolling_zscore(log_return, 300)

    # è®¡ç®— log_log_return
    log_log_return = np.log(log_return + 1)
    # log_log_return_2 = np.log(np.log(column)).diff().fillna(0).shift(-1)

    # å¹³ç§»æ•°æ®ä½¿å…¶ä¸ºæ­£å€¼
    log_return_shifted = log_return - np.min(log_return) + 1
    # åº”ç”¨ Box-Cox å˜æ¢
    boxcox_transformed, _ = boxcox(log_return_shifted)

    # åº”ç”¨ Yeo-Johnson å˜æ¢
    yeo_johnson_transformed, _ = yeojohnson(log_return)

    # åº”ç”¨ Winsorizing
    winsorized_log_return = mstats.winsorize(log_return, limits=[0.05, 0.05])

    # # åº”ç”¨ RobustScaler
    # scaler = RobustScaler()
    # scaled_log_return = scaler.fit_transform(log_return).flatten()

    # ç»˜åˆ¶ç›´æ–¹å›¾
    plt.figure(figsize=(12, 6))

    # ç»˜åˆ¶ log return çš„ç›´æ–¹å›¾
    plt.hist(log_return, bins=160, alpha=0.3, color='blue', label='Log Return')

    # ç»˜åˆ¶ log_log_return çš„ç›´æ–¹å›¾
    plt.hist(log_log_return, bins=160, alpha=0.3,
             color='orange', label='Log Log Return')

    # ç»˜åˆ¶ boxcox_transformed çš„ç›´æ–¹å›¾
    plt.hist(boxcox_transformed, bins=160, alpha=0.3,
             color='green', label='Box-Cox Transformed')

    # ç»˜åˆ¶ yeo_johnson_transformed çš„ç›´æ–¹å›¾
    plt.hist(yeo_johnson_transformed, bins=160, alpha=0.3,
             color='red', label='Yeo-Johnson Transformed')

    # ç»˜åˆ¶ winsorized_log_return çš„ç›´æ–¹å›¾
    plt.hist(winsorized_log_return, bins=160, alpha=0.3,
             color='red', label='Winsorized Transformed')

    # è®¡ç®—å¹¶æ˜¾ç¤º skewness å’Œ kurtosis
    log_return_skewness = skew(log_return)
    log_return_kurtosis = kurtosis(log_return)
    log_log_return_skewness = skew(log_log_return)
    log_log_return_kurtosis = kurtosis(log_log_return)
    boxcox_skewness = skew(boxcox_transformed)
    boxcox_kurtosis = kurtosis(boxcox_transformed)
    yeo_johnson_skewness = skew(yeo_johnson_transformed)
    yeo_johnson_kurtosis = kurtosis(yeo_johnson_transformed)
    winsorized_skewness = skew(winsorized_log_return)
    winsorized_kurtosis = kurtosis(winsorized_log_return)

    # åœ¨å›¾ä¸Šæ˜¾ç¤º skewness å’Œ kurtosis
    plt.text(0.05, 0.95,
             f'Log Return Skewness: {log_return_skewness:.2f}\nLog Return Kurtosis: {log_return_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='blue', alpha=0.5))
    plt.text(0.05, 0.85,
             f'Log Log Return Skewness: {log_log_return_skewness:.2f}\nLog Log Return Kurtosis: {log_log_return_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='orange', alpha=0.5))
    plt.text(0.05, 0.75,
             f'Box-Cox Transformed Skewness: {boxcox_skewness:.2f}\nBox-Cox Transformed Kurtosis: {boxcox_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='green', alpha=0.5))
    plt.text(0.05, 0.65,
             f'Yeo-Johnson Transformed Skewness: {yeo_johnson_skewness:.2f}\nYeo-Johnson Transformed Kurtosis: {yeo_johnson_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='red', alpha=0.5))
    plt.text(0.05, 0.55,
             f'Winsorized Skewness: {winsorized_skewness:.2f}\nWinsorized Kurtosis: {winsorized_kurtosis:.2f}',
             fontsize=10, transform=plt.gca().transAxes, va='top', ha='left', bbox=dict(facecolor='purple', alpha=0.5))

    # æ·»åŠ å›¾ä¾‹å’Œæ ‡ç­¾
    plt.legend()
    plt.title('Histogram of Transformed Series')
    plt.xlabel('Value')
    plt.ylabel('Frequency')

    # æ˜¾ç¤ºå›¾å½¢
    plt.show()


def check_zscore_window_series(df, column, n_values=[50, 100, 200, 250, 300, 450, 600, 1200, 2400, 4800, 9600]):
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))

    if column == 'c':
        # è®¡ç®—åŸå§‹å¯¹æ•°æ”¶ç›Šç‡ï¼Œæ­¤æ—¶ column åº”ä¸º NumPy ndarray
        log_return = (np.log(df[column]).diff(1).fillna(0)*1).shift(-1)
        # å°† NaN å€¼æ›¿æ¢ä¸º0
        log_return = np.where(np.isnan(log_return), 0, log_return)
    else:
        log_return = df[column].values

    # ç»˜åˆ¶åŸå§‹å¯¹æ•°æ”¶ç›Šç‡çš„ç›´æ–¹å›¾
    ax1.hist(log_return, bins=100, alpha=0.5,
             color='blue', label='Original Log Return')

    # è®¡ç®—ååº¦å’Œå³°åº¦
    skewness_orig = skew(log_return)
    kurtosis_orig = kurtosis(log_return)
    ax1.text(0.01, 0.9, f'Original Skew: {skewness_orig:.2f}, Kurtosis: {kurtosis_orig:.2f}',
             transform=ax1.transAxes, fontsize=10, color='blue')

    # é¢œè‰²ç”Ÿæˆå™¨
    color_cycle = plt.cm.viridis(np.linspace(0, 1, len(n_values)))

    # è®¡ç®—å¹¶ç»˜åˆ¶æ¯ä¸ªnå€¼çš„æ»šåŠ¨æ ‡å‡†åŒ–å¯¹æ•°æ”¶ç›Šç‡
    for n, color in zip(n_values, color_cycle):
        # rolling_mean = np.convolve(log_return, np.ones(n) / n, mode='valid')
        # # å¡«å……ä½¿é•¿åº¦ä¸€è‡´
        # rolling_mean = np.concatenate(
        #     (np.full(n - 1, np.nan), rolling_mean, np.full(len(log_return) - len(rolling_mean) - (n - 1), np.nan)))
        # rolling_std = np.sqrt(np.convolve((log_return - rolling_mean) ** 2, np.ones(n) / n, mode='valid'))
        # rolling_std = np.concatenate(
        #     (np.full(n - 1, np.nan), rolling_std, np.full(len(log_return) - len(rolling_std) - (n - 1), np.nan)))
        # norm_log_return = (log_return - rolling_mean) / rolling_std

        norm_log_return = _rolling_zscore_np(log_return, n)

        # ç»˜åˆ¶ç›´æ–¹å›¾
        ax1.hist(norm_log_return, bins=100, alpha=0.5,
                 color=color, label=f'Norm Log Return n={n}')

        # è®¡ç®—ååº¦å’Œå³°åº¦
        skewness = skew(norm_log_return[~np.isnan(norm_log_return)])
        kurtosis_val = kurtosis(norm_log_return[~np.isnan(norm_log_return)])
        ax1.text(0.01, 0.8 - 0.07 * n_values.index(n),
                 f'n={n} Skew: {skewness:.2f}, Kurtosis: {kurtosis_val:.2f}',
                 transform=ax1.transAxes, fontsize=10, color=color)

    # åœ¨ç¬¬äºŒä¸ªå­å›¾ä¸Šè®¾ç½®ä¸¤ä¸ªyè½´
    ax2_2 = ax2.twinx()
    for n, color in zip(n_values, color_cycle):
        norm_log_return = _rolling_zscore_np(log_return, n)
        ax2.plot(np.arange(len(df[column])), norm_log_return.cumsum(
        ), color=color, label=f'Cumulative Norm Log Ret n={n}')

    # ç»˜åˆ¶åŸå§‹æ•°å€¼
    ax2_2.plot(np.arange(len(df[column])), df[column], color='black',
               linewidth=2, label='Original Values', alpha=0.7)
    ax2.set_ylabel('Cumulative Norm Log Ret')
    ax2_2.set_ylabel('Original Values')

    ax1.set_title('Histogram of Log Returns and Normalized Log Returns')
    ax1.set_xlabel('Log Return Value')
    ax1.set_ylabel('Frequency')
    ax1.legend()
    ax2.legend(loc='upper left')
    ax2_2.legend(loc='upper right')

    plt.tight_layout()
    plt.show()


def _rolling_zscore(x1, n=300):  # æ ‡å‡†å·®æ ‡å‡†åŒ–
    x1 = x1.flatten().astype(np.double)
    x1 = np.nan_to_num(x1)
    x1_rolling_avg = ta.SMA(x1, n)  # ä½¿ç”¨TA-Libä¸­çš„ç®€å•ç§»åŠ¨å¹³å‡å‡½æ•°SMA
    x_value = _DIVP(x1, ta.STDDEV(x1, n))
    # x_value = np.clip(x_value, -6, 6)
    return np.nan_to_num(x_value)


def _rolling_zscore_np(x1, n=300):  # æ ‡å‡†å·®æ ‡å‡†åŒ–
    x = np.asarray(x1, dtype=np.float64)
    x1 = np.nan_to_num(x1)
    x1_rolling_avg = ta.SMA(x1, n)  # ä½¿ç”¨TA-Libä¸­çš„ç®€å•ç§»åŠ¨å¹³å‡å‡½æ•°SMA
    x_value = _DIVP(x1, ta.STDDEV(x1, n))
    # x_value = np.clip(x_value, -6, 6)
    return np.nan_to_num(x_value)


def _DIVP(x1, x2):  # é›¶åˆ†æ¯ä¿æŠ¤çš„é™¤æ³•
    x1 = x1.flatten().astype(np.double)
    x2 = x2.flatten().astype(np.double)
    x = np.nan_to_num(np.where(x2 != 0, np.divide(x1, x2), 0))

    return x


def cal_ret(sym: str, freq: str, n: int) -> pd.Series:
    '''è®¡ç®—æœªæ¥nä¸ªå‘¨æœŸçš„æ”¶ç›Šç‡
    params
    sym:å“ç§
    freq:é™é¢‘å‘¨æœŸ
    n:ç¬¬å‡ ä¸ªå‘¨æœŸåçš„æ”¶ç›Šç‡'''
    z = data_load(sym)
    z = resample(z, freq)

    ret = (np.log(z.c).diff(n)*1).shift(-n)  # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
    ret = np.where(np.isnan(ret), 0, ret)

    # å…³é”® - å¯¹labelè¿›è¡Œäº†rolling_zscoreå¤„ç†ï¼ï¼
    ret_ = _rolling_zscore_np(ret, n)
    return ret_
