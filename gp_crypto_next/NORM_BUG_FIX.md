# Normå‡½æ•°åœ¨ç²—ç²’åº¦é¢„è®¡ç®—ä¸­çš„æ•°æ®æ³„éœ²é—®é¢˜ä¿®å¤

## ğŸš¨ é—®é¢˜æè¿°

åœ¨ `dataload.py` çš„ç²—ç²’åº¦é¢„è®¡ç®—ä¼˜åŒ–ä¸­ï¼Œå­˜åœ¨ä¸¥é‡çš„ç‰¹å¾æ ‡å‡†åŒ–é—®é¢˜ï¼š

### é—®é¢˜æ ¹æº

```python
# dataload.py ç¬¬1250è¡Œ
base_feature = originalFeature.BaseFeature(coarse_bars.copy(), include_categories=include_categories)
features_df = base_feature.init_feature_df
```

æ¯ç»„ç²—ç²’åº¦æ•°æ®**ç‹¬ç«‹è®¡ç®—ç‰¹å¾**ï¼Œç‰¹å¾è®¡ç®—ä¸­ä½¿ç”¨ `norm()` å‡½æ•°ï¼š

```python
# functions.py ç¬¬154è¡Œ
def norm(x, rolling_window=2000):
    factors_std = factors_data.rolling(window=rolling_window, min_periods=1).std()
    factor_value = (factors_data) / factors_std
    return np.nan_to_num(factor_value).flatten()
```

### é—®é¢˜å½±å“

1. **æ ‡å‡†åŒ–åŸºå‡†ä¸ä¸€è‡´**ï¼š
   - ç»„0ä½¿ç”¨ç»„0æ•°æ®çš„rolling std
   - ç»„1ä½¿ç”¨ç»„1æ•°æ®çš„rolling std
   - åŒä¸€å¸‚åœºçŠ¶æ€è¢«æ ‡å‡†åŒ–æˆä¸åŒå€¼

2. **æ•°æ®æ³„éœ²**ï¼š
   - Rolling windowä½¿ç”¨æœªæ¥æ•°æ®
   - å‰2000ä¸ªç‚¹æ ‡å‡†åŒ–ä¸ç¨³å®š

3. **ç‰¹å¾ä¸ä¸€è‡´**ï¼š
   - TFTè®­ç»ƒæ—¶ä¼šçœ‹åˆ°åŒä¸€æ—¶åˆ»çš„ä¸åŒæ ‡å‡†åŒ–å€¼
   - æ¨¡å‹æ— æ³•å­¦åˆ°ç¨³å®šçš„æ¨¡å¼

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šåœ¨ç²—ç²’åº¦é˜¶æ®µä¸æ ‡å‡†åŒ–ï¼ˆæ¨èï¼‰â­

#### æ­¥éª¤1ï¼šä¿®æ”¹ `originalFeature.py`

åœ¨ `BaseFeature` ç±»ä¸­æ·»åŠ  `apply_norm` å‚æ•°ï¼š

```python
class BaseFeature:
    def __init__(self, data, include_categories=None, apply_norm=True):
        """
        å‚æ•°:
            data: OHLCVæ•°æ®
            include_categories: è¦è®¡ç®—çš„ç‰¹å¾ç±»åˆ«
            apply_norm: æ˜¯å¦åº”ç”¨normæ ‡å‡†åŒ–ï¼ˆé»˜è®¤Trueï¼Œå…¼å®¹åŸæœ‰ä»£ç ï¼‰
        """
        self.data = data
        self.apply_norm = apply_norm
        self.include_categories = include_categories if include_categories else ['all']
        
        # è®¡ç®—ç‰¹å¾
        self.init_feature_df = self._calculate_all_features()
    
    def _apply_feature_func(self, feature_name, feature_func):
        """åº”ç”¨ç‰¹å¾è®¡ç®—å‡½æ•°"""
        try:
            raw_values = feature_func(self.data)
            
            if self.apply_norm:
                # åŸæœ‰é€»è¾‘ï¼šåº”ç”¨normæ ‡å‡†åŒ–
                return norm(raw_values)
            else:
                # æ–°é€»è¾‘ï¼šä¸æ ‡å‡†åŒ–ï¼Œè¿”å›åŸå§‹å€¼
                return np.nan_to_num(raw_values)
                
        except Exception as e:
            print(f"ç‰¹å¾ {feature_name} è®¡ç®—å¤±è´¥: {e}")
            return np.zeros(len(self.data))
```

#### æ­¥éª¤2ï¼šä¿®æ”¹ `dataload.py`

åœ¨ç²—ç²’åº¦é¢„è®¡ç®—æ—¶ä¸åº”ç”¨æ ‡å‡†åŒ–ï¼š

```python
# ç¬¬1250è¡Œå·¦å³
print(f"  - è®¡ç®—BaseFeatureï¼ˆä¸æ ‡å‡†åŒ–ï¼‰...")

# âš ï¸ é‡è¦ï¼šç²—ç²’åº¦é˜¶æ®µä¸æ ‡å‡†åŒ–
base_feature = originalFeature.BaseFeature(
    coarse_bars.copy(), 
    include_categories=include_categories,
    apply_norm=False  # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä¸æ ‡å‡†åŒ–ï¼
)
features_df = base_feature.init_feature_df

print(f"  âœ“ ç»„{i}å®Œæˆ: {len(features_df)} ä¸ªæ¡¶, {len(features_df.columns)} ä¸ªç‰¹å¾ï¼ˆåŸå§‹å€¼ï¼‰")
```

#### æ­¥éª¤3ï¼šåœ¨æœ€ç»ˆæ•°æ®ç”Ÿæˆæ—¶ç»Ÿä¸€æ ‡å‡†åŒ–

åœ¨ç”Ÿæˆè®­ç»ƒæ•°æ®æ—¶ï¼Œå¯¹æ‰€æœ‰ç‰¹å¾ç»Ÿä¸€æ ‡å‡†åŒ–ï¼š

```python
# åœ¨ dataload.py çš„æœ€åï¼Œç”Ÿæˆæœ€ç»ˆæ•°æ®æ—¶
def normalize_features_uniformly(df, feature_columns, rolling_window=2000):
    """
    ç»Ÿä¸€æ ‡å‡†åŒ–æ‰€æœ‰ç‰¹å¾
    
    å‚æ•°:
        df: åŒ…å«æ‰€æœ‰ç‰¹å¾çš„DataFrame
        feature_columns: éœ€è¦æ ‡å‡†åŒ–çš„ç‰¹å¾åˆ—
        rolling_window: æ»šåŠ¨çª—å£å¤§å°
    """
    print(f"\nğŸ“Š ç»Ÿä¸€æ ‡å‡†åŒ– {len(feature_columns)} ä¸ªç‰¹å¾...")
    
    normalized_df = df.copy()
    
    for col in feature_columns:
        if col in df.columns:
            normalized_df[col] = norm(df[col].values, rolling_window=rolling_window)
    
    print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ")
    return normalized_df


# ä½¿ç”¨ç¤ºä¾‹ï¼ˆåœ¨ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæ•°æ®æ—¶ï¼‰
# å‡è®¾ final_df æ˜¯åˆå¹¶äº†æ‰€æœ‰ç²—ç²’åº¦ç‰¹å¾çš„æœ€ç»ˆDataFrame
feature_cols = [col for col in final_df.columns if col.startswith('lgp_') or col.startswith('ori_')]

# ç»Ÿä¸€æ ‡å‡†åŒ–
final_df = normalize_features_uniformly(final_df, feature_cols, rolling_window=2000)
```

### æ–¹æ¡ˆ2ï¼šä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å‚æ•°

#### ä¿®æ”¹æ€è·¯

1. å…ˆç”¨å®Œæ•´æ•°æ®è®¡ç®—æ¯ä¸ªç‰¹å¾çš„rolling std
2. ä¿å­˜è¿™äº›å‚æ•°
3. åœ¨æ¯ç»„ç²—ç²’åº¦è®¡ç®—æ—¶ï¼Œä½¿ç”¨é¢„è®¡ç®—çš„å‚æ•°

#### å®ç°ä»£ç 

```python
# æ–°å¢ï¼šå¸¦å‚æ•°çš„normå‡½æ•°
def norm_with_params(x, global_std=None, rolling_window=2000):
    """
    ä½¿ç”¨å…¨å±€å‚æ•°è¿›è¡Œæ ‡å‡†åŒ–
    
    å‚æ•°:
        x: è¾“å…¥æ•°æ®
        global_std: é¢„è®¡ç®—çš„æ ‡å‡†å·®ï¼ˆå¦‚æœä¸ºNoneåˆ™è®¡ç®—ï¼‰
        rolling_window: çª—å£å¤§å°
    
    è¿”å›:
        æ ‡å‡†åŒ–åçš„æ•°æ®, [æ ‡å‡†å·®å‚æ•°]
    """
    factors_data = pd.DataFrame(x, columns=['factor'])
    factors_data = factors_data.replace([np.inf, -np.inf, np.nan], 0.0)
    
    if global_std is None:
        # è®¡ç®—æ¨¡å¼ï¼šè¿”å›æ•°æ®å’Œå‚æ•°
        factors_std = factors_data.rolling(window=rolling_window, min_periods=1).std()
        factor_value = factors_data / factors_std
        factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
        return np.nan_to_num(factor_value).flatten(), factors_std.values.flatten()
    else:
        # ä½¿ç”¨æ¨¡å¼ï¼šä½¿ç”¨é¢„è®¡ç®—çš„å‚æ•°
        factors_std = pd.DataFrame(global_std, columns=['factor'])
        factor_value = factors_data / factors_std
        factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
        return np.nan_to_num(factor_value).flatten()
```

### æ–¹æ¡ˆ3ï¼šä½¿ç”¨Expanding Windowï¼ˆæœ€ä¿å®ˆï¼‰

é¿å…ä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼Œæ”¹ç”¨expanding windowï¼š

```python
def norm_expanding(x, min_periods=100):
    """
    ä½¿ç”¨expanding windowæ ‡å‡†åŒ–ï¼ˆä¸ä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼‰
    
    ä¼˜ç‚¹ï¼š
    - ä¸ä¼šæœ‰lookback bias
    - æ¯ä¸ªç‚¹åªä½¿ç”¨å†å²æ•°æ®
    
    ç¼ºç‚¹ï¼š
    - å‰æœŸæ ‡å‡†åŒ–å¯èƒ½ä¸ç¨³å®š
    - å¯¹å¸‚åœºregimeå˜åŒ–é€‚åº”æ…¢
    """
    factors_data = pd.DataFrame(x, columns=['factor'])
    factors_data = factors_data.replace([np.inf, -np.inf, np.nan], 0.0)
    
    # ä½¿ç”¨expandingè€Œä¸æ˜¯rolling
    factors_std = factors_data.expanding(min_periods=min_periods).std()
    factor_value = factors_data / factors_std
    factor_value = factor_value.replace([np.inf, -np.inf, np.nan], 0.0)
    
    return np.nan_to_num(factor_value).flatten()
```

## ğŸ¯ æ¨èæ–¹æ¡ˆ

**æ¨èä½¿ç”¨æ–¹æ¡ˆ1**ï¼Œç†ç”±ï¼š

1. âœ… **å®ç°ç®€å•**ï¼šåªéœ€ä¿®æ”¹å‡ è¡Œä»£ç 
2. âœ… **æ•ˆæœæœ€å¥½**ï¼šæ‰€æœ‰æ•°æ®ä½¿ç”¨ç»Ÿä¸€æ ‡å‡†åŒ–åŸºå‡†
3. âœ… **å…¼å®¹æ€§å¥½**ï¼šä¸å½±å“åŸæœ‰ä»£ç é€»è¾‘
4. âœ… **æ˜“äºè°ƒè¯•**ï¼šæ ‡å‡†åŒ–åœ¨æœ€åä¸€æ­¥ï¼Œå®¹æ˜“æ£€æŸ¥

## ğŸ“‹ å®Œæ•´ä¿®å¤æµç¨‹

### 1. å¤‡ä»½åŸå§‹ä»£ç 

```bash
cp gp_crypto_next/originalFeature.py gp_crypto_next/originalFeature.py.backup
cp gp_crypto_next/dataload.py gp_crypto_next/dataload.py.backup
```

### 2. ä¿®æ”¹ `originalFeature.py`

åœ¨ `BaseFeature.__init__()` ä¸­æ·»åŠ  `apply_norm=True` å‚æ•°ã€‚

### 3. ä¿®æ”¹ `dataload.py`

åœ¨ç¬¬1250è¡Œå·¦å³ï¼Œä¿®æ”¹ä¸ºï¼š

```python
base_feature = originalFeature.BaseFeature(
    coarse_bars.copy(), 
    include_categories=include_categories,
    apply_norm=False  # ç²—ç²’åº¦ä¸æ ‡å‡†åŒ–
)
```

### 4. åœ¨æ•°æ®ç”Ÿæˆpipelineæœ€åæ·»åŠ ç»Ÿä¸€æ ‡å‡†åŒ–

```python
# åœ¨ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæ•°æ®æ—¶
final_df = normalize_features_uniformly(final_df, feature_columns)
```

### 5. æµ‹è¯•éªŒè¯

```python
# æµ‹è¯•ä»£ç 
import pandas as pd
import numpy as np

# ç”Ÿæˆæµ‹è¯•æ•°æ®
test_data = pd.DataFrame({
    'o': np.random.randn(1000),
    'h': np.random.randn(1000),
    'l': np.random.randn(1000),
    'c': np.random.randn(1000),
    'vol': np.random.randn(1000),
})

# æµ‹è¯•ä¸æ ‡å‡†åŒ–
features_raw = originalFeature.BaseFeature(test_data, apply_norm=False)
print("åŸå§‹ç‰¹å¾å‡å€¼:", features_raw.init_feature_df.mean().mean())

# æµ‹è¯•æ ‡å‡†åŒ–
features_norm = originalFeature.BaseFeature(test_data, apply_norm=True)
print("æ ‡å‡†åŒ–ç‰¹å¾å‡å€¼:", features_norm.init_feature_df.mean().mean())
```

## ğŸ” éªŒè¯æ ‡å‡†åŒ–ä¸€è‡´æ€§

æ£€æŸ¥ä¿®å¤åç‰¹å¾çš„ä¸€è‡´æ€§ï¼š

```python
def check_feature_consistency(df, feature_col):
    """æ£€æŸ¥ç‰¹å¾åœ¨ä¸åŒæ—¶é—´æ®µçš„ä¸€è‡´æ€§"""
    # åˆ†æˆ3æ®µ
    n = len(df)
    seg1 = df[feature_col].iloc[:n//3]
    seg2 = df[feature_col].iloc[n//3:2*n//3]
    seg3 = df[feature_col].iloc[2*n//3:]
    
    print(f"ç‰¹å¾ {feature_col} çš„ä¸€è‡´æ€§æ£€æŸ¥:")
    print(f"  æ®µ1 å‡å€¼/æ ‡å‡†å·®: {seg1.mean():.4f} / {seg1.std():.4f}")
    print(f"  æ®µ2 å‡å€¼/æ ‡å‡†å·®: {seg2.mean():.4f} / {seg2.std():.4f}")
    print(f"  æ®µ3 å‡å€¼/æ ‡å‡†å·®: {seg3.mean():.4f} / {seg3.std():.4f}")
    
    # ç†æƒ³æƒ…å†µï¼šæ ‡å‡†åŒ–åå‡å€¼â‰ˆ0ï¼Œæ ‡å‡†å·®â‰ˆ1
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è®­ç»ƒTFTæ—¶çš„å½±å“**ï¼š
   - ä¿®å¤åç‰¹å¾åˆ†å¸ƒä¼šæ›´ç¨³å®š
   - æ¨¡å‹è®­ç»ƒå¯èƒ½æ›´å¿«æ”¶æ•›
   - é¢„æµ‹æ€§èƒ½å¯èƒ½æå‡

2. **å†å²æ¨¡å‹å…¼å®¹æ€§**ï¼š
   - ç”¨æ–°æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹ä¸èƒ½ç›´æ¥æ›¿æ¢æ—§æ¨¡å‹
   - éœ€è¦é‡æ–°è®­ç»ƒ

3. **è®¡ç®—æ•ˆç‡**ï¼š
   - ç»Ÿä¸€æ ‡å‡†åŒ–åªéœ€è®¡ç®—ä¸€æ¬¡
   - æ¯”åŸæ–¹æ¡ˆæ›´å¿«

## ğŸ“š å‚è€ƒèµ„æ–™

- [Feature Scaling Best Practices](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Time Series Data Leakage](https://machinelearningmastery.com/data-leakage-machine-learning/)
- [Rolling vs Expanding Windows](https://pandas.pydata.org/docs/reference/window.html)

---

**ä¿®å¤æ—¥æœŸ**: 2025-11-05  
**é—®é¢˜ç­‰çº§**: ğŸ”´ ä¸¥é‡  
**å½±å“èŒƒå›´**: æ‰€æœ‰ä½¿ç”¨ç²—ç²’åº¦é¢„è®¡ç®—çš„è®­ç»ƒæ•°æ®  
**ä¿®å¤ä¼˜å…ˆçº§**: é«˜

